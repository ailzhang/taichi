[Taichi] version 1.7.0, llvm 15.0.4, commit 04d25c54, linux, python 3.10.9
[Taichi] Starting on arch=cuda
[I 04/20/23 19:15:55.761 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Initial IR:
kernel {
  $0 : for @tmp0 in 1d_ext_arr (element_dim=0, dt=f32, grad=false) noneblock_dim=adaptive {
    $1 = alloca @tmp1
    @tmp1 = [@tmp0] (dt=[Tensor (1) i32])
    $3 = alloca @tmp2
    @tmp2 = 1d_ext_arr (element_dim=0, dt=f32, grad=false)[@tmp1[0]]
    $5 = alloca @tmp3
    @tmp3 = [@tmp0] (dt=[Tensor (1) i32])
    #@tmp0 (snode=S2place<f32>)[@tmp3[0]] = @tmp2
  }
}
[I 04/20/23 19:15:55.761 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Lowered:
kernel {
  $0 = external_tensor_shape_along_axis 0, arg_id 0
  <i32> $1 = const 0
  <i32> $2 = const 1
  $3 = mul $2 $0
  $4 : for in range($1, $3) block_dim=adaptive {
    $5 = loop $4 index 0
    <i32> $6 = mod $5 $0
    decorate $6 : Loop-unique 1
    $8 = div $5 $0
    <[Tensor (1) i32]> $9 = alloca
    <[Tensor (1) i32]> $10 = [$6]
    $11 : local store [$9 <- $10]
    <f32> $12 = alloca
    <i32> $13 = const 0
    <*i32> $14 = shift ptr [$9 + $13]
    <i32> $15 = local load [$14]
    <struct[none]{0(data_ptr, at 0B): *f32}> $16 = argaddr[0]
    <f32> $17 = external_ptr $16, [$15] element_dim=0 layout=AOS is_grad=false
    <f32> $18 = global load $17
    $19 : local store [$12 <- $18]
    <[Tensor (1) i32]> $20 = alloca
    <[Tensor (1) i32]> $21 = [$6]
    $22 : local store [$20 <- $21]
    <f32> $23 = local load [$12]
    <i32> $24 = const 0
    <*i32> $25 = shift ptr [$20 + $24]
    <i32> $26 = local load [$25]
    <f32> $27 = global ptr [S2place<f32>], index [$26] activate=true
    $28 : global store [$27 <- $23]
  }
}
[I 04/20/23 19:15:55.761 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Immutable local vars eliminated:
kernel {
  $0 = external_tensor_shape_along_axis 0, arg_id 0
  <i32> $1 = const 0
  <i32> $2 = const 1
  $3 = mul $2 $0
  $4 : for in range($1, $3) block_dim=adaptive {
    $5 = loop $4 index 0
    <i32> $6 = mod $5 $0
    decorate $6 : Loop-unique 1
    $8 = div $5 $0
    <[Tensor (1) i32]> $9 = alloca
    <[Tensor (1) i32]> $10 = [$6]
    $11 : local store [$9 <- $10]
    <i32> $12 = const 0
    <*i32> $13 = shift ptr [$9 + $12]
    <i32> $14 = local load [$13]
    <struct[none]{0(data_ptr, at 0B): *f32}> $15 = argaddr[0]
    <f32> $16 = external_ptr $15, [$14] element_dim=0 layout=AOS is_grad=false
    <f32> $17 = global load $16
    <[Tensor (1) i32]> $18 = alloca
    <[Tensor (1) i32]> $19 = [$6]
    $20 : local store [$18 <- $19]
    <i32> $21 = const 0
    <*i32> $22 = shift ptr [$18 + $21]
    <i32> $23 = local load [$22]
    <f32> $24 = global ptr [S2place<f32>], index [$23] activate=true
    $25 : global store [$24 <- $17]
  }
}
[I 04/20/23 19:15:55.761 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Typechecked:
kernel {
  <i32> $0 = external_tensor_shape_along_axis 0, arg_id 0
  <i32> $1 = const 0
  <i32> $2 = const 1
  <i32> $3 = mul $2 $0
  $4 : for in range($1, $3) block_dim=adaptive {
    <i32> $5 = loop $4 index 0
    <i32> $6 = mod $5 $0
    decorate $6 : Loop-unique 1
    <i32> $8 = div $5 $0
    <[Tensor (1) i32]> $9 = alloca
    <[Tensor (1) i32]> $10 = [$6]
    <[Tensor (1) i32]> $11 : local store [$9 <- $10]
    <i32> $12 = const 0
    <*i32> $13 = shift ptr [$9 + $12]
    <i32> $14 = local load [$13]
    <*struct[none]{0(data_ptr, at 0B): *f32}> $15 = argaddr[0]
    <*f32> $16 = external_ptr $15, [$14] element_dim=0 layout=AOS is_grad=false
    <f32> $17 = global load $16
    <[Tensor (1) i32]> $18 = alloca
    <[Tensor (1) i32]> $19 = [$6]
    <[Tensor (1) i32]> $20 : local store [$18 <- $19]
    <i32> $21 = const 0
    <*i32> $22 = shift ptr [$18 + $21]
    <i32> $23 = local load [$22]
    <*f32> $24 = global ptr [S2place<f32>], index [$23] activate=true
    $25 : global store [$24 <- $17]
  }
}
[I 04/20/23 19:15:55.762 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Bit Loop Vectorized:
kernel {
  <i32> $0 = external_tensor_shape_along_axis 0, arg_id 0
  <i32> $1 = const 0
  <i32> $2 = const 1
  <i32> $3 = mul $2 $0
  $4 : for in range($1, $3) block_dim=adaptive {
    <i32> $5 = loop $4 index 0
    <i32> $6 = mod $5 $0
    decorate $6 : Loop-unique 1
    <[Tensor (1) i32]> $8 = alloca
    <[Tensor (1) i32]> $9 = [$6]
    <[Tensor (1) i32]> $10 : local store [$8 <- $9]
    <i32> $11 = const 0
    <*i32> $12 = shift ptr [$8 + $11]
    <i32> $13 = local load [$12]
    <*struct[none]{0(data_ptr, at 0B): *f32}> $14 = argaddr[0]
    <*f32> $15 = external_ptr $14, [$13] element_dim=0 layout=AOS is_grad=false
    <f32> $16 = global load $15
    <[Tensor (1) i32]> $17 = alloca
    <[Tensor (1) i32]> $18 = [$6]
    <[Tensor (1) i32]> $19 : local store [$17 <- $18]
    <i32> $20 = const 0
    <*i32> $21 = shift ptr [$17 + $20]
    <i32> $22 = local load [$21]
    <*f32> $23 = global ptr [S2place<f32>], index [$22] activate=true
    $24 : global store [$23 <- $16]
  }
}
[I 04/20/23 19:15:55.762 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Matrix ptr lowered:
kernel {
  <i32> $0 = external_tensor_shape_along_axis 0, arg_id 0
  <i32> $1 = const 0
  <i32> $2 = const 1
  <i32> $3 = mul $2 $0
  $4 : for in range($1, $3) block_dim=adaptive {
    <i32> $5 = loop $4 index 0
    <i32> $6 = mod $5 $0
    decorate $6 : Loop-unique 1
    <[Tensor (1) i32]> $8 = alloca
    <[Tensor (1) i32]> $9 = [$6]
    <[Tensor (1) i32]> $10 : local store [$8 <- $9]
    <i32> $11 = const 0
    <*i32> $12 = shift ptr [$8 + $11]
    <i32> $13 = local load [$12]
    <*struct[none]{0(data_ptr, at 0B): *f32}> $14 = argaddr[0]
    <*f32> $15 = external_ptr $14, [$13] element_dim=0 layout=AOS is_grad=false
    <f32> $16 = global load $15
    <[Tensor (1) i32]> $17 = alloca
    <[Tensor (1) i32]> $18 = [$6]
    <[Tensor (1) i32]> $19 : local store [$17 <- $18]
    <i32> $20 = const 0
    <*i32> $21 = shift ptr [$17 + $20]
    <i32> $22 = local load [$21]
    <*f32> $23 = global ptr [S2place<f32>], index [$22] activate=true
    $24 : global store [$23 <- $16]
  }
}
[I 04/20/23 19:15:55.762 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Simplified I:
kernel {
  <i32> $0 = const 0
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  $2 : for in range($0, $1) block_dim=adaptive {
    <i32> $3 = loop $2 index 0
    <i32> $4 = mod $3 $1
    decorate $4 : Loop-unique 1
    <[Tensor (1) i32]> $6 = alloca
    <[Tensor (1) i32]> $7 = [$4]
    <[Tensor (1) i32]> $8 : local store [$6 <- $7]
    <*i32> $9 = shift ptr [$6 + $0]
    <i32> $10 = local load [$9]
    <*struct[none]{0(data_ptr, at 0B): *f32}> $11 = argaddr[0]
    <*f32> $12 = external_ptr $11, [$10] element_dim=0 layout=AOS is_grad=false
    <f32> $13 = global load $12
    <[Tensor (1) i32]> $14 = alloca
    <[Tensor (1) i32]> $15 : local store [$14 <- $7]
    <*i32> $16 = shift ptr [$14 + $0]
    <i32> $17 = local load [$16]
    <*f32> $18 = global ptr [S2place<f32>], index [$17] activate=true
    $19 : global store [$18 <- $13]
  }
}
[I 04/20/23 19:15:55.762 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Scalarized:
kernel {
  <i32> $0 = const 0
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  $2 : for in range($0, $1) block_dim=adaptive {
    <i32> $3 = loop $2 index 0
    <i32> $4 = mod $3 $1
    decorate $4 : Loop-unique 1
    <i32> $6 = alloca
    $7 : local store [$6 <- $4]
    <i32> $8 = local load [$6]
    <*struct[none]{0(data_ptr, at 0B): *f32}> $9 = argaddr[0]
    <*f32> $10 = external_ptr $9, [$8] element_dim=0 layout=AOS is_grad=false
    <f32> $11 = global load $10
    <i32> $12 = alloca
    $13 : local store [$12 <- $4]
    <i32> $14 = local load [$12]
    <*f32> $15 = global ptr [S2place<f32>], index [$14] activate=true
    $16 : global store [$15 <- $11]
  }
}
[I 04/20/23 19:15:55.762 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Access flagged I:
kernel {
  <i32> $0 = const 0
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  $2 : for in range($0, $1) block_dim=adaptive {
    <i32> $3 = loop $2 index 0
    <i32> $4 = mod $3 $1
    decorate $4 : Loop-unique 1
    <i32> $6 = alloca
    $7 : local store [$6 <- $4]
    <i32> $8 = local load [$6]
    <*struct[none]{0(data_ptr, at 0B): *f32}> $9 = argaddr[0]
    <*f32> $10 = external_ptr $9, [$8] element_dim=0 layout=AOS is_grad=false
    <f32> $11 = global load $10
    <i32> $12 = alloca
    $13 : local store [$12 <- $4]
    <i32> $14 = local load [$12]
    <*f32> $15 = global ptr [S2place<f32>], index [$14] activate=true
    $16 : global store [$15 <- $11]
  }
}
[I 04/20/23 19:15:55.763 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Simplified II:
kernel {
  <i32> $0 = const 0
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  $2 : for in range($0, $1) block_dim=adaptive {
    <i32> $3 = loop $2 index 0
    <i32> $4 = mod $3 $1
    decorate $4 : Loop-unique 1
    <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
    <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
    <f32> $8 = global load $7
    <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=true
    $10 : global store [$9 <- $8]
  }
}
[I 04/20/23 19:15:55.763 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Offloaded:
kernel {
  $0 = offloaded
  body {
    <i32> $1 = const 0
    <i32> $2 = external_tensor_shape_along_axis 0, arg_id 0
    <*i32> $3 = global tmp var (offset = 0 B)
    $4 : global store [$3 <- $2]
  }
  $5 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
  body {
    <i32> $6 = loop $5 index 0
    <*i32> $7 = global tmp var (offset = 0 B)
    <i32> $8 = global load $7
    <i32> $9 = mod $6 $8
    decorate $9 : Loop-unique 1
    <*struct[none]{0(data_ptr, at 0B): *f32}> $11 = argaddr[0]
    <*f32> $12 = external_ptr $11, [$9] element_dim=0 layout=AOS is_grad=false
    <f32> $13 = global load $12
    <*f32> $14 = global ptr [S2place<f32>], index [$9] activate=true
    $15 : global store [$14 <- $13]
  }
}
[I 04/20/23 19:15:55.763 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Optimized by CFG:
kernel {
  $0 = offloaded
  body {
    <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
    <*i32> $2 = global tmp var (offset = 0 B)
    $3 : global store [$2 <- $1]
  }
  $4 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
  body {
    <i32> $5 = loop $4 index 0
    <*i32> $6 = global tmp var (offset = 0 B)
    <i32> $7 = global load $6
    <i32> $8 = mod $5 $7
    decorate $8 : Loop-unique 1
    <*struct[none]{0(data_ptr, at 0B): *f32}> $10 = argaddr[0]
    <*f32> $11 = external_ptr $10, [$8] element_dim=0 layout=AOS is_grad=false
    <f32> $12 = global load $11
    <*f32> $13 = global ptr [S2place<f32>], index [$8] activate=true
    $14 : global store [$13 <- $12]
  }
}
[I 04/20/23 19:15:55.763 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Access flagged II:
kernel {
  $0 = offloaded
  body {
    <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
    <*i32> $2 = global tmp var (offset = 0 B)
    $3 : global store [$2 <- $1]
  }
  $4 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
  body {
    <i32> $5 = loop $4 index 0
    <*i32> $6 = global tmp var (offset = 0 B)
    <i32> $7 = global load $6
    <i32> $8 = mod $5 $7
    decorate $8 : Loop-unique 1
    <*struct[none]{0(data_ptr, at 0B): *f32}> $10 = argaddr[0]
    <*f32> $11 = external_ptr $10, [$8] element_dim=0 layout=AOS is_grad=false
    <f32> $12 = global load $11
    <*f32> $13 = global ptr [S2place<f32>], index [$8] activate=true
    $14 : global store [$13 <- $12]
  }
}
[I 04/20/23 19:15:55.763 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Simplified III:
kernel {
  $0 = offloaded
  body {
    <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
    <*i32> $2 = global tmp var (offset = 0 B)
    $3 : global store [$2 <- $1]
  }
  $4 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
  body {
    <i32> $5 = loop $4 index 0
    <*i32> $6 = global tmp var (offset = 0 B)
    <i32> $7 = global load $6
    <i32> $8 = mod $5 $7
    decorate $8 : Loop-unique 1
    <*struct[none]{0(data_ptr, at 0B): *f32}> $10 = argaddr[0]
    <*f32> $11 = external_ptr $10, [$8] element_dim=0 layout=AOS is_grad=false
    <f32> $12 = global load $11
    <*f32> $13 = global ptr [S2place<f32>], index [$8] activate=true
    $14 : global store [$13 <- $12]
  }
}
[I 04/20/23 19:15:55.791 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Start offload_to_executable:
[I 04/20/23 19:15:55.791 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Start offload_to_executable:
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
kernel {
  <*i32> $2 = global tmp var (offset = 0 B)
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  $3 : global store [$2 <- $1]
}
}
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
[I 04/20/23 19:15:55.791 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Detect read-only accesses:
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 0 B)
  $3 : global store [$2 <- $1]
}
}
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
[I 04/20/23 19:15:55.791 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Atomics demoted I:
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
  <f32> $8 = global load $7
  <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=true
  $10 : global store [$9 <- $8]
}
}
kernel {
$0 = offloaded
body {
[I 04/20/23 19:15:55.791 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Detect read-only accesses:
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 0 B)
  $3 : global store [$2 <- $1]
}
}
kernel {
[I 04/20/23 19:15:55.791 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Cache loop-invariant global vars:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 0 B)
  $3 : global store [$2 <- $1]
}
}
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
[I 04/20/23 19:15:55.792 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Dense struct-for demoted:
  <f32> $8 = global load $7
  <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=true
  $10 : global store [$9 <- $8]
}
}
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
[I 04/20/23 19:15:55.792 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Atomics demoted I:
  <*i32> $2 = global tmp var (offset = 0 B)
  $3 : global store [$2 <- $1]
}
}
kernel {
[I 04/20/23 19:15:55.792 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] No-access mesh-for demoted:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
  <f32> $8 = global load $7
  <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=true
  $10 : global store [$9 <- $8]
}
}
kernel {
$0 = offloaded
[I 04/20/23 19:15:55.792 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Cache loop-invariant global vars:
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 0 B)
  $3 : global store [$2 <- $1]
}
}
kernel {
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
[I 04/20/23 19:15:55.792 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Make thread local:
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
  <f32> $8 = global load $7
  <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=true
  $10 : global store [$9 <- $8]
}
}
kernel {
[I 04/20/23 19:15:55.792 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Dense struct-for demoted:
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 0 B)
  $3 : global store [$2 <- $1]
}
}
kernel {
[I 04/20/23 19:15:55.792 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Make mesh thread local:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
  <f32> $8 = global load $7
  <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=true
  $10 : global store [$9 <- $8]
}
}
kernel {
[I 04/20/23 19:15:55.792 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] No-access mesh-for demoted:
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 0 B)
  $3 : global store [$2 <- $1]
}
}
kernel {
[I 04/20/23 19:15:55.792 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Make mesh block local:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
  <f32> $8 = global load $7
  <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=true
  $10 : global store [$9 <- $8]
}
}
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 0 B)
  $3 : global store [$2 <- $1]
[I 04/20/23 19:15:55.792 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Make thread local:
}
}
kernel {
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
[I 04/20/23 19:15:55.792 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Simplified X:
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
  <f32> $8 = global load $7
  <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=true
  $10 : global store [$9 <- $8]
}
}
kernel {
[I 04/20/23 19:15:55.792 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Make mesh thread local:
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 0 B)
  $3 : global store [$2 <- $1]
}
}
kernel {
[I 04/20/23 19:15:55.792 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Make block local:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
kernel {
  <f32> $8 = global load $7
  <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=true
  $10 : global store [$9 <- $8]
$0 = offloaded
body {
}
}
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 0 B)
[I 04/20/23 19:15:55.792 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Make mesh block local:
  $3 : global store [$2 <- $1]
}
}
kernel {
[I 04/20/23 19:15:55.792 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Demote mesh statements:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
kernel {
$0 = offloaded
body {
  <*i32> $2 = global tmp var (offset = 0 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
  <*i32> $2 = global tmp var (offset = 0 B)
  <f32> $8 = global load $7
  <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=true
  $10 : global store [$9 <- $8]
}
}
  $3 : global store [$2 <- $1]
}
}
[I 04/20/23 19:15:55.792 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Atomics demoted II:
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 0 B)
[I 04/20/23 19:15:55.792 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Simplified X:
  $3 : global store [$2 <- $1]
}
}
kernel {
[I 04/20/23 19:15:55.792 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Remove range assumption:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
  <f32> $8 = global load $7
  <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=true
  $10 : global store [$9 <- $8]
}
}
kernel {
[I 04/20/23 19:15:55.792 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Make block local:
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 0 B)
  $3 : global store [$2 <- $1]
}
}
kernel {
[I 04/20/23 19:15:55.792 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Remove loop_unique:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
  <f32> $8 = global load $7
  <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=true
  $10 : global store [$9 <- $8]
}
}
kernel {
[I 04/20/23 19:15:55.793 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Demote mesh statements:
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 0 B)
  $3 : global store [$2 <- $1]
}
}
kernel {
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
  <f32> $8 = global load $7
[I 04/20/23 19:15:55.793 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Simplified before lower access:
  <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=true
  $10 : global store [$9 <- $8]
}
}
kernel {
[I 04/20/23 19:15:55.793 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Atomics demoted II:
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 0 B)
  $3 : global store [$2 <- $1]
}
}
kernel {
[I 04/20/23 19:15:55.793 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Access lowered:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
  <f32> $8 = global load $7
  <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=true
  $10 : global store [$9 <- $8]
}
}
kernel {
[I 04/20/23 19:15:55.793 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Remove range assumption:
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 0 B)
  $3 : global store [$2 <- $1]
}
}
kernel {
[I 04/20/23 19:15:55.793 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] DIE:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
  <f32> $8 = global load $7
  <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=true
  $10 : global store [$9 <- $8]
}
}
[I 04/20/23 19:15:55.793 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Remove loop_unique:
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 0 B)
  $3 : global store [$2 <- $1]
}
}
kernel {
[I 04/20/23 19:15:55.793 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Access flagged III:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
  <f32> $8 = global load $7
  <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=true
  $10 : global store [$9 <- $8]
}
}
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 0 B)
  $3 : global store [$2 <- $1]
}
}
[I 04/20/23 19:15:55.793 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Simplified before lower access:
kernel {
[I 04/20/23 19:15:55.793 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Operations demoted:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
  <f32> $8 = global load $7
  <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=true
  $10 : global store [$9 <- $8]
}
}
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
[I 04/20/23 19:15:55.793 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Access lowered:
  <*i32> $2 = global tmp var (offset = 0 B)
  $3 : global store [$2 <- $1]
}
}
kernel {
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
[I 04/20/23 19:15:55.793 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Simplified IV:
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
  <f32> $8 = global load $7
  <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=true
  <*gen> $10 = get root [S0root][root]
  <i32> $11 = linearized(ind {}, stride {})
  <*gen> $12 = [S0root][root]::lookup($10, $11) activate = false
  <*gen> $13 = get child [S0root->S1dense] $12
  <i32> $14 = const 0
  <i32> $15 = bit_shr $4 $14
  <i32> $16 = linearized(ind {$15}, stride {8388608})
  <*gen> $17 = [S1dense][dense]::lookup($13, $16) activate = false
  <*f32> $18 = get child [S1dense->S2place<f32>] $17
  $19 : global store [$18 <- $8]
}
}
kernel {
[I 04/20/23 19:15:55.793 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] DIE:
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 0 B)
  $3 : global store [$2 <- $1]
}
}
kernel {
[I 04/20/23 19:15:55.793 1820178] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Bit struct stores optimized:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
  <f32> $8 = global load $7
  <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=true
  <*gen> $10 = get root [S0root][root]
  <i32> $11 = linearized(ind {}, stride {})
  <*gen> $12 = [S0root][root]::lookup($10, $11) activate = false
  <*gen> $13 = get child [S0root->S1dense] $12
  <i32> $14 = const 0
  <i32> $15 = bit_shr $4 $14
  <i32> $16 = linearized(ind {$15}, stride {8388608})
  <*gen> $17 = [S1dense][dense]::lookup($13, $16) activate = false
  <*f32> $18 = get child [S1dense->S2place<f32>] $17
  $19 : global store [$18 <- $8]
}
}
kernel {
[I 04/20/23 19:15:55.793 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Access flagged III:
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 0 B)
  $3 : global store [$2 <- $1]
}
}
kernel {
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
  <f32> $8 = global load $7
  <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=false
  <*gen> $10 = get root [S0root][root]
  <i32> $11 = linearized(ind {}, stride {})
  <*gen> $12 = [S0root][root]::lookup($10, $11) activate = false
  <*gen> $13 = get child [S0root->S1dense] $12
  <i32> $14 = const 0
  <i32> $15 = bit_shr $4 $14
  <i32> $16 = linearized(ind {$15}, stride {8388608})
  <*gen> $17 = [S1dense][dense]::lookup($13, $16) activate = false
  <*f32> $18 = get child [S1dense->S2place<f32>] $17
  $19 : global store [$18 <- $8]
}
}
[I 04/20/23 19:15:55.794 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Operations demoted:
kernel {
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
  <f32> $8 = global load $7
  <*f32> $9 = global ptr [S2place<f32>], index [$4] activate=false
  <*gen> $10 = get root [S0root][root]
  <i32> $11 = linearized(ind {}, stride {})
  <*gen> $12 = [S0root][root]::lookup($10, $11) activate = false
  <*gen> $13 = get child [S0root->S1dense] $12
  <i32> $14 = const 0
  <u32> $15 = reinterpret_cast_bits<u32> $4
  <u32> $16 = cast_value<u32> $14
  <u32> $17 = bit_sar $15 $16
  <i32> $18 = reinterpret_cast_bits<i32> $17
  <i32> $19 = linearized(ind {$18}, stride {8388608})
  <*gen> $20 = [S1dense][dense]::lookup($13, $19) activate = false
  <*f32> $21 = get child [S1dense->S2place<f32>] $20
  $22 : global store [$21 <- $8]
}
}
[I 04/20/23 19:15:55.794 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Simplified IV:
kernel {
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
  <f32> $8 = global load $7
  <*gen> $9 = get root [S0root][root]
  <i32> $10 = const 0
  <*gen> $11 = [S0root][root]::lookup($9, $10) activate = false
  <*gen> $12 = get child [S0root->S1dense] $11
  <*gen> $13 = [S1dense][dense]::lookup($12, $4) activate = false
  <*f32> $14 = get child [S1dense->S2place<f32>] $13
  $15 : global store [$14 <- $8]
}
}
[I 04/20/23 19:15:55.794 1820180] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_0] Bit struct stores optimized:
kernel {
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 0 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *f32}> $6 = argaddr[0]
  <*f32> $7 = external_ptr $6, [$4] element_dim=0 layout=AOS is_grad=false
  <f32> $8 = global load $7
  <*gen> $9 = get root [S0root][root]
  <i32> $10 = const 0
  <*gen> $11 = [S0root][root]::lookup($9, $10) activate = false
  <*gen> $12 = get child [S0root->S1dense] $11
  <*gen> $13 = [S1dense][dense]::lookup($12, $4) activate = false
  <*f32> $14 = get child [S1dense->S2place<f32>] $13
  $15 : global store [$14 <- $8]
}
}
[I 04/20/23 19:15:55.837 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Initial IR:
kernel {
  $0 : for @tmp0, @tmp1 in 2d_ext_arr (element_dim=0, dt=i64, grad=false) noneblock_dim=adaptive {
    $1 = alloca @tmp2
    @tmp2 = [@tmp0, @tmp1] (dt=[Tensor (2) i32])
    $3 = alloca @tmp3
    @tmp3 = 2d_ext_arr (element_dim=0, dt=i64, grad=false)[@tmp2[0], @tmp2[1]]
    $5 = alloca @tmp4
    @tmp4 = [@tmp0, @tmp1] (dt=[Tensor (2) i32])
    #@tmp3 (snode=S6place<i64>)[@tmp4[0], @tmp4[1]] = @tmp3
  }
}
[I 04/20/23 19:15:55.838 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Lowered:
kernel {
  $0 = external_tensor_shape_along_axis 0, arg_id 0
  $1 = external_tensor_shape_along_axis 1, arg_id 0
  <i32> $2 = const 0
  <i32> $3 = const 1
  $4 = mul $3 $0
  $5 = mul $4 $1
  $6 : for in range($2, $5) block_dim=adaptive {
    $7 = loop $6 index 0
    <i32> $8 = mod $7 $1
    decorate $8 : Loop-unique 1
    $10 = div $7 $1
    <i32> $11 = mod $10 $0
    decorate $11 : Loop-unique 1
    $13 = div $10 $0
    <[Tensor (2) i32]> $14 = alloca
    <[Tensor (2) i32]> $15 = [$11, $8]
    $16 : local store [$14 <- $15]
    <i64> $17 = alloca
    <i32> $18 = const 0
    <*i32> $19 = shift ptr [$14 + $18]
    <i32> $20 = local load [$19]
    <i32> $21 = const 1
    <*i32> $22 = shift ptr [$14 + $21]
    <i32> $23 = local load [$22]
    <struct[none]{0(data_ptr, at 0B): *i64}> $24 = argaddr[0]
    <i64> $25 = external_ptr $24, [$20, $23] element_dim=0 layout=AOS is_grad=false
    <i64> $26 = global load $25
    $27 : local store [$17 <- $26]
    <[Tensor (2) i32]> $28 = alloca
    <[Tensor (2) i32]> $29 = [$11, $8]
    $30 : local store [$28 <- $29]
    <i64> $31 = local load [$17]
    <i32> $32 = const 0
    <*i32> $33 = shift ptr [$28 + $32]
    <i32> $34 = local load [$33]
    <i32> $35 = const 1
    <*i32> $36 = shift ptr [$28 + $35]
    <i32> $37 = local load [$36]
    <i64> $38 = global ptr [S6place<i64>], index [$34, $37] activate=true
    $39 : global store [$38 <- $31]
  }
}
[I 04/20/23 19:15:55.838 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Immutable local vars eliminated:
kernel {
  $0 = external_tensor_shape_along_axis 0, arg_id 0
  $1 = external_tensor_shape_along_axis 1, arg_id 0
  <i32> $2 = const 0
  <i32> $3 = const 1
  $4 = mul $3 $0
  $5 = mul $4 $1
  $6 : for in range($2, $5) block_dim=adaptive {
    $7 = loop $6 index 0
    <i32> $8 = mod $7 $1
    decorate $8 : Loop-unique 1
    $10 = div $7 $1
    <i32> $11 = mod $10 $0
    decorate $11 : Loop-unique 1
    $13 = div $10 $0
    <[Tensor (2) i32]> $14 = alloca
    <[Tensor (2) i32]> $15 = [$11, $8]
    $16 : local store [$14 <- $15]
    <i32> $17 = const 0
    <*i32> $18 = shift ptr [$14 + $17]
    <i32> $19 = local load [$18]
    <i32> $20 = const 1
    <*i32> $21 = shift ptr [$14 + $20]
    <i32> $22 = local load [$21]
    <struct[none]{0(data_ptr, at 0B): *i64}> $23 = argaddr[0]
    <i64> $24 = external_ptr $23, [$19, $22] element_dim=0 layout=AOS is_grad=false
    <i64> $25 = global load $24
    <[Tensor (2) i32]> $26 = alloca
    <[Tensor (2) i32]> $27 = [$11, $8]
    $28 : local store [$26 <- $27]
    <i32> $29 = const 0
    <*i32> $30 = shift ptr [$26 + $29]
    <i32> $31 = local load [$30]
    <i32> $32 = const 1
    <*i32> $33 = shift ptr [$26 + $32]
    <i32> $34 = local load [$33]
    <i64> $35 = global ptr [S6place<i64>], index [$31, $34] activate=true
    $36 : global store [$35 <- $25]
  }
}
[I 04/20/23 19:15:55.838 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Typechecked:
kernel {
  <i32> $0 = external_tensor_shape_along_axis 0, arg_id 0
  <i32> $1 = external_tensor_shape_along_axis 1, arg_id 0
  <i32> $2 = const 0
  <i32> $3 = const 1
  <i32> $4 = mul $3 $0
  <i32> $5 = mul $4 $1
  $6 : for in range($2, $5) block_dim=adaptive {
    <i32> $7 = loop $6 index 0
    <i32> $8 = mod $7 $1
    decorate $8 : Loop-unique 1
    <i32> $10 = div $7 $1
    <i32> $11 = mod $10 $0
    decorate $11 : Loop-unique 1
    <i32> $13 = div $10 $0
    <[Tensor (2) i32]> $14 = alloca
    <[Tensor (2) i32]> $15 = [$11, $8]
    <[Tensor (2) i32]> $16 : local store [$14 <- $15]
    <i32> $17 = const 0
    <*i32> $18 = shift ptr [$14 + $17]
    <i32> $19 = local load [$18]
    <i32> $20 = const 1
    <*i32> $21 = shift ptr [$14 + $20]
    <i32> $22 = local load [$21]
    <*struct[none]{0(data_ptr, at 0B): *i64}> $23 = argaddr[0]
    <*i64> $24 = external_ptr $23, [$19, $22] element_dim=0 layout=AOS is_grad=false
    <i64> $25 = global load $24
    <[Tensor (2) i32]> $26 = alloca
    <[Tensor (2) i32]> $27 = [$11, $8]
    <[Tensor (2) i32]> $28 : local store [$26 <- $27]
    <i32> $29 = const 0
    <*i32> $30 = shift ptr [$26 + $29]
    <i32> $31 = local load [$30]
    <i32> $32 = const 1
    <*i32> $33 = shift ptr [$26 + $32]
    <i32> $34 = local load [$33]
    <*i64> $35 = global ptr [S6place<i64>], index [$31, $34] activate=true
    $36 : global store [$35 <- $25]
  }
}
[I 04/20/23 19:15:55.838 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Bit Loop Vectorized:
kernel {
  <i32> $0 = external_tensor_shape_along_axis 0, arg_id 0
  <i32> $1 = external_tensor_shape_along_axis 1, arg_id 0
  <i32> $2 = const 0
  <i32> $3 = const 1
  <i32> $4 = mul $3 $0
  <i32> $5 = mul $4 $1
  $6 : for in range($2, $5) block_dim=adaptive {
    <i32> $7 = loop $6 index 0
    <i32> $8 = mod $7 $1
    decorate $8 : Loop-unique 1
    <i32> $10 = div $7 $1
    <i32> $11 = mod $10 $0
    decorate $11 : Loop-unique 1
    <[Tensor (2) i32]> $13 = alloca
    <[Tensor (2) i32]> $14 = [$11, $8]
    <[Tensor (2) i32]> $15 : local store [$13 <- $14]
    <i32> $16 = const 0
    <*i32> $17 = shift ptr [$13 + $16]
    <i32> $18 = local load [$17]
    <i32> $19 = const 1
    <*i32> $20 = shift ptr [$13 + $19]
    <i32> $21 = local load [$20]
    <*struct[none]{0(data_ptr, at 0B): *i64}> $22 = argaddr[0]
    <*i64> $23 = external_ptr $22, [$18, $21] element_dim=0 layout=AOS is_grad=false
    <i64> $24 = global load $23
    <[Tensor (2) i32]> $25 = alloca
    <[Tensor (2) i32]> $26 = [$11, $8]
    <[Tensor (2) i32]> $27 : local store [$25 <- $26]
    <i32> $28 = const 0
    <*i32> $29 = shift ptr [$25 + $28]
    <i32> $30 = local load [$29]
    <i32> $31 = const 1
    <*i32> $32 = shift ptr [$25 + $31]
    <i32> $33 = local load [$32]
    <*i64> $34 = global ptr [S6place<i64>], index [$30, $33] activate=true
    $35 : global store [$34 <- $24]
  }
}
[I 04/20/23 19:15:55.839 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Matrix ptr lowered:
kernel {
  <i32> $0 = external_tensor_shape_along_axis 0, arg_id 0
  <i32> $1 = external_tensor_shape_along_axis 1, arg_id 0
  <i32> $2 = const 0
  <i32> $3 = const 1
  <i32> $4 = mul $3 $0
  <i32> $5 = mul $4 $1
  $6 : for in range($2, $5) block_dim=adaptive {
    <i32> $7 = loop $6 index 0
    <i32> $8 = mod $7 $1
    decorate $8 : Loop-unique 1
    <i32> $10 = div $7 $1
    <i32> $11 = mod $10 $0
    decorate $11 : Loop-unique 1
    <[Tensor (2) i32]> $13 = alloca
    <[Tensor (2) i32]> $14 = [$11, $8]
    <[Tensor (2) i32]> $15 : local store [$13 <- $14]
    <i32> $16 = const 0
    <*i32> $17 = shift ptr [$13 + $16]
    <i32> $18 = local load [$17]
    <i32> $19 = const 1
    <*i32> $20 = shift ptr [$13 + $19]
    <i32> $21 = local load [$20]
    <*struct[none]{0(data_ptr, at 0B): *i64}> $22 = argaddr[0]
    <*i64> $23 = external_ptr $22, [$18, $21] element_dim=0 layout=AOS is_grad=false
    <i64> $24 = global load $23
    <[Tensor (2) i32]> $25 = alloca
    <[Tensor (2) i32]> $26 = [$11, $8]
    <[Tensor (2) i32]> $27 : local store [$25 <- $26]
    <i32> $28 = const 0
    <*i32> $29 = shift ptr [$25 + $28]
    <i32> $30 = local load [$29]
    <i32> $31 = const 1
    <*i32> $32 = shift ptr [$25 + $31]
    <i32> $33 = local load [$32]
    <*i64> $34 = global ptr [S6place<i64>], index [$30, $33] activate=true
    $35 : global store [$34 <- $24]
  }
}
[I 04/20/23 19:15:55.839 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Simplified I:
kernel {
  <i32> $0 = const 1
  <i32> $1 = const 0
  <i32> $2 = external_tensor_shape_along_axis 0, arg_id 0
  <i32> $3 = external_tensor_shape_along_axis 1, arg_id 0
  <i32> $4 = mul $2 $3
  $5 : for in range($1, $4) block_dim=adaptive {
    <i32> $6 = loop $5 index 0
    <i32> $7 = mod $6 $3
    decorate $7 : Loop-unique 1
    <i32> $9 = div $6 $3
    <i32> $10 = mod $9 $2
    decorate $10 : Loop-unique 1
    <[Tensor (2) i32]> $12 = alloca
    <[Tensor (2) i32]> $13 = [$10, $7]
    <[Tensor (2) i32]> $14 : local store [$12 <- $13]
    <*i32> $15 = shift ptr [$12 + $1]
    <i32> $16 = local load [$15]
    <*i32> $17 = shift ptr [$12 + $0]
    <i32> $18 = local load [$17]
    <*struct[none]{0(data_ptr, at 0B): *i64}> $19 = argaddr[0]
    <*i64> $20 = external_ptr $19, [$16, $18] element_dim=0 layout=AOS is_grad=false
    <i64> $21 = global load $20
    <[Tensor (2) i32]> $22 = alloca
    <[Tensor (2) i32]> $23 : local store [$22 <- $13]
    <*i32> $24 = shift ptr [$22 + $1]
    <i32> $25 = local load [$24]
    <*i32> $26 = shift ptr [$22 + $0]
    <i32> $27 = local load [$26]
    <*i64> $28 = global ptr [S6place<i64>], index [$25, $27] activate=true
    $29 : global store [$28 <- $21]
  }
}
[I 04/20/23 19:15:55.839 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Scalarized:
kernel {
  <i32> $0 = const 0
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <i32> $2 = external_tensor_shape_along_axis 1, arg_id 0
  <i32> $3 = mul $1 $2
  $4 : for in range($0, $3) block_dim=adaptive {
    <i32> $5 = loop $4 index 0
    <i32> $6 = mod $5 $2
    decorate $6 : Loop-unique 1
    <i32> $8 = div $5 $2
    <i32> $9 = mod $8 $1
    decorate $9 : Loop-unique 1
    <i32> $11 = alloca
    <i32> $12 = alloca
    $13 : local store [$11 <- $9]
    $14 : local store [$12 <- $6]
    <i32> $15 = local load [$11]
    <i32> $16 = local load [$12]
    <*struct[none]{0(data_ptr, at 0B): *i64}> $17 = argaddr[0]
    <*i64> $18 = external_ptr $17, [$15, $16] element_dim=0 layout=AOS is_grad=false
    <i64> $19 = global load $18
    <i32> $20 = alloca
    <i32> $21 = alloca
    $22 : local store [$20 <- $9]
    $23 : local store [$21 <- $6]
    <i32> $24 = local load [$20]
    <i32> $25 = local load [$21]
    <*i64> $26 = global ptr [S6place<i64>], index [$24, $25] activate=true
    $27 : global store [$26 <- $19]
  }
}
[I 04/20/23 19:15:55.839 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Access flagged I:
kernel {
  <i32> $0 = const 0
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <i32> $2 = external_tensor_shape_along_axis 1, arg_id 0
  <i32> $3 = mul $1 $2
  $4 : for in range($0, $3) block_dim=adaptive {
    <i32> $5 = loop $4 index 0
    <i32> $6 = mod $5 $2
    decorate $6 : Loop-unique 1
    <i32> $8 = div $5 $2
    <i32> $9 = mod $8 $1
    decorate $9 : Loop-unique 1
    <i32> $11 = alloca
    <i32> $12 = alloca
    $13 : local store [$11 <- $9]
    $14 : local store [$12 <- $6]
    <i32> $15 = local load [$11]
    <i32> $16 = local load [$12]
    <*struct[none]{0(data_ptr, at 0B): *i64}> $17 = argaddr[0]
    <*i64> $18 = external_ptr $17, [$15, $16] element_dim=0 layout=AOS is_grad=false
    <i64> $19 = global load $18
    <i32> $20 = alloca
    <i32> $21 = alloca
    $22 : local store [$20 <- $9]
    $23 : local store [$21 <- $6]
    <i32> $24 = local load [$20]
    <i32> $25 = local load [$21]
    <*i64> $26 = global ptr [S6place<i64>], index [$24, $25] activate=true
    $27 : global store [$26 <- $19]
  }
}
[I 04/20/23 19:15:55.840 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Simplified II:
kernel {
  <i32> $0 = const 0
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <i32> $2 = external_tensor_shape_along_axis 1, arg_id 0
  <i32> $3 = mul $1 $2
  $4 : for in range($0, $3) block_dim=adaptive {
    <i32> $5 = loop $4 index 0
    <i32> $6 = mod $5 $2
    decorate $6 : Loop-unique 1
    <i32> $8 = div $5 $2
    <i32> $9 = mod $8 $1
    decorate $9 : Loop-unique 1
    <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
    <*i64> $12 = external_ptr $11, [$9, $6] element_dim=0 layout=AOS is_grad=false
    <i64> $13 = global load $12
    <*i64> $14 = global ptr [S6place<i64>], index [$9, $6] activate=true
    $15 : global store [$14 <- $13]
  }
}
[I 04/20/23 19:15:55.840 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Offloaded:
kernel {
  $0 = offloaded
  body {
    <i32> $1 = const 0
    <i32> $2 = external_tensor_shape_along_axis 0, arg_id 0
    <*i32> $3 = global tmp var (offset = 8 B)
    $4 : global store [$3 <- $2]
    <i32> $5 = external_tensor_shape_along_axis 1, arg_id 0
    <*i32> $6 = global tmp var (offset = 4 B)
    $7 : global store [$6 <- $5]
    <i32> $8 = mul $2 $5
    <*i32> $9 = global tmp var (offset = 0 B)
    $10 : global store [$9 <- $8]
  }
  $11 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
  body {
    <i32> $12 = loop $11 index 0
    <*i32> $13 = global tmp var (offset = 4 B)
    <i32> $14 = global load $13
    <i32> $15 = mod $12 $14
    decorate $15 : Loop-unique 1
    <*i32> $17 = global tmp var (offset = 4 B)
    <i32> $18 = global load $17
    <i32> $19 = div $12 $18
    <*i32> $20 = global tmp var (offset = 8 B)
    <i32> $21 = global load $20
    <i32> $22 = mod $19 $21
    decorate $22 : Loop-unique 1
    <*struct[none]{0(data_ptr, at 0B): *i64}> $24 = argaddr[0]
    <*i64> $25 = external_ptr $24, [$22, $15] element_dim=0 layout=AOS is_grad=false
    <i64> $26 = global load $25
    <*i64> $27 = global ptr [S6place<i64>], index [$22, $15] activate=true
    $28 : global store [$27 <- $26]
  }
}
[I 04/20/23 19:15:55.840 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Optimized by CFG:
kernel {
  $0 = offloaded
  body {
    <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
    <*i32> $2 = global tmp var (offset = 8 B)
    $3 : global store [$2 <- $1]
    <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
    <*i32> $5 = global tmp var (offset = 4 B)
    $6 : global store [$5 <- $4]
    <i32> $7 = mul $1 $4
    <*i32> $8 = global tmp var (offset = 0 B)
    $9 : global store [$8 <- $7]
  }
  $10 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
  body {
    <i32> $11 = loop $10 index 0
    <*i32> $12 = global tmp var (offset = 4 B)
    <i32> $13 = global load $12
    <i32> $14 = mod $11 $13
    decorate $14 : Loop-unique 1
    <*i32> $16 = global tmp var (offset = 4 B)
    <i32> $17 = global load $16
    <i32> $18 = div $11 $17
    <*i32> $19 = global tmp var (offset = 8 B)
    <i32> $20 = global load $19
    <i32> $21 = mod $18 $20
    decorate $21 : Loop-unique 1
    <*struct[none]{0(data_ptr, at 0B): *i64}> $23 = argaddr[0]
    <*i64> $24 = external_ptr $23, [$21, $14] element_dim=0 layout=AOS is_grad=false
    <i64> $25 = global load $24
    <*i64> $26 = global ptr [S6place<i64>], index [$21, $14] activate=true
    $27 : global store [$26 <- $25]
  }
}
[I 04/20/23 19:15:55.841 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Access flagged II:
kernel {
  $0 = offloaded
  body {
    <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
    <*i32> $2 = global tmp var (offset = 8 B)
    $3 : global store [$2 <- $1]
    <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
    <*i32> $5 = global tmp var (offset = 4 B)
    $6 : global store [$5 <- $4]
    <i32> $7 = mul $1 $4
    <*i32> $8 = global tmp var (offset = 0 B)
    $9 : global store [$8 <- $7]
  }
  $10 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
  body {
    <i32> $11 = loop $10 index 0
    <*i32> $12 = global tmp var (offset = 4 B)
    <i32> $13 = global load $12
    <i32> $14 = mod $11 $13
    decorate $14 : Loop-unique 1
    <*i32> $16 = global tmp var (offset = 4 B)
    <i32> $17 = global load $16
    <i32> $18 = div $11 $17
    <*i32> $19 = global tmp var (offset = 8 B)
    <i32> $20 = global load $19
    <i32> $21 = mod $18 $20
    decorate $21 : Loop-unique 1
    <*struct[none]{0(data_ptr, at 0B): *i64}> $23 = argaddr[0]
    <*i64> $24 = external_ptr $23, [$21, $14] element_dim=0 layout=AOS is_grad=false
    <i64> $25 = global load $24
    <*i64> $26 = global ptr [S6place<i64>], index [$21, $14] activate=true
    $27 : global store [$26 <- $25]
  }
}
[I 04/20/23 19:15:55.841 1820159] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Simplified III:
kernel {
  $0 = offloaded
  body {
    <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
    <*i32> $2 = global tmp var (offset = 8 B)
    $3 : global store [$2 <- $1]
    <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
    <*i32> $5 = global tmp var (offset = 4 B)
    $6 : global store [$5 <- $4]
    <i32> $7 = mul $1 $4
    <*i32> $8 = global tmp var (offset = 0 B)
    $9 : global store [$8 <- $7]
  }
  $10 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
  body {
    <i32> $11 = loop $10 index 0
    <*i32> $12 = global tmp var (offset = 4 B)
    <i32> $13 = global load $12
    <i32> $14 = mod $11 $13
    decorate $14 : Loop-unique 1
    <i32> $16 = div $11 $13
    <*i32> $17 = global tmp var (offset = 8 B)
    <i32> $18 = global load $17
    <i32> $19 = mod $16 $18
    decorate $19 : Loop-unique 1
    <*struct[none]{0(data_ptr, at 0B): *i64}> $21 = argaddr[0]
    <*i64> $22 = external_ptr $21, [$19, $14] element_dim=0 layout=AOS is_grad=false
    <i64> $23 = global load $22
    <*i64> $24 = global ptr [S6place<i64>], index [$19, $14] activate=true
    $25 : global store [$24 <- $23]
  }
}
[I 04/20/23 19:15:55.867 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Start offload_to_executable:
kernel {
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
  <i64> $13 = global load $12
  <*i64> $14 = global ptr [S6place<i64>], index [$9, $4] activate=true
  $15 : global store [$14 <- $13]
}
}
[I 04/20/23 19:15:55.867 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Detect read-only accesses:
kernel {
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
  <i64> $13 = global load $12
  <*i64> $14 = global ptr [S6place<i64>], index [$9, $4] activate=true
  $15 : global store [$14 <- $13]
}
}
[I 04/20/23 19:15:55.867 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Atomics demoted I:
kernel {
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
  <i64> $13 = global load $12
  <*i64> $14 = global ptr [S6place<i64>], index [$9, $4] activate=true
  $15 : global store [$14 <- $13]
}
[I 04/20/23 19:15:55.867 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Start offload_to_executable:
}
kernel {
[I 04/20/23 19:15:55.867 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Cache loop-invariant global vars:
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
kernel {
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
[I 04/20/23 19:15:55.867 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Detect read-only accesses:
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
  <i64> $13 = global load $12
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <*i64> $14 = global ptr [S6place<i64>], index [$9, $4] activate=true
  $15 : global store [$14 <- $13]
}
}
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
[I 04/20/23 19:15:55.867 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Dense struct-for demoted:
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
kernel {
[I 04/20/23 19:15:55.867 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Atomics demoted I:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
[I 04/20/23 19:15:55.867 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Cache loop-invariant global vars:
  <i64> $13 = global load $12
  <*i64> $14 = global ptr [S6place<i64>], index [$9, $4] activate=true
  $15 : global store [$14 <- $13]
}
}
kernel {
$0 = offloaded
[I 04/20/23 19:15:55.867 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] No-access mesh-for demoted:
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
kernel {
[I 04/20/23 19:15:55.867 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Dense struct-for demoted:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
  <i64> $13 = global load $12
  <*i64> $14 = global ptr [S6place<i64>], index [$9, $4] activate=true
  $15 : global store [$14 <- $13]
}
}
kernel {
$0 = offloaded
body {
[I 04/20/23 19:15:55.868 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Make thread local:
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
kernel {
[I 04/20/23 19:15:55.868 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] No-access mesh-for demoted:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
  <i64> $13 = global load $12
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
  <*i64> $14 = global ptr [S6place<i64>], index [$9, $4] activate=true
  $15 : global store [$14 <- $13]
}
}
[I 04/20/23 19:15:55.868 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Make thread local:
kernel {
[I 04/20/23 19:15:55.868 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Make mesh thread local:
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
kernel {
[I 04/20/23 19:15:55.868 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Make mesh thread local:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
  <i64> $13 = global load $12
  <*i64> $14 = global ptr [S6place<i64>], index [$9, $4] activate=true
  $15 : global store [$14 <- $13]
}
}
kernel {
[I 04/20/23 19:15:55.868 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Make mesh block local:
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
kernel {
[I 04/20/23 19:15:55.868 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Make mesh block local:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
  <i64> $13 = global load $12
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
  <*i64> $14 = global ptr [S6place<i64>], index [$9, $4] activate=true
  $15 : global store [$14 <- $13]
}
}
[I 04/20/23 19:15:55.868 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Simplified X:
kernel {
$0 = offloaded
body {
[I 04/20/23 19:15:55.868 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Simplified X:
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
kernel {
[I 04/20/23 19:15:55.868 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Make block local:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
[I 04/20/23 19:15:55.868 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Demote mesh statements:
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
  <i64> $13 = global load $12
  <*i64> $14 = global ptr [S6place<i64>], index [$9, $4] activate=true
  $15 : global store [$14 <- $13]
}
}
kernel {
[I 04/20/23 19:15:55.868 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Make block local:
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
[I 04/20/23 19:15:55.868 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Atomics demoted II:
kernel {
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
[I 04/20/23 19:15:55.868 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Remove range assumption:
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
  <i64> $13 = global load $12
  <*i64> $14 = global ptr [S6place<i64>], index [$9, $4] activate=true
  $15 : global store [$14 <- $13]
}
}
kernel {
$0 = offloaded
[I 04/20/23 19:15:55.869 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Demote mesh statements:
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
kernel {
[I 04/20/23 19:15:55.869 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Remove loop_unique:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
  <i64> $13 = global load $12
  <*i64> $14 = global ptr [S6place<i64>], index [$9, $4] activate=true
  $15 : global store [$14 <- $13]
}
}
[I 04/20/23 19:15:55.869 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Atomics demoted II:
kernel {
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
[I 04/20/23 19:15:55.869 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Simplified before lower access:
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
  <i64> $13 = global load $12
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
  <*i64> $14 = global ptr [S6place<i64>], index [$9, $4] activate=true
[I 04/20/23 19:15:55.869 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Access lowered:
  $15 : global store [$14 <- $13]
}
}
kernel {
$0 = offloaded
body {
[I 04/20/23 19:15:55.869 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Remove range assumption:
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
kernel {
[I 04/20/23 19:15:55.869 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] DIE:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
[I 04/20/23 19:15:55.869 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Access flagged III:
  <i64> $13 = global load $12
  <*i64> $14 = global ptr [S6place<i64>], index [$9, $4] activate=true
  $15 : global store [$14 <- $13]
}
}
kernel {
[I 04/20/23 19:15:55.869 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Remove loop_unique:
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
kernel {
[I 04/20/23 19:15:55.869 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Operations demoted:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
  <i64> $13 = global load $12
  <*i64> $14 = global ptr [S6place<i64>], index [$9, $4] activate=true
  $15 : global store [$14 <- $13]
}
}
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
[I 04/20/23 19:15:55.869 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Simplified before lower access:
kernel {
[I 04/20/23 19:15:55.869 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Simplified IV:
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
  <i64> $13 = global load $12
[I 04/20/23 19:15:55.869 1820179] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Bit struct stores optimized:
  <*i64> $14 = global ptr [S6place<i64>], index [$9, $4] activate=true
  $15 : global store [$14 <- $13]
}
}
kernel {
$0 = offloaded
body {
  <i32> $1 = external_tensor_shape_along_axis 0, arg_id 0
  <*i32> $2 = global tmp var (offset = 8 B)
  $3 : global store [$2 <- $1]
  <i32> $4 = external_tensor_shape_along_axis 1, arg_id 0
[I 04/20/23 19:15:55.869 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Access lowered:
  <*i32> $5 = global tmp var (offset = 4 B)
  $6 : global store [$5 <- $4]
  <i32> $7 = mul $1 $4
  <*i32> $8 = global tmp var (offset = 0 B)
  $9 : global store [$8 <- $7]
}
}
kernel {
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
  <i64> $13 = global load $12
  <*i64> $14 = global ptr [S6place<i64>], index [$9, $4] activate=true
  <*gen> $15 = get root [S0root][root]
  <i32> $16 = linearized(ind {}, stride {})
  <*gen> $17 = [S0root][root]::lookup($15, $16) activate = false
  <*gen> $18 = get child [S0root->S5dense] $17
  <i32> $19 = const 0
  <i32> $20 = bit_shr $9 $19
  <i32> $21 = const 0
  <i32> $22 = bit_shr $4 $21
  <i32> $23 = linearized(ind {$20, $22}, stride {8192, 3})
  <*gen> $24 = [S5dense][dense]::lookup($18, $23) activate = false
  <*i64> $25 = get child [S5dense->S6place<i64>] $24
  $26 : global store [$25 <- $13]
}
}
[I 04/20/23 19:15:55.870 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] DIE:
kernel {
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
  <i64> $13 = global load $12
  <*i64> $14 = global ptr [S6place<i64>], index [$9, $4] activate=true
  <*gen> $15 = get root [S0root][root]
  <i32> $16 = linearized(ind {}, stride {})
  <*gen> $17 = [S0root][root]::lookup($15, $16) activate = false
  <*gen> $18 = get child [S0root->S5dense] $17
  <i32> $19 = const 0
  <i32> $20 = bit_shr $9 $19
  <i32> $21 = const 0
  <i32> $22 = bit_shr $4 $21
  <i32> $23 = linearized(ind {$20, $22}, stride {8192, 3})
  <*gen> $24 = [S5dense][dense]::lookup($18, $23) activate = false
  <*i64> $25 = get child [S5dense->S6place<i64>] $24
  $26 : global store [$25 <- $13]
}
}
[I 04/20/23 19:15:55.870 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Access flagged III:
kernel {
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
  <i64> $13 = global load $12
  <*i64> $14 = global ptr [S6place<i64>], index [$9, $4] activate=false
  <*gen> $15 = get root [S0root][root]
  <i32> $16 = linearized(ind {}, stride {})
  <*gen> $17 = [S0root][root]::lookup($15, $16) activate = false
  <*gen> $18 = get child [S0root->S5dense] $17
  <i32> $19 = const 0
  <i32> $20 = bit_shr $9 $19
  <i32> $21 = const 0
  <i32> $22 = bit_shr $4 $21
  <i32> $23 = linearized(ind {$20, $22}, stride {8192, 3})
  <*gen> $24 = [S5dense][dense]::lookup($18, $23) activate = false
  <*i64> $25 = get child [S5dense->S6place<i64>] $24
  $26 : global store [$25 <- $13]
}
}
[I 04/20/23 19:15:55.870 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Operations demoted:
kernel {
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
  <i64> $13 = global load $12
  <*i64> $14 = global ptr [S6place<i64>], index [$9, $4] activate=false
  <*gen> $15 = get root [S0root][root]
  <i32> $16 = linearized(ind {}, stride {})
  <*gen> $17 = [S0root][root]::lookup($15, $16) activate = false
  <*gen> $18 = get child [S0root->S5dense] $17
  <i32> $19 = const 0
  <u32> $20 = reinterpret_cast_bits<u32> $9
  <u32> $21 = cast_value<u32> $19
  <u32> $22 = bit_sar $20 $21
  <i32> $23 = reinterpret_cast_bits<i32> $22
  <i32> $24 = const 0
  <u32> $25 = reinterpret_cast_bits<u32> $4
  <u32> $26 = cast_value<u32> $24
  <u32> $27 = bit_sar $25 $26
  <i32> $28 = reinterpret_cast_bits<i32> $27
  <i32> $29 = linearized(ind {$23, $28}, stride {8192, 3})
  <*gen> $30 = [S5dense][dense]::lookup($18, $29) activate = false
  <*i64> $31 = get child [S5dense->S6place<i64>] $30
  $32 : global store [$31 <- $13]
}
}
[I 04/20/23 19:15:55.870 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Simplified IV:
kernel {
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
  <i64> $13 = global load $12
  <*gen> $14 = get root [S0root][root]
  <i32> $15 = const 0
  <*gen> $16 = [S0root][root]::lookup($14, $15) activate = false
  <*gen> $17 = get child [S0root->S5dense] $16
  <i32> $18 = const 3
  <i32> $19 = mul $9 $18
  <i32> $20 = add $4 $19
  <*gen> $21 = [S5dense][dense]::lookup($17, $20) activate = false
  <*i64> $22 = get child [S5dense->S6place<i64>] $21
  $23 : global store [$22 <- $13]
}
}
[I 04/20/23 19:15:55.870 1820177] [compile_to_offloads.cpp:operator()@23] [torch2ti_c74_1] Bit struct stores optimized:
kernel {
$0 = offloaded range_for(0, tmp(offset=0B)) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*i32> $2 = global tmp var (offset = 4 B)
  <i32> $3 = global load $2
  <i32> $4 = mod $1 $3
  decorate $4 : Loop-unique 1
  <i32> $6 = div $1 $3
  <*i32> $7 = global tmp var (offset = 8 B)
  <i32> $8 = global load $7
  <i32> $9 = mod $6 $8
  decorate $9 : Loop-unique 1
  <*struct[none]{0(data_ptr, at 0B): *i64}> $11 = argaddr[0]
  <*i64> $12 = external_ptr $11, [$9, $4] element_dim=0 layout=AOS is_grad=false
  <i64> $13 = global load $12
  <*gen> $14 = get root [S0root][root]
  <i32> $15 = const 0
  <*gen> $16 = [S0root][root]::lookup($14, $15) activate = false
  <*gen> $17 = get child [S0root->S5dense] $16
  <i32> $18 = const 3
  <i32> $19 = mul $9 $18
  <i32> $20 = add $4 $19
  <*gen> $21 = [S5dense][dense]::lookup($17, $20) activate = false
  <*i64> $22 = get child [S5dense->S6place<i64>] $21
  $23 : global store [$22 <- $13]
}
}
[I 04/20/23 19:15:55.906 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Initial IR:
kernel {
  $0 : for @tmp0, @tmp1 in S8place<f32> noneblock_dim=adaptive {
    $1 = alloca @tmp2
    @tmp2 = 0.0
    $3 = alloca @tmp3
    @tmp3 = [@tmp0, @tmp1] (dt=[Tensor (2) i32])
    #@tmp4 (snode=S8place<f32>)[@tmp3[0], @tmp3[1]] = @tmp2
  }
}
[I 04/20/23 19:15:55.906 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Lowered:
kernel {
  $0 : struct for in S7dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <f32> $3 = alloca
    <f32> $4 = const 0.0
    $5 : local store [$3 <- $4]
    <[Tensor (2) i32]> $6 = alloca
    <[Tensor (2) i32]> $7 = [$1, $2]
    $8 : local store [$6 <- $7]
    <f32> $9 = local load [$3]
    <i32> $10 = const 0
    <*i32> $11 = shift ptr [$6 + $10]
    <i32> $12 = local load [$11]
    <i32> $13 = const 1
    <*i32> $14 = shift ptr [$6 + $13]
    <i32> $15 = local load [$14]
    <f32> $16 = global ptr [S8place<f32>], index [$12, $15] activate=true
    $17 : global store [$16 <- $9]
  }
}
[I 04/20/23 19:15:55.906 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Immutable local vars eliminated:
kernel {
  $0 : struct for in S7dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <f32> $3 = const 0.0
    <[Tensor (2) i32]> $4 = alloca
    <[Tensor (2) i32]> $5 = [$1, $2]
    $6 : local store [$4 <- $5]
    <i32> $7 = const 0
    <*i32> $8 = shift ptr [$4 + $7]
    <i32> $9 = local load [$8]
    <i32> $10 = const 1
    <*i32> $11 = shift ptr [$4 + $10]
    <i32> $12 = local load [$11]
    <f32> $13 = global ptr [S8place<f32>], index [$9, $12] activate=true
    $14 : global store [$13 <- $3]
  }
}
[I 04/20/23 19:15:55.906 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Typechecked:
kernel {
  $0 : struct for in S7dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <f32> $3 = const 0.0
    <[Tensor (2) i32]> $4 = alloca
    <[Tensor (2) i32]> $5 = [$1, $2]
    <[Tensor (2) i32]> $6 : local store [$4 <- $5]
    <i32> $7 = const 0
    <*i32> $8 = shift ptr [$4 + $7]
    <i32> $9 = local load [$8]
    <i32> $10 = const 1
    <*i32> $11 = shift ptr [$4 + $10]
    <i32> $12 = local load [$11]
    <*f32> $13 = global ptr [S8place<f32>], index [$9, $12] activate=true
    $14 : global store [$13 <- $3]
  }
}
[I 04/20/23 19:15:55.907 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Bit Loop Vectorized:
kernel {
  $0 : struct for in S7dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <f32> $3 = const 0.0
    <[Tensor (2) i32]> $4 = alloca
    <[Tensor (2) i32]> $5 = [$1, $2]
    <[Tensor (2) i32]> $6 : local store [$4 <- $5]
    <i32> $7 = const 0
    <*i32> $8 = shift ptr [$4 + $7]
    <i32> $9 = local load [$8]
    <i32> $10 = const 1
    <*i32> $11 = shift ptr [$4 + $10]
    <i32> $12 = local load [$11]
    <*f32> $13 = global ptr [S8place<f32>], index [$9, $12] activate=true
    $14 : global store [$13 <- $3]
  }
}
[I 04/20/23 19:15:55.907 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Matrix ptr lowered:
kernel {
  $0 : struct for in S7dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <f32> $3 = const 0.0
    <[Tensor (2) i32]> $4 = alloca
    <[Tensor (2) i32]> $5 = [$1, $2]
    <[Tensor (2) i32]> $6 : local store [$4 <- $5]
    <i32> $7 = const 0
    <*i32> $8 = shift ptr [$4 + $7]
    <i32> $9 = local load [$8]
    <i32> $10 = const 1
    <*i32> $11 = shift ptr [$4 + $10]
    <i32> $12 = local load [$11]
    <*f32> $13 = global ptr [S8place<f32>], index [$9, $12] activate=true
    $14 : global store [$13 <- $3]
  }
}
[I 04/20/23 19:15:55.907 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Simplified I:
kernel {
  <i32> $0 = const 1
  <i32> $1 = const 0
  <f32> $2 = const 0.0
  $3 : struct for in S7dense noneblock_dim=adaptive {
    <i32> $4 = loop $3 index 0
    <i32> $5 = loop $3 index 1
    <[Tensor (2) i32]> $6 = alloca
    <[Tensor (2) i32]> $7 = [$4, $5]
    <[Tensor (2) i32]> $8 : local store [$6 <- $7]
    <*i32> $9 = shift ptr [$6 + $1]
    <i32> $10 = local load [$9]
    <*i32> $11 = shift ptr [$6 + $0]
    <i32> $12 = local load [$11]
    <*f32> $13 = global ptr [S8place<f32>], index [$10, $12] activate=true
    $14 : global store [$13 <- $2]
  }
}
[I 04/20/23 19:15:55.907 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Scalarized:
kernel {
  <f32> $0 = const 0.0
  $1 : struct for in S7dense noneblock_dim=adaptive {
    <i32> $2 = loop $1 index 0
    <i32> $3 = loop $1 index 1
    <i32> $4 = alloca
    <i32> $5 = alloca
    $6 : local store [$4 <- $2]
    $7 : local store [$5 <- $3]
    <i32> $8 = local load [$4]
    <i32> $9 = local load [$5]
    <*f32> $10 = global ptr [S8place<f32>], index [$8, $9] activate=true
    $11 : global store [$10 <- $0]
  }
}
[I 04/20/23 19:15:55.907 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Access flagged I:
kernel {
  <f32> $0 = const 0.0
  $1 : struct for in S7dense noneblock_dim=adaptive {
    <i32> $2 = loop $1 index 0
    <i32> $3 = loop $1 index 1
    <i32> $4 = alloca
    <i32> $5 = alloca
    $6 : local store [$4 <- $2]
    $7 : local store [$5 <- $3]
    <i32> $8 = local load [$4]
    <i32> $9 = local load [$5]
    <*f32> $10 = global ptr [S8place<f32>], index [$8, $9] activate=true
    $11 : global store [$10 <- $0]
  }
}
[I 04/20/23 19:15:55.907 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Simplified II:
kernel {
  <f32> $0 = const 0.0
  $1 : struct for in S7dense noneblock_dim=adaptive {
    <i32> $2 = loop $1 index 0
    <i32> $3 = loop $1 index 1
    <*f32> $4 = global ptr [S8place<f32>], index [$2, $3] activate=true
    $5 : global store [$4 <- $0]
  }
}
[I 04/20/23 19:15:55.907 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Offloaded:
kernel {
  $0 = offloaded
  body {
    <f32> $1 = const 0.0
  }
  $2 = offloaded struct_for(S7dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $3 = loop $2 index 0
    <i32> $4 = loop $2 index 1
    <*f32> $5 = global ptr [S8place<f32>], index [$3, $4] activate=true
    <f32> $6 = const 0.0
    $7 : global store [$5 <- $6]
  }
}
[I 04/20/23 19:15:55.907 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Optimized by CFG:
kernel {
  $0 = offloaded
  body {
  }
  $1 = offloaded struct_for(S7dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $2 = loop $1 index 0
    <i32> $3 = loop $1 index 1
    <*f32> $4 = global ptr [S8place<f32>], index [$2, $3] activate=true
    <f32> $5 = const 0.0
    $6 : global store [$4 <- $5]
  }
}
[I 04/20/23 19:15:55.907 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Access flagged II:
kernel {
  $0 = offloaded
  body {
  }
  $1 = offloaded struct_for(S7dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $2 = loop $1 index 0
    <i32> $3 = loop $1 index 1
    <*f32> $4 = global ptr [S8place<f32>], index [$2, $3] activate=false
    <f32> $5 = const 0.0
    $6 : global store [$4 <- $5]
  }
}
[I 04/20/23 19:15:55.907 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Simplified III:
kernel {
  $0 = offloaded struct_for(S7dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <*f32> $3 = global ptr [S8place<f32>], index [$1, $2] activate=false
    <f32> $4 = const 0.0
    $5 : global store [$3 <- $4]
  }
}
[I 04/20/23 19:15:55.907 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Start offload_to_executable:
kernel {
$0 = offloaded struct_for(S7dense) grid_dim=2624 block_dim=128 bls=none
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = loop $0 index 1
  <*f32> $3 = global ptr [S8place<f32>], index [$1, $2] activate=false
  <f32> $4 = const 0.0
  $5 : global store [$3 <- $4]
}
}
[I 04/20/23 19:15:55.907 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Detect read-only accesses:
kernel {
$0 = offloaded struct_for(S7dense) grid_dim=2624 block_dim=128 bls=none
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = loop $0 index 1
  <*f32> $3 = global ptr [S8place<f32>], index [$1, $2] activate=false
  <f32> $4 = const 0.0
  $5 : global store [$3 <- $4]
}
}
[I 04/20/23 19:15:55.907 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Atomics demoted I:
kernel {
$0 = offloaded struct_for(S7dense) grid_dim=2624 block_dim=128 bls=none
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = loop $0 index 1
  <*f32> $3 = global ptr [S8place<f32>], index [$1, $2] activate=false
  <f32> $4 = const 0.0
  $5 : global store [$3 <- $4]
}
}
[I 04/20/23 19:15:55.907 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Cache loop-invariant global vars:
kernel {
$0 = offloaded struct_for(S7dense) grid_dim=2624 block_dim=128 bls=none
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = loop $0 index 1
  <*f32> $3 = global ptr [S8place<f32>], index [$1, $2] activate=false
  <f32> $4 = const 0.0
  $5 : global store [$3 <- $4]
}
}
[I 04/20/23 19:15:55.907 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Dense struct-for demoted:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = const 0
  <i32> $3 = loop $0 index 0
  <i32> $4 = const 0
  <i32> $5 = bit_shr $3 $4
  <i32> $6 = const 3
  <i32> $7 = div $5 $6
  <i32> $8 = const 1
  <i32> $9 = mul $7 $8
  <i32> $10 = add $1 $9
  <i32> $11 = const 3
  <i32> $12 = mod $5 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $12 $13
  <i32> $15 = const 1
  <i32> $16 = mul $14 $15
  <i32> $17 = add $2 $16
  <*f32> $18 = global ptr [S8place<f32>], index [$10, $17] activate=false
  <f32> $19 = const 0.0
  $20 : global store [$18 <- $19]
}
}
[I 04/20/23 19:15:55.908 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] No-access mesh-for demoted:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = const 0
  <i32> $3 = loop $0 index 0
  <i32> $4 = const 0
  <i32> $5 = bit_shr $3 $4
  <i32> $6 = const 3
  <i32> $7 = div $5 $6
  <i32> $8 = const 1
  <i32> $9 = mul $7 $8
  <i32> $10 = add $1 $9
  <i32> $11 = const 3
  <i32> $12 = mod $5 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $12 $13
  <i32> $15 = const 1
  <i32> $16 = mul $14 $15
  <i32> $17 = add $2 $16
  <*f32> $18 = global ptr [S8place<f32>], index [$10, $17] activate=false
  <f32> $19 = const 0.0
  $20 : global store [$18 <- $19]
}
}
[I 04/20/23 19:15:55.908 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Make thread local:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = const 0
  <i32> $3 = loop $0 index 0
  <i32> $4 = const 0
  <i32> $5 = bit_shr $3 $4
  <i32> $6 = const 3
  <i32> $7 = div $5 $6
  <i32> $8 = const 1
  <i32> $9 = mul $7 $8
  <i32> $10 = add $1 $9
  <i32> $11 = const 3
  <i32> $12 = mod $5 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $12 $13
  <i32> $15 = const 1
  <i32> $16 = mul $14 $15
  <i32> $17 = add $2 $16
  <*f32> $18 = global ptr [S8place<f32>], index [$10, $17] activate=false
  <f32> $19 = const 0.0
  $20 : global store [$18 <- $19]
}
}
[I 04/20/23 19:15:55.908 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Make mesh thread local:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = const 0
  <i32> $3 = loop $0 index 0
  <i32> $4 = const 0
  <i32> $5 = bit_shr $3 $4
  <i32> $6 = const 3
  <i32> $7 = div $5 $6
  <i32> $8 = const 1
  <i32> $9 = mul $7 $8
  <i32> $10 = add $1 $9
  <i32> $11 = const 3
  <i32> $12 = mod $5 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $12 $13
  <i32> $15 = const 1
  <i32> $16 = mul $14 $15
  <i32> $17 = add $2 $16
  <*f32> $18 = global ptr [S8place<f32>], index [$10, $17] activate=false
  <f32> $19 = const 0.0
  $20 : global store [$18 <- $19]
}
}
[I 04/20/23 19:15:55.908 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Make mesh block local:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = const 0
  <i32> $3 = loop $0 index 0
  <i32> $4 = const 0
  <i32> $5 = bit_shr $3 $4
  <i32> $6 = const 3
  <i32> $7 = div $5 $6
  <i32> $8 = const 1
  <i32> $9 = mul $7 $8
  <i32> $10 = add $1 $9
  <i32> $11 = const 3
  <i32> $12 = mod $5 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $12 $13
  <i32> $15 = const 1
  <i32> $16 = mul $14 $15
  <i32> $17 = add $2 $16
  <*f32> $18 = global ptr [S8place<f32>], index [$10, $17] activate=false
  <f32> $19 = const 0.0
  $20 : global store [$18 <- $19]
}
}
[I 04/20/23 19:15:55.908 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Simplified X:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S8place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 0.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:55.908 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Make block local:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S8place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 0.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:55.908 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Demote mesh statements:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S8place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 0.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:55.908 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Atomics demoted II:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S8place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 0.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:55.908 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Remove range assumption:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S8place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 0.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:55.908 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Remove loop_unique:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S8place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 0.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:55.908 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Simplified before lower access:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S8place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 0.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:55.908 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Access lowered:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S8place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 0.0
  <*gen> $7 = get root [S0root][root]
  <i32> $8 = linearized(ind {}, stride {})
  <*gen> $9 = [S0root][root]::lookup($7, $8) activate = false
  <*gen> $10 = get child [S0root->S7dense] $9
  <i32> $11 = const 0
  <i32> $12 = bit_shr $3 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $4 $13
  <i32> $15 = linearized(ind {$12, $14}, stride {8192, 3})
  <*gen> $16 = [S7dense][dense]::lookup($10, $15) activate = false
  <*f32> $17 = get child [S7dense->S8place<f32>] $16
  $18 : global store [$17 <- $6]
}
}
[I 04/20/23 19:15:55.908 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] DIE:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <f32> $5 = const 0.0
  <*gen> $6 = get root [S0root][root]
  <i32> $7 = linearized(ind {}, stride {})
  <*gen> $8 = [S0root][root]::lookup($6, $7) activate = false
  <*gen> $9 = get child [S0root->S7dense] $8
  <i32> $10 = const 0
  <i32> $11 = bit_shr $3 $10
  <i32> $12 = const 0
  <i32> $13 = bit_shr $4 $12
  <i32> $14 = linearized(ind {$11, $13}, stride {8192, 3})
  <*gen> $15 = [S7dense][dense]::lookup($9, $14) activate = false
  <*f32> $16 = get child [S7dense->S8place<f32>] $15
  $17 : global store [$16 <- $5]
}
}
[I 04/20/23 19:15:55.908 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Access flagged III:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <f32> $5 = const 0.0
  <*gen> $6 = get root [S0root][root]
  <i32> $7 = linearized(ind {}, stride {})
  <*gen> $8 = [S0root][root]::lookup($6, $7) activate = false
  <*gen> $9 = get child [S0root->S7dense] $8
  <i32> $10 = const 0
  <i32> $11 = bit_shr $3 $10
  <i32> $12 = const 0
  <i32> $13 = bit_shr $4 $12
  <i32> $14 = linearized(ind {$11, $13}, stride {8192, 3})
  <*gen> $15 = [S7dense][dense]::lookup($9, $14) activate = false
  <*f32> $16 = get child [S7dense->S8place<f32>] $15
  $17 : global store [$16 <- $5]
}
}
[I 04/20/23 19:15:55.909 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Operations demoted:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <f32> $5 = const 0.0
  <*gen> $6 = get root [S0root][root]
  <i32> $7 = linearized(ind {}, stride {})
  <*gen> $8 = [S0root][root]::lookup($6, $7) activate = false
  <*gen> $9 = get child [S0root->S7dense] $8
  <i32> $10 = const 0
  <u32> $11 = reinterpret_cast_bits<u32> $3
  <u32> $12 = cast_value<u32> $10
  <u32> $13 = bit_sar $11 $12
  <i32> $14 = reinterpret_cast_bits<i32> $13
  <i32> $15 = const 0
  <u32> $16 = reinterpret_cast_bits<u32> $4
  <u32> $17 = cast_value<u32> $15
  <u32> $18 = bit_sar $16 $17
  <i32> $19 = reinterpret_cast_bits<i32> $18
  <i32> $20 = linearized(ind {$14, $19}, stride {8192, 3})
  <*gen> $21 = [S7dense][dense]::lookup($9, $20) activate = false
  <*f32> $22 = get child [S7dense->S8place<f32>] $21
  $23 : global store [$22 <- $5]
}
}
[I 04/20/23 19:15:55.909 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Simplified IV:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <f32> $5 = const 0.0
  <*gen> $6 = get root [S0root][root]
  <i32> $7 = const 0
  <*gen> $8 = [S0root][root]::lookup($6, $7) activate = false
  <*gen> $9 = get child [S0root->S7dense] $8
  <i32> $10 = mul $3 $2
  <i32> $11 = add $4 $10
  <*gen> $12 = [S7dense][dense]::lookup($9, $11) activate = false
  <*f32> $13 = get child [S7dense->S8place<f32>] $12
  $14 : global store [$13 <- $5]
}
}
[I 04/20/23 19:15:55.909 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_0] Bit struct stores optimized:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <f32> $5 = const 0.0
  <*gen> $6 = get root [S0root][root]
  <i32> $7 = const 0
  <*gen> $8 = [S0root][root]::lookup($6, $7) activate = false
  <*gen> $9 = get child [S0root->S7dense] $8
  <i32> $10 = mul $3 $2
  <i32> $11 = add $4 $10
  <*gen> $12 = [S7dense][dense]::lookup($9, $11) activate = false
  <*f32> $13 = get child [S7dense->S8place<f32>] $12
  $14 : global store [$13 <- $5]
}
}
[I 04/20/23 19:15:55.943 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Initial IR:
kernel {
  $0 : for @tmp0 in S4place<f32> noneblock_dim=adaptive {
    $1 = alloca @tmp1
    @tmp1 = 0
    $3 = alloca @tmp2
    @tmp2 = [@tmp0] (dt=[Tensor (1) i32])
    #@tmp1 (snode=S4place<f32>)[@tmp2[0]] = @tmp1
  }
}
[I 04/20/23 19:15:55.943 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Lowered:
kernel {
  $0 : struct for in S3dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = alloca
    <i32> $3 = const 0
    $4 : local store [$2 <- $3]
    <[Tensor (1) i32]> $5 = alloca
    <[Tensor (1) i32]> $6 = [$1]
    $7 : local store [$5 <- $6]
    <i32> $8 = local load [$2]
    <i32> $9 = const 0
    <*i32> $10 = shift ptr [$5 + $9]
    <i32> $11 = local load [$10]
    <f32> $12 = global ptr [S4place<f32>], index [$11] activate=true
    $13 : global store [$12 <- $8]
  }
}
[I 04/20/23 19:15:55.943 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Immutable local vars eliminated:
kernel {
  $0 : struct for in S3dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = const 0
    <[Tensor (1) i32]> $3 = alloca
    <[Tensor (1) i32]> $4 = [$1]
    $5 : local store [$3 <- $4]
    <i32> $6 = const 0
    <*i32> $7 = shift ptr [$3 + $6]
    <i32> $8 = local load [$7]
    <f32> $9 = global ptr [S4place<f32>], index [$8] activate=true
    $10 : global store [$9 <- $2]
  }
}
[I 04/20/23 19:15:55.943 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Typechecked:
kernel {
  $0 : struct for in S3dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = const 0
    <[Tensor (1) i32]> $3 = alloca
    <[Tensor (1) i32]> $4 = [$1]
    <[Tensor (1) i32]> $5 : local store [$3 <- $4]
    <i32> $6 = const 0
    <*i32> $7 = shift ptr [$3 + $6]
    <i32> $8 = local load [$7]
    <*f32> $9 = global ptr [S4place<f32>], index [$8] activate=true
    <f32> $10 = cast_value<f32> $2
    $11 : global store [$9 <- $10]
  }
}
[I 04/20/23 19:15:55.943 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Bit Loop Vectorized:
kernel {
  $0 : struct for in S3dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = const 0
    <[Tensor (1) i32]> $3 = alloca
    <[Tensor (1) i32]> $4 = [$1]
    <[Tensor (1) i32]> $5 : local store [$3 <- $4]
    <i32> $6 = const 0
    <*i32> $7 = shift ptr [$3 + $6]
    <i32> $8 = local load [$7]
    <*f32> $9 = global ptr [S4place<f32>], index [$8] activate=true
    <f32> $10 = cast_value<f32> $2
    $11 : global store [$9 <- $10]
  }
}
[I 04/20/23 19:15:55.944 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Matrix ptr lowered:
kernel {
  $0 : struct for in S3dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = const 0
    <[Tensor (1) i32]> $3 = alloca
    <[Tensor (1) i32]> $4 = [$1]
    <[Tensor (1) i32]> $5 : local store [$3 <- $4]
    <i32> $6 = const 0
    <*i32> $7 = shift ptr [$3 + $6]
    <i32> $8 = local load [$7]
    <*f32> $9 = global ptr [S4place<f32>], index [$8] activate=true
    <f32> $10 = cast_value<f32> $2
    $11 : global store [$9 <- $10]
  }
}
[I 04/20/23 19:15:55.944 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Simplified I:
kernel {
  <f32> $0 = const 0.0
  <i32> $1 = const 0
  $2 : struct for in S3dense noneblock_dim=adaptive {
    <i32> $3 = loop $2 index 0
    <[Tensor (1) i32]> $4 = alloca
    <[Tensor (1) i32]> $5 = [$3]
    <[Tensor (1) i32]> $6 : local store [$4 <- $5]
    <*i32> $7 = shift ptr [$4 + $1]
    <i32> $8 = local load [$7]
    <*f32> $9 = global ptr [S4place<f32>], index [$8] activate=true
    $10 : global store [$9 <- $0]
  }
}
[I 04/20/23 19:15:55.944 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Scalarized:
kernel {
  <f32> $0 = const 0.0
  $1 : struct for in S3dense noneblock_dim=adaptive {
    <i32> $2 = loop $1 index 0
    <i32> $3 = alloca
    $4 : local store [$3 <- $2]
    <i32> $5 = local load [$3]
    <*f32> $6 = global ptr [S4place<f32>], index [$5] activate=true
    $7 : global store [$6 <- $0]
  }
}
[I 04/20/23 19:15:55.944 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Access flagged I:
kernel {
  <f32> $0 = const 0.0
  $1 : struct for in S3dense noneblock_dim=adaptive {
    <i32> $2 = loop $1 index 0
    <i32> $3 = alloca
    $4 : local store [$3 <- $2]
    <i32> $5 = local load [$3]
    <*f32> $6 = global ptr [S4place<f32>], index [$5] activate=true
    $7 : global store [$6 <- $0]
  }
}
[I 04/20/23 19:15:55.944 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Simplified II:
kernel {
  <f32> $0 = const 0.0
  $1 : struct for in S3dense noneblock_dim=adaptive {
    <i32> $2 = loop $1 index 0
    <*f32> $3 = global ptr [S4place<f32>], index [$2] activate=true
    $4 : global store [$3 <- $0]
  }
}
[I 04/20/23 19:15:55.944 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Offloaded:
kernel {
  $0 = offloaded
  body {
    <f32> $1 = const 0.0
  }
  $2 = offloaded struct_for(S3dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $3 = loop $2 index 0
    <*f32> $4 = global ptr [S4place<f32>], index [$3] activate=true
    <f32> $5 = const 0.0
    $6 : global store [$4 <- $5]
  }
}
[I 04/20/23 19:15:55.944 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Optimized by CFG:
kernel {
  $0 = offloaded
  body {
  }
  $1 = offloaded struct_for(S3dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $2 = loop $1 index 0
    <*f32> $3 = global ptr [S4place<f32>], index [$2] activate=true
    <f32> $4 = const 0.0
    $5 : global store [$3 <- $4]
  }
}
[I 04/20/23 19:15:55.944 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Access flagged II:
kernel {
  $0 = offloaded
  body {
  }
  $1 = offloaded struct_for(S3dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $2 = loop $1 index 0
    <*f32> $3 = global ptr [S4place<f32>], index [$2] activate=false
    <f32> $4 = const 0.0
    $5 : global store [$3 <- $4]
  }
}
[I 04/20/23 19:15:55.944 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Simplified III:
kernel {
  $0 = offloaded struct_for(S3dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $1 = loop $0 index 0
    <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
    <f32> $3 = const 0.0
    $4 : global store [$2 <- $3]
  }
}
[I 04/20/23 19:15:55.944 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Start offload_to_executable:
kernel {
$0 = offloaded struct_for(S3dense) grid_dim=2624 block_dim=128 bls=none
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = const 0.0
  $4 : global store [$2 <- $3]
}
}
[I 04/20/23 19:15:55.944 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Detect read-only accesses:
kernel {
$0 = offloaded struct_for(S3dense) grid_dim=2624 block_dim=128 bls=none
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = const 0.0
  $4 : global store [$2 <- $3]
}
}
[I 04/20/23 19:15:55.944 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Atomics demoted I:
kernel {
$0 = offloaded struct_for(S3dense) grid_dim=2624 block_dim=128 bls=none
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = const 0.0
  $4 : global store [$2 <- $3]
}
}
[I 04/20/23 19:15:55.944 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Cache loop-invariant global vars:
kernel {
$0 = offloaded struct_for(S3dense) grid_dim=2624 block_dim=128 bls=none
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = const 0.0
  $4 : global store [$2 <- $3]
}
}
[I 04/20/23 19:15:55.944 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Dense struct-for demoted:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <i32> $4 = bit_shr $2 $3
  <i32> $5 = const 0
  <i32> $6 = bit_shr $4 $5
  <i32> $7 = const 1
  <i32> $8 = mul $6 $7
  <i32> $9 = add $1 $8
  <*f32> $10 = global ptr [S4place<f32>], index [$9] activate=false
  <f32> $11 = const 0.0
  $12 : global store [$10 <- $11]
}
}
[I 04/20/23 19:15:55.944 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] No-access mesh-for demoted:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <i32> $4 = bit_shr $2 $3
  <i32> $5 = const 0
  <i32> $6 = bit_shr $4 $5
  <i32> $7 = const 1
  <i32> $8 = mul $6 $7
  <i32> $9 = add $1 $8
  <*f32> $10 = global ptr [S4place<f32>], index [$9] activate=false
  <f32> $11 = const 0.0
  $12 : global store [$10 <- $11]
}
}
[I 04/20/23 19:15:55.944 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Make thread local:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <i32> $4 = bit_shr $2 $3
  <i32> $5 = const 0
  <i32> $6 = bit_shr $4 $5
  <i32> $7 = const 1
  <i32> $8 = mul $6 $7
  <i32> $9 = add $1 $8
  <*f32> $10 = global ptr [S4place<f32>], index [$9] activate=false
  <f32> $11 = const 0.0
  $12 : global store [$10 <- $11]
}
}
[I 04/20/23 19:15:55.944 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Make mesh thread local:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <i32> $4 = bit_shr $2 $3
  <i32> $5 = const 0
  <i32> $6 = bit_shr $4 $5
  <i32> $7 = const 1
  <i32> $8 = mul $6 $7
  <i32> $9 = add $1 $8
  <*f32> $10 = global ptr [S4place<f32>], index [$9] activate=false
  <f32> $11 = const 0.0
  $12 : global store [$10 <- $11]
}
}
[I 04/20/23 19:15:55.945 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Make mesh block local:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <i32> $4 = bit_shr $2 $3
  <i32> $5 = const 0
  <i32> $6 = bit_shr $4 $5
  <i32> $7 = const 1
  <i32> $8 = mul $6 $7
  <i32> $9 = add $1 $8
  <*f32> $10 = global ptr [S4place<f32>], index [$9] activate=false
  <f32> $11 = const 0.0
  $12 : global store [$10 <- $11]
}
}
[I 04/20/23 19:15:55.945 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Simplified X:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = const 0.0
  $4 : global store [$2 <- $3]
}
}
[I 04/20/23 19:15:55.945 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Make block local:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = const 0.0
  $4 : global store [$2 <- $3]
}
}
[I 04/20/23 19:15:55.945 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Demote mesh statements:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = const 0.0
  $4 : global store [$2 <- $3]
}
}
[I 04/20/23 19:15:55.945 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Atomics demoted II:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = const 0.0
  $4 : global store [$2 <- $3]
}
}
[I 04/20/23 19:15:55.945 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Remove range assumption:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = const 0.0
  $4 : global store [$2 <- $3]
}
}
[I 04/20/23 19:15:55.945 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Remove loop_unique:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = const 0.0
  $4 : global store [$2 <- $3]
}
}
[I 04/20/23 19:15:55.945 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Simplified before lower access:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = const 0.0
  $4 : global store [$2 <- $3]
}
}
[I 04/20/23 19:15:55.945 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Access lowered:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = const 0.0
  <*gen> $4 = get root [S0root][root]
  <i32> $5 = linearized(ind {}, stride {})
  <*gen> $6 = [S0root][root]::lookup($4, $5) activate = false
  <*gen> $7 = get child [S0root->S3dense] $6
  <i32> $8 = const 0
  <i32> $9 = bit_shr $1 $8
  <i32> $10 = linearized(ind {$9}, stride {8388608})
  <*gen> $11 = [S3dense][dense]::lookup($7, $10) activate = false
  <*f32> $12 = get child [S3dense->S4place<f32>] $11
  $13 : global store [$12 <- $3]
}
}
[I 04/20/23 19:15:55.945 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] DIE:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <f32> $2 = const 0.0
  <*gen> $3 = get root [S0root][root]
  <i32> $4 = linearized(ind {}, stride {})
  <*gen> $5 = [S0root][root]::lookup($3, $4) activate = false
  <*gen> $6 = get child [S0root->S3dense] $5
  <i32> $7 = const 0
  <i32> $8 = bit_shr $1 $7
  <i32> $9 = linearized(ind {$8}, stride {8388608})
  <*gen> $10 = [S3dense][dense]::lookup($6, $9) activate = false
  <*f32> $11 = get child [S3dense->S4place<f32>] $10
  $12 : global store [$11 <- $2]
}
}
[I 04/20/23 19:15:55.945 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Access flagged III:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <f32> $2 = const 0.0
  <*gen> $3 = get root [S0root][root]
  <i32> $4 = linearized(ind {}, stride {})
  <*gen> $5 = [S0root][root]::lookup($3, $4) activate = false
  <*gen> $6 = get child [S0root->S3dense] $5
  <i32> $7 = const 0
  <i32> $8 = bit_shr $1 $7
  <i32> $9 = linearized(ind {$8}, stride {8388608})
  <*gen> $10 = [S3dense][dense]::lookup($6, $9) activate = false
  <*f32> $11 = get child [S3dense->S4place<f32>] $10
  $12 : global store [$11 <- $2]
}
}
[I 04/20/23 19:15:55.945 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Operations demoted:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <f32> $2 = const 0.0
  <*gen> $3 = get root [S0root][root]
  <i32> $4 = linearized(ind {}, stride {})
  <*gen> $5 = [S0root][root]::lookup($3, $4) activate = false
  <*gen> $6 = get child [S0root->S3dense] $5
  <i32> $7 = const 0
  <u32> $8 = reinterpret_cast_bits<u32> $1
  <u32> $9 = cast_value<u32> $7
  <u32> $10 = bit_sar $8 $9
  <i32> $11 = reinterpret_cast_bits<i32> $10
  <i32> $12 = linearized(ind {$11}, stride {8388608})
  <*gen> $13 = [S3dense][dense]::lookup($6, $12) activate = false
  <*f32> $14 = get child [S3dense->S4place<f32>] $13
  $15 : global store [$14 <- $2]
}
}
[I 04/20/23 19:15:55.945 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Simplified IV:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <f32> $2 = const 0.0
  <*gen> $3 = get root [S0root][root]
  <i32> $4 = const 0
  <*gen> $5 = [S0root][root]::lookup($3, $4) activate = false
  <*gen> $6 = get child [S0root->S3dense] $5
  <*gen> $7 = [S3dense][dense]::lookup($6, $1) activate = false
  <*f32> $8 = get child [S3dense->S4place<f32>] $7
  $9 : global store [$8 <- $2]
}
}
[I 04/20/23 19:15:55.945 1820179] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_1] Bit struct stores optimized:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <f32> $2 = const 0.0
  <*gen> $3 = get root [S0root][root]
  <i32> $4 = const 0
  <*gen> $5 = [S0root][root]::lookup($3, $4) activate = false
  <*gen> $6 = get child [S0root->S3dense] $5
  <*gen> $7 = [S3dense][dense]::lookup($6, $1) activate = false
  <*f32> $8 = get child [S3dense->S4place<f32>] $7
  $9 : global store [$8 <- $2]
}
}
[I 04/20/23 19:15:55.980 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Initial IR:
kernel {
  $0 : for @tmp0, @tmp1 in S10place<f32> noneblock_dim=adaptive {
    $1 = alloca @tmp2
    @tmp2 = 0
    $3 = alloca @tmp3
    @tmp3 = [@tmp0, @tmp1] (dt=[Tensor (2) i32])
    #@tmp5 (snode=S10place<f32>)[@tmp3[0], @tmp3[1]] = @tmp2
  }
}
[I 04/20/23 19:15:55.980 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Lowered:
kernel {
  $0 : struct for in S9dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <i32> $3 = alloca
    <i32> $4 = const 0
    $5 : local store [$3 <- $4]
    <[Tensor (2) i32]> $6 = alloca
    <[Tensor (2) i32]> $7 = [$1, $2]
    $8 : local store [$6 <- $7]
    <i32> $9 = local load [$3]
    <i32> $10 = const 0
    <*i32> $11 = shift ptr [$6 + $10]
    <i32> $12 = local load [$11]
    <i32> $13 = const 1
    <*i32> $14 = shift ptr [$6 + $13]
    <i32> $15 = local load [$14]
    <f32> $16 = global ptr [S10place<f32>], index [$12, $15] activate=true
    $17 : global store [$16 <- $9]
  }
}
[I 04/20/23 19:15:55.980 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Immutable local vars eliminated:
kernel {
  $0 : struct for in S9dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <i32> $3 = const 0
    <[Tensor (2) i32]> $4 = alloca
    <[Tensor (2) i32]> $5 = [$1, $2]
    $6 : local store [$4 <- $5]
    <i32> $7 = const 0
    <*i32> $8 = shift ptr [$4 + $7]
    <i32> $9 = local load [$8]
    <i32> $10 = const 1
    <*i32> $11 = shift ptr [$4 + $10]
    <i32> $12 = local load [$11]
    <f32> $13 = global ptr [S10place<f32>], index [$9, $12] activate=true
    $14 : global store [$13 <- $3]
  }
}
[I 04/20/23 19:15:55.980 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Typechecked:
kernel {
  $0 : struct for in S9dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <i32> $3 = const 0
    <[Tensor (2) i32]> $4 = alloca
    <[Tensor (2) i32]> $5 = [$1, $2]
    <[Tensor (2) i32]> $6 : local store [$4 <- $5]
    <i32> $7 = const 0
    <*i32> $8 = shift ptr [$4 + $7]
    <i32> $9 = local load [$8]
    <i32> $10 = const 1
    <*i32> $11 = shift ptr [$4 + $10]
    <i32> $12 = local load [$11]
    <*f32> $13 = global ptr [S10place<f32>], index [$9, $12] activate=true
    <f32> $14 = cast_value<f32> $3
    $15 : global store [$13 <- $14]
  }
}
[I 04/20/23 19:15:55.980 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Bit Loop Vectorized:
kernel {
  $0 : struct for in S9dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <i32> $3 = const 0
    <[Tensor (2) i32]> $4 = alloca
    <[Tensor (2) i32]> $5 = [$1, $2]
    <[Tensor (2) i32]> $6 : local store [$4 <- $5]
    <i32> $7 = const 0
    <*i32> $8 = shift ptr [$4 + $7]
    <i32> $9 = local load [$8]
    <i32> $10 = const 1
    <*i32> $11 = shift ptr [$4 + $10]
    <i32> $12 = local load [$11]
    <*f32> $13 = global ptr [S10place<f32>], index [$9, $12] activate=true
    <f32> $14 = cast_value<f32> $3
    $15 : global store [$13 <- $14]
  }
}
[I 04/20/23 19:15:55.980 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Matrix ptr lowered:
kernel {
  $0 : struct for in S9dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <i32> $3 = const 0
    <[Tensor (2) i32]> $4 = alloca
    <[Tensor (2) i32]> $5 = [$1, $2]
    <[Tensor (2) i32]> $6 : local store [$4 <- $5]
    <i32> $7 = const 0
    <*i32> $8 = shift ptr [$4 + $7]
    <i32> $9 = local load [$8]
    <i32> $10 = const 1
    <*i32> $11 = shift ptr [$4 + $10]
    <i32> $12 = local load [$11]
    <*f32> $13 = global ptr [S10place<f32>], index [$9, $12] activate=true
    <f32> $14 = cast_value<f32> $3
    $15 : global store [$13 <- $14]
  }
}
[I 04/20/23 19:15:55.980 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Simplified I:
kernel {
  <f32> $0 = const 0.0
  <i32> $1 = const 1
  <i32> $2 = const 0
  $3 : struct for in S9dense noneblock_dim=adaptive {
    <i32> $4 = loop $3 index 0
    <i32> $5 = loop $3 index 1
    <[Tensor (2) i32]> $6 = alloca
    <[Tensor (2) i32]> $7 = [$4, $5]
    <[Tensor (2) i32]> $8 : local store [$6 <- $7]
    <*i32> $9 = shift ptr [$6 + $2]
    <i32> $10 = local load [$9]
    <*i32> $11 = shift ptr [$6 + $1]
    <i32> $12 = local load [$11]
    <*f32> $13 = global ptr [S10place<f32>], index [$10, $12] activate=true
    $14 : global store [$13 <- $0]
  }
}
[I 04/20/23 19:15:55.980 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Scalarized:
kernel {
  <f32> $0 = const 0.0
  $1 : struct for in S9dense noneblock_dim=adaptive {
    <i32> $2 = loop $1 index 0
    <i32> $3 = loop $1 index 1
    <i32> $4 = alloca
    <i32> $5 = alloca
    $6 : local store [$4 <- $2]
    $7 : local store [$5 <- $3]
    <i32> $8 = local load [$4]
    <i32> $9 = local load [$5]
    <*f32> $10 = global ptr [S10place<f32>], index [$8, $9] activate=true
    $11 : global store [$10 <- $0]
  }
}
[I 04/20/23 19:15:55.980 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Access flagged I:
kernel {
  <f32> $0 = const 0.0
  $1 : struct for in S9dense noneblock_dim=adaptive {
    <i32> $2 = loop $1 index 0
    <i32> $3 = loop $1 index 1
    <i32> $4 = alloca
    <i32> $5 = alloca
    $6 : local store [$4 <- $2]
    $7 : local store [$5 <- $3]
    <i32> $8 = local load [$4]
    <i32> $9 = local load [$5]
    <*f32> $10 = global ptr [S10place<f32>], index [$8, $9] activate=true
    $11 : global store [$10 <- $0]
  }
}
[I 04/20/23 19:15:55.981 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Simplified II:
kernel {
  <f32> $0 = const 0.0
  $1 : struct for in S9dense noneblock_dim=adaptive {
    <i32> $2 = loop $1 index 0
    <i32> $3 = loop $1 index 1
    <*f32> $4 = global ptr [S10place<f32>], index [$2, $3] activate=true
    $5 : global store [$4 <- $0]
  }
}
[I 04/20/23 19:15:55.981 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Offloaded:
kernel {
  $0 = offloaded
  body {
    <f32> $1 = const 0.0
  }
  $2 = offloaded struct_for(S9dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $3 = loop $2 index 0
    <i32> $4 = loop $2 index 1
    <*f32> $5 = global ptr [S10place<f32>], index [$3, $4] activate=true
    <f32> $6 = const 0.0
    $7 : global store [$5 <- $6]
  }
}
[I 04/20/23 19:15:55.981 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Optimized by CFG:
kernel {
  $0 = offloaded
  body {
  }
  $1 = offloaded struct_for(S9dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $2 = loop $1 index 0
    <i32> $3 = loop $1 index 1
    <*f32> $4 = global ptr [S10place<f32>], index [$2, $3] activate=true
    <f32> $5 = const 0.0
    $6 : global store [$4 <- $5]
  }
}
[I 04/20/23 19:15:55.981 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Access flagged II:
kernel {
  $0 = offloaded
  body {
  }
  $1 = offloaded struct_for(S9dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $2 = loop $1 index 0
    <i32> $3 = loop $1 index 1
    <*f32> $4 = global ptr [S10place<f32>], index [$2, $3] activate=false
    <f32> $5 = const 0.0
    $6 : global store [$4 <- $5]
  }
}
[I 04/20/23 19:15:55.981 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Simplified III:
kernel {
  $0 = offloaded struct_for(S9dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <*f32> $3 = global ptr [S10place<f32>], index [$1, $2] activate=false
    <f32> $4 = const 0.0
    $5 : global store [$3 <- $4]
  }
}
[I 04/20/23 19:15:55.981 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Start offload_to_executable:
kernel {
$0 = offloaded struct_for(S9dense) grid_dim=2624 block_dim=128 bls=none
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = loop $0 index 1
  <*f32> $3 = global ptr [S10place<f32>], index [$1, $2] activate=false
  <f32> $4 = const 0.0
  $5 : global store [$3 <- $4]
}
}
[I 04/20/23 19:15:55.981 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Detect read-only accesses:
kernel {
$0 = offloaded struct_for(S9dense) grid_dim=2624 block_dim=128 bls=none
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = loop $0 index 1
  <*f32> $3 = global ptr [S10place<f32>], index [$1, $2] activate=false
  <f32> $4 = const 0.0
  $5 : global store [$3 <- $4]
}
}
[I 04/20/23 19:15:55.981 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Atomics demoted I:
kernel {
$0 = offloaded struct_for(S9dense) grid_dim=2624 block_dim=128 bls=none
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = loop $0 index 1
  <*f32> $3 = global ptr [S10place<f32>], index [$1, $2] activate=false
  <f32> $4 = const 0.0
  $5 : global store [$3 <- $4]
}
}
[I 04/20/23 19:15:55.981 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Cache loop-invariant global vars:
kernel {
$0 = offloaded struct_for(S9dense) grid_dim=2624 block_dim=128 bls=none
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = loop $0 index 1
  <*f32> $3 = global ptr [S10place<f32>], index [$1, $2] activate=false
  <f32> $4 = const 0.0
  $5 : global store [$3 <- $4]
}
}
[I 04/20/23 19:15:55.981 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Dense struct-for demoted:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = const 0
  <i32> $3 = loop $0 index 0
  <i32> $4 = const 0
  <i32> $5 = bit_shr $3 $4
  <i32> $6 = const 3
  <i32> $7 = div $5 $6
  <i32> $8 = const 1
  <i32> $9 = mul $7 $8
  <i32> $10 = add $1 $9
  <i32> $11 = const 3
  <i32> $12 = mod $5 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $12 $13
  <i32> $15 = const 1
  <i32> $16 = mul $14 $15
  <i32> $17 = add $2 $16
  <*f32> $18 = global ptr [S10place<f32>], index [$10, $17] activate=false
  <f32> $19 = const 0.0
  $20 : global store [$18 <- $19]
}
}
[I 04/20/23 19:15:55.981 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] No-access mesh-for demoted:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = const 0
  <i32> $3 = loop $0 index 0
  <i32> $4 = const 0
  <i32> $5 = bit_shr $3 $4
  <i32> $6 = const 3
  <i32> $7 = div $5 $6
  <i32> $8 = const 1
  <i32> $9 = mul $7 $8
  <i32> $10 = add $1 $9
  <i32> $11 = const 3
  <i32> $12 = mod $5 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $12 $13
  <i32> $15 = const 1
  <i32> $16 = mul $14 $15
  <i32> $17 = add $2 $16
  <*f32> $18 = global ptr [S10place<f32>], index [$10, $17] activate=false
  <f32> $19 = const 0.0
  $20 : global store [$18 <- $19]
}
}
[I 04/20/23 19:15:55.981 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Make thread local:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = const 0
  <i32> $3 = loop $0 index 0
  <i32> $4 = const 0
  <i32> $5 = bit_shr $3 $4
  <i32> $6 = const 3
  <i32> $7 = div $5 $6
  <i32> $8 = const 1
  <i32> $9 = mul $7 $8
  <i32> $10 = add $1 $9
  <i32> $11 = const 3
  <i32> $12 = mod $5 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $12 $13
  <i32> $15 = const 1
  <i32> $16 = mul $14 $15
  <i32> $17 = add $2 $16
  <*f32> $18 = global ptr [S10place<f32>], index [$10, $17] activate=false
  <f32> $19 = const 0.0
  $20 : global store [$18 <- $19]
}
}
[I 04/20/23 19:15:55.981 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Make mesh thread local:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = const 0
  <i32> $3 = loop $0 index 0
  <i32> $4 = const 0
  <i32> $5 = bit_shr $3 $4
  <i32> $6 = const 3
  <i32> $7 = div $5 $6
  <i32> $8 = const 1
  <i32> $9 = mul $7 $8
  <i32> $10 = add $1 $9
  <i32> $11 = const 3
  <i32> $12 = mod $5 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $12 $13
  <i32> $15 = const 1
  <i32> $16 = mul $14 $15
  <i32> $17 = add $2 $16
  <*f32> $18 = global ptr [S10place<f32>], index [$10, $17] activate=false
  <f32> $19 = const 0.0
  $20 : global store [$18 <- $19]
}
}
[I 04/20/23 19:15:55.982 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Make mesh block local:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = const 0
  <i32> $3 = loop $0 index 0
  <i32> $4 = const 0
  <i32> $5 = bit_shr $3 $4
  <i32> $6 = const 3
  <i32> $7 = div $5 $6
  <i32> $8 = const 1
  <i32> $9 = mul $7 $8
  <i32> $10 = add $1 $9
  <i32> $11 = const 3
  <i32> $12 = mod $5 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $12 $13
  <i32> $15 = const 1
  <i32> $16 = mul $14 $15
  <i32> $17 = add $2 $16
  <*f32> $18 = global ptr [S10place<f32>], index [$10, $17] activate=false
  <f32> $19 = const 0.0
  $20 : global store [$18 <- $19]
}
}
[I 04/20/23 19:15:55.982 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Simplified X:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S10place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 0.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:55.982 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Make block local:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S10place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 0.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:55.982 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Demote mesh statements:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S10place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 0.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:55.982 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Atomics demoted II:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S10place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 0.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:55.982 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Remove range assumption:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S10place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 0.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:55.982 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Remove loop_unique:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S10place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 0.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:55.982 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Simplified before lower access:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S10place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 0.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:55.982 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Access lowered:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S10place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 0.0
  <*gen> $7 = get root [S0root][root]
  <i32> $8 = linearized(ind {}, stride {})
  <*gen> $9 = [S0root][root]::lookup($7, $8) activate = false
  <*gen> $10 = get child [S0root->S9dense] $9
  <i32> $11 = const 0
  <i32> $12 = bit_shr $3 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $4 $13
  <i32> $15 = linearized(ind {$12, $14}, stride {8192, 3})
  <*gen> $16 = [S9dense][dense]::lookup($10, $15) activate = false
  <*f32> $17 = get child [S9dense->S10place<f32>] $16
  $18 : global store [$17 <- $6]
}
}
[I 04/20/23 19:15:55.982 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] DIE:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <f32> $5 = const 0.0
  <*gen> $6 = get root [S0root][root]
  <i32> $7 = linearized(ind {}, stride {})
  <*gen> $8 = [S0root][root]::lookup($6, $7) activate = false
  <*gen> $9 = get child [S0root->S9dense] $8
  <i32> $10 = const 0
  <i32> $11 = bit_shr $3 $10
  <i32> $12 = const 0
  <i32> $13 = bit_shr $4 $12
  <i32> $14 = linearized(ind {$11, $13}, stride {8192, 3})
  <*gen> $15 = [S9dense][dense]::lookup($9, $14) activate = false
  <*f32> $16 = get child [S9dense->S10place<f32>] $15
  $17 : global store [$16 <- $5]
}
}
[I 04/20/23 19:15:55.982 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Access flagged III:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <f32> $5 = const 0.0
  <*gen> $6 = get root [S0root][root]
  <i32> $7 = linearized(ind {}, stride {})
  <*gen> $8 = [S0root][root]::lookup($6, $7) activate = false
  <*gen> $9 = get child [S0root->S9dense] $8
  <i32> $10 = const 0
  <i32> $11 = bit_shr $3 $10
  <i32> $12 = const 0
  <i32> $13 = bit_shr $4 $12
  <i32> $14 = linearized(ind {$11, $13}, stride {8192, 3})
  <*gen> $15 = [S9dense][dense]::lookup($9, $14) activate = false
  <*f32> $16 = get child [S9dense->S10place<f32>] $15
  $17 : global store [$16 <- $5]
}
}
[I 04/20/23 19:15:55.983 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Operations demoted:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <f32> $5 = const 0.0
  <*gen> $6 = get root [S0root][root]
  <i32> $7 = linearized(ind {}, stride {})
  <*gen> $8 = [S0root][root]::lookup($6, $7) activate = false
  <*gen> $9 = get child [S0root->S9dense] $8
  <i32> $10 = const 0
  <u32> $11 = reinterpret_cast_bits<u32> $3
  <u32> $12 = cast_value<u32> $10
  <u32> $13 = bit_sar $11 $12
  <i32> $14 = reinterpret_cast_bits<i32> $13
  <i32> $15 = const 0
  <u32> $16 = reinterpret_cast_bits<u32> $4
  <u32> $17 = cast_value<u32> $15
  <u32> $18 = bit_sar $16 $17
  <i32> $19 = reinterpret_cast_bits<i32> $18
  <i32> $20 = linearized(ind {$14, $19}, stride {8192, 3})
  <*gen> $21 = [S9dense][dense]::lookup($9, $20) activate = false
  <*f32> $22 = get child [S9dense->S10place<f32>] $21
  $23 : global store [$22 <- $5]
}
}
[I 04/20/23 19:15:55.983 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Simplified IV:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <f32> $5 = const 0.0
  <*gen> $6 = get root [S0root][root]
  <i32> $7 = const 0
  <*gen> $8 = [S0root][root]::lookup($6, $7) activate = false
  <*gen> $9 = get child [S0root->S9dense] $8
  <i32> $10 = mul $3 $2
  <i32> $11 = add $4 $10
  <*gen> $12 = [S9dense][dense]::lookup($9, $11) activate = false
  <*f32> $13 = get child [S9dense->S10place<f32>] $12
  $14 : global store [$13 <- $5]
}
}
[I 04/20/23 19:15:55.983 1820178] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_2] Bit struct stores optimized:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <f32> $5 = const 0.0
  <*gen> $6 = get root [S0root][root]
  <i32> $7 = const 0
  <*gen> $8 = [S0root][root]::lookup($6, $7) activate = false
  <*gen> $9 = get child [S0root->S9dense] $8
  <i32> $10 = mul $3 $2
  <i32> $11 = add $4 $10
  <*gen> $12 = [S9dense][dense]::lookup($9, $11) activate = false
  <*f32> $13 = get child [S9dense->S10place<f32>] $12
  $14 : global store [$13 <- $5]
}
}
[I 04/20/23 19:15:56.026 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Initial IR:
kernel {
  $0 : for @tmp0, @tmp1 in S10place<f32> noneblock_dim=adaptive {
    $1 = alloca @tmp2
    @tmp2 = 1.0
    $3 = alloca @tmp3
    @tmp3 = [@tmp0, @tmp1] (dt=[Tensor (2) i32])
    #@tmp5 (snode=S10place<f32>)[@tmp3[0], @tmp3[1]] = @tmp2
  }
}
[I 04/20/23 19:15:56.026 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Lowered:
kernel {
  $0 : struct for in S9dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <f32> $3 = alloca
    <f32> $4 = const 1.0
    $5 : local store [$3 <- $4]
    <[Tensor (2) i32]> $6 = alloca
    <[Tensor (2) i32]> $7 = [$1, $2]
    $8 : local store [$6 <- $7]
    <f32> $9 = local load [$3]
    <i32> $10 = const 0
    <*i32> $11 = shift ptr [$6 + $10]
    <i32> $12 = local load [$11]
    <i32> $13 = const 1
    <*i32> $14 = shift ptr [$6 + $13]
    <i32> $15 = local load [$14]
    <f32> $16 = global ptr [S10place<f32>], index [$12, $15] activate=true
    $17 : global store [$16 <- $9]
  }
}
[I 04/20/23 19:15:56.026 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Immutable local vars eliminated:
kernel {
  $0 : struct for in S9dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <f32> $3 = const 1.0
    <[Tensor (2) i32]> $4 = alloca
    <[Tensor (2) i32]> $5 = [$1, $2]
    $6 : local store [$4 <- $5]
    <i32> $7 = const 0
    <*i32> $8 = shift ptr [$4 + $7]
    <i32> $9 = local load [$8]
    <i32> $10 = const 1
    <*i32> $11 = shift ptr [$4 + $10]
    <i32> $12 = local load [$11]
    <f32> $13 = global ptr [S10place<f32>], index [$9, $12] activate=true
    $14 : global store [$13 <- $3]
  }
}
[I 04/20/23 19:15:56.026 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Typechecked:
kernel {
  $0 : struct for in S9dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <f32> $3 = const 1.0
    <[Tensor (2) i32]> $4 = alloca
    <[Tensor (2) i32]> $5 = [$1, $2]
    <[Tensor (2) i32]> $6 : local store [$4 <- $5]
    <i32> $7 = const 0
    <*i32> $8 = shift ptr [$4 + $7]
    <i32> $9 = local load [$8]
    <i32> $10 = const 1
    <*i32> $11 = shift ptr [$4 + $10]
    <i32> $12 = local load [$11]
    <*f32> $13 = global ptr [S10place<f32>], index [$9, $12] activate=true
    $14 : global store [$13 <- $3]
  }
}
[I 04/20/23 19:15:56.026 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Bit Loop Vectorized:
kernel {
  $0 : struct for in S9dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <f32> $3 = const 1.0
    <[Tensor (2) i32]> $4 = alloca
    <[Tensor (2) i32]> $5 = [$1, $2]
    <[Tensor (2) i32]> $6 : local store [$4 <- $5]
    <i32> $7 = const 0
    <*i32> $8 = shift ptr [$4 + $7]
    <i32> $9 = local load [$8]
    <i32> $10 = const 1
    <*i32> $11 = shift ptr [$4 + $10]
    <i32> $12 = local load [$11]
    <*f32> $13 = global ptr [S10place<f32>], index [$9, $12] activate=true
    $14 : global store [$13 <- $3]
  }
}
[I 04/20/23 19:15:56.026 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Matrix ptr lowered:
kernel {
  $0 : struct for in S9dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <f32> $3 = const 1.0
    <[Tensor (2) i32]> $4 = alloca
    <[Tensor (2) i32]> $5 = [$1, $2]
    <[Tensor (2) i32]> $6 : local store [$4 <- $5]
    <i32> $7 = const 0
    <*i32> $8 = shift ptr [$4 + $7]
    <i32> $9 = local load [$8]
    <i32> $10 = const 1
    <*i32> $11 = shift ptr [$4 + $10]
    <i32> $12 = local load [$11]
    <*f32> $13 = global ptr [S10place<f32>], index [$9, $12] activate=true
    $14 : global store [$13 <- $3]
  }
}
[I 04/20/23 19:15:56.026 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Simplified I:
kernel {
  <i32> $0 = const 1
  <i32> $1 = const 0
  <f32> $2 = const 1.0
  $3 : struct for in S9dense noneblock_dim=adaptive {
    <i32> $4 = loop $3 index 0
    <i32> $5 = loop $3 index 1
    <[Tensor (2) i32]> $6 = alloca
    <[Tensor (2) i32]> $7 = [$4, $5]
    <[Tensor (2) i32]> $8 : local store [$6 <- $7]
    <*i32> $9 = shift ptr [$6 + $1]
    <i32> $10 = local load [$9]
    <*i32> $11 = shift ptr [$6 + $0]
    <i32> $12 = local load [$11]
    <*f32> $13 = global ptr [S10place<f32>], index [$10, $12] activate=true
    $14 : global store [$13 <- $2]
  }
}
[I 04/20/23 19:15:56.027 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Scalarized:
kernel {
  <f32> $0 = const 1.0
  $1 : struct for in S9dense noneblock_dim=adaptive {
    <i32> $2 = loop $1 index 0
    <i32> $3 = loop $1 index 1
    <i32> $4 = alloca
    <i32> $5 = alloca
    $6 : local store [$4 <- $2]
    $7 : local store [$5 <- $3]
    <i32> $8 = local load [$4]
    <i32> $9 = local load [$5]
    <*f32> $10 = global ptr [S10place<f32>], index [$8, $9] activate=true
    $11 : global store [$10 <- $0]
  }
}
[I 04/20/23 19:15:56.027 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Access flagged I:
kernel {
  <f32> $0 = const 1.0
  $1 : struct for in S9dense noneblock_dim=adaptive {
    <i32> $2 = loop $1 index 0
    <i32> $3 = loop $1 index 1
    <i32> $4 = alloca
    <i32> $5 = alloca
    $6 : local store [$4 <- $2]
    $7 : local store [$5 <- $3]
    <i32> $8 = local load [$4]
    <i32> $9 = local load [$5]
    <*f32> $10 = global ptr [S10place<f32>], index [$8, $9] activate=true
    $11 : global store [$10 <- $0]
  }
}
[I 04/20/23 19:15:56.027 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Simplified II:
kernel {
  <f32> $0 = const 1.0
  $1 : struct for in S9dense noneblock_dim=adaptive {
    <i32> $2 = loop $1 index 0
    <i32> $3 = loop $1 index 1
    <*f32> $4 = global ptr [S10place<f32>], index [$2, $3] activate=true
    $5 : global store [$4 <- $0]
  }
}
[I 04/20/23 19:15:56.027 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Offloaded:
kernel {
  $0 = offloaded
  body {
    <f32> $1 = const 1.0
  }
  $2 = offloaded struct_for(S9dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $3 = loop $2 index 0
    <i32> $4 = loop $2 index 1
    <*f32> $5 = global ptr [S10place<f32>], index [$3, $4] activate=true
    <f32> $6 = const 1.0
    $7 : global store [$5 <- $6]
  }
}
[I 04/20/23 19:15:56.027 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Optimized by CFG:
kernel {
  $0 = offloaded
  body {
  }
  $1 = offloaded struct_for(S9dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $2 = loop $1 index 0
    <i32> $3 = loop $1 index 1
    <*f32> $4 = global ptr [S10place<f32>], index [$2, $3] activate=true
    <f32> $5 = const 1.0
    $6 : global store [$4 <- $5]
  }
}
[I 04/20/23 19:15:56.027 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Access flagged II:
kernel {
  $0 = offloaded
  body {
  }
  $1 = offloaded struct_for(S9dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $2 = loop $1 index 0
    <i32> $3 = loop $1 index 1
    <*f32> $4 = global ptr [S10place<f32>], index [$2, $3] activate=false
    <f32> $5 = const 1.0
    $6 : global store [$4 <- $5]
  }
}
[I 04/20/23 19:15:56.027 1820159] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Simplified III:
kernel {
  $0 = offloaded struct_for(S9dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <*f32> $3 = global ptr [S10place<f32>], index [$1, $2] activate=false
    <f32> $4 = const 1.0
    $5 : global store [$3 <- $4]
  }
}
[I 04/20/23 19:15:56.027 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Start offload_to_executable:
kernel {
$0 = offloaded struct_for(S9dense) grid_dim=2624 block_dim=128 bls=none
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = loop $0 index 1
  <*f32> $3 = global ptr [S10place<f32>], index [$1, $2] activate=false
  <f32> $4 = const 1.0
  $5 : global store [$3 <- $4]
}
}
[I 04/20/23 19:15:56.027 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Detect read-only accesses:
kernel {
$0 = offloaded struct_for(S9dense) grid_dim=2624 block_dim=128 bls=none
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = loop $0 index 1
  <*f32> $3 = global ptr [S10place<f32>], index [$1, $2] activate=false
  <f32> $4 = const 1.0
  $5 : global store [$3 <- $4]
}
}
[I 04/20/23 19:15:56.027 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Atomics demoted I:
kernel {
$0 = offloaded struct_for(S9dense) grid_dim=2624 block_dim=128 bls=none
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = loop $0 index 1
  <*f32> $3 = global ptr [S10place<f32>], index [$1, $2] activate=false
  <f32> $4 = const 1.0
  $5 : global store [$3 <- $4]
}
}
[I 04/20/23 19:15:56.027 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Cache loop-invariant global vars:
kernel {
$0 = offloaded struct_for(S9dense) grid_dim=2624 block_dim=128 bls=none
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = loop $0 index 1
  <*f32> $3 = global ptr [S10place<f32>], index [$1, $2] activate=false
  <f32> $4 = const 1.0
  $5 : global store [$3 <- $4]
}
}
[I 04/20/23 19:15:56.027 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Dense struct-for demoted:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = const 0
  <i32> $3 = loop $0 index 0
  <i32> $4 = const 0
  <i32> $5 = bit_shr $3 $4
  <i32> $6 = const 3
  <i32> $7 = div $5 $6
  <i32> $8 = const 1
  <i32> $9 = mul $7 $8
  <i32> $10 = add $1 $9
  <i32> $11 = const 3
  <i32> $12 = mod $5 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $12 $13
  <i32> $15 = const 1
  <i32> $16 = mul $14 $15
  <i32> $17 = add $2 $16
  <*f32> $18 = global ptr [S10place<f32>], index [$10, $17] activate=false
  <f32> $19 = const 1.0
  $20 : global store [$18 <- $19]
}
}
[I 04/20/23 19:15:56.027 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] No-access mesh-for demoted:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = const 0
  <i32> $3 = loop $0 index 0
  <i32> $4 = const 0
  <i32> $5 = bit_shr $3 $4
  <i32> $6 = const 3
  <i32> $7 = div $5 $6
  <i32> $8 = const 1
  <i32> $9 = mul $7 $8
  <i32> $10 = add $1 $9
  <i32> $11 = const 3
  <i32> $12 = mod $5 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $12 $13
  <i32> $15 = const 1
  <i32> $16 = mul $14 $15
  <i32> $17 = add $2 $16
  <*f32> $18 = global ptr [S10place<f32>], index [$10, $17] activate=false
  <f32> $19 = const 1.0
  $20 : global store [$18 <- $19]
}
}
[I 04/20/23 19:15:56.027 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Make thread local:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = const 0
  <i32> $3 = loop $0 index 0
  <i32> $4 = const 0
  <i32> $5 = bit_shr $3 $4
  <i32> $6 = const 3
  <i32> $7 = div $5 $6
  <i32> $8 = const 1
  <i32> $9 = mul $7 $8
  <i32> $10 = add $1 $9
  <i32> $11 = const 3
  <i32> $12 = mod $5 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $12 $13
  <i32> $15 = const 1
  <i32> $16 = mul $14 $15
  <i32> $17 = add $2 $16
  <*f32> $18 = global ptr [S10place<f32>], index [$10, $17] activate=false
  <f32> $19 = const 1.0
  $20 : global store [$18 <- $19]
}
}
[I 04/20/23 19:15:56.027 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Make mesh thread local:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = const 0
  <i32> $3 = loop $0 index 0
  <i32> $4 = const 0
  <i32> $5 = bit_shr $3 $4
  <i32> $6 = const 3
  <i32> $7 = div $5 $6
  <i32> $8 = const 1
  <i32> $9 = mul $7 $8
  <i32> $10 = add $1 $9
  <i32> $11 = const 3
  <i32> $12 = mod $5 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $12 $13
  <i32> $15 = const 1
  <i32> $16 = mul $14 $15
  <i32> $17 = add $2 $16
  <*f32> $18 = global ptr [S10place<f32>], index [$10, $17] activate=false
  <f32> $19 = const 1.0
  $20 : global store [$18 <- $19]
}
}
[I 04/20/23 19:15:56.028 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Make mesh block local:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = const 0
  <i32> $3 = loop $0 index 0
  <i32> $4 = const 0
  <i32> $5 = bit_shr $3 $4
  <i32> $6 = const 3
  <i32> $7 = div $5 $6
  <i32> $8 = const 1
  <i32> $9 = mul $7 $8
  <i32> $10 = add $1 $9
  <i32> $11 = const 3
  <i32> $12 = mod $5 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $12 $13
  <i32> $15 = const 1
  <i32> $16 = mul $14 $15
  <i32> $17 = add $2 $16
  <*f32> $18 = global ptr [S10place<f32>], index [$10, $17] activate=false
  <f32> $19 = const 1.0
  $20 : global store [$18 <- $19]
}
}
[I 04/20/23 19:15:56.028 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Simplified X:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S10place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 1.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:56.028 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Make block local:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S10place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 1.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:56.028 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Demote mesh statements:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S10place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 1.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:56.028 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Atomics demoted II:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S10place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 1.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:56.028 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Remove range assumption:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S10place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 1.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:56.028 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Remove loop_unique:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S10place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 1.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:56.028 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Simplified before lower access:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S10place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 1.0
  $7 : global store [$5 <- $6]
}
}
[I 04/20/23 19:15:56.028 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Access lowered:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S10place<f32>], index [$3, $4] activate=false
  <f32> $6 = const 1.0
  <*gen> $7 = get root [S0root][root]
  <i32> $8 = linearized(ind {}, stride {})
  <*gen> $9 = [S0root][root]::lookup($7, $8) activate = false
  <*gen> $10 = get child [S0root->S9dense] $9
  <i32> $11 = const 0
  <i32> $12 = bit_shr $3 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $4 $13
  <i32> $15 = linearized(ind {$12, $14}, stride {8192, 3})
  <*gen> $16 = [S9dense][dense]::lookup($10, $15) activate = false
  <*f32> $17 = get child [S9dense->S10place<f32>] $16
  $18 : global store [$17 <- $6]
}
}
[I 04/20/23 19:15:56.028 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] DIE:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <f32> $5 = const 1.0
  <*gen> $6 = get root [S0root][root]
  <i32> $7 = linearized(ind {}, stride {})
  <*gen> $8 = [S0root][root]::lookup($6, $7) activate = false
  <*gen> $9 = get child [S0root->S9dense] $8
  <i32> $10 = const 0
  <i32> $11 = bit_shr $3 $10
  <i32> $12 = const 0
  <i32> $13 = bit_shr $4 $12
  <i32> $14 = linearized(ind {$11, $13}, stride {8192, 3})
  <*gen> $15 = [S9dense][dense]::lookup($9, $14) activate = false
  <*f32> $16 = get child [S9dense->S10place<f32>] $15
  $17 : global store [$16 <- $5]
}
}
[I 04/20/23 19:15:56.028 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Access flagged III:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <f32> $5 = const 1.0
  <*gen> $6 = get root [S0root][root]
  <i32> $7 = linearized(ind {}, stride {})
  <*gen> $8 = [S0root][root]::lookup($6, $7) activate = false
  <*gen> $9 = get child [S0root->S9dense] $8
  <i32> $10 = const 0
  <i32> $11 = bit_shr $3 $10
  <i32> $12 = const 0
  <i32> $13 = bit_shr $4 $12
  <i32> $14 = linearized(ind {$11, $13}, stride {8192, 3})
  <*gen> $15 = [S9dense][dense]::lookup($9, $14) activate = false
  <*f32> $16 = get child [S9dense->S10place<f32>] $15
  $17 : global store [$16 <- $5]
}
}
[I 04/20/23 19:15:56.028 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Operations demoted:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <f32> $5 = const 1.0
  <*gen> $6 = get root [S0root][root]
  <i32> $7 = linearized(ind {}, stride {})
  <*gen> $8 = [S0root][root]::lookup($6, $7) activate = false
  <*gen> $9 = get child [S0root->S9dense] $8
  <i32> $10 = const 0
  <u32> $11 = reinterpret_cast_bits<u32> $3
  <u32> $12 = cast_value<u32> $10
  <u32> $13 = bit_sar $11 $12
  <i32> $14 = reinterpret_cast_bits<i32> $13
  <i32> $15 = const 0
  <u32> $16 = reinterpret_cast_bits<u32> $4
  <u32> $17 = cast_value<u32> $15
  <u32> $18 = bit_sar $16 $17
  <i32> $19 = reinterpret_cast_bits<i32> $18
  <i32> $20 = linearized(ind {$14, $19}, stride {8192, 3})
  <*gen> $21 = [S9dense][dense]::lookup($9, $20) activate = false
  <*f32> $22 = get child [S9dense->S10place<f32>] $21
  $23 : global store [$22 <- $5]
}
}
[I 04/20/23 19:15:56.029 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Simplified IV:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <f32> $5 = const 1.0
  <*gen> $6 = get root [S0root][root]
  <i32> $7 = const 0
  <*gen> $8 = [S0root][root]::lookup($6, $7) activate = false
  <*gen> $9 = get child [S0root->S9dense] $8
  <i32> $10 = mul $3 $2
  <i32> $11 = add $4 $10
  <*gen> $12 = [S9dense][dense]::lookup($9, $11) activate = false
  <*f32> $13 = get child [S9dense->S10place<f32>] $12
  $14 : global store [$13 <- $5]
}
}
[I 04/20/23 19:15:56.029 1820177] [compile_to_offloads.cpp:operator()@23] [fill_tensor_c0_3] Bit struct stores optimized:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <f32> $5 = const 1.0
  <*gen> $6 = get root [S0root][root]
  <i32> $7 = const 0
  <*gen> $8 = [S0root][root]::lookup($6, $7) activate = false
  <*gen> $9 = get child [S0root->S9dense] $8
  <i32> $10 = mul $3 $2
  <i32> $11 = add $4 $10
  <*gen> $12 = [S9dense][dense]::lookup($9, $11) activate = false
  <*f32> $13 = get child [S9dense->S10place<f32>] $12
  $14 : global store [$13 <- $5]
}
}
[I 04/20/23 19:15:56.064 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Initial IR:
kernel {
  $0 : for @tmp0 in range((cast_value<i32> 0), (cast_value<i32> 8192)) block_dim=256 {
    $1 = alloca @tmp1
    @tmp1 = (cast_value<i32> #@tmp3 (snode=S6place<i64>)[@tmp0, 0])
    $3 = alloca @tmp2
    @tmp2 = @tmp1
    $5 = alloca @tmp3
    @tmp3 = 0.0
    $7 = alloca @tmp4
    @tmp4 = @tmp3
    $9 : for @tmp5 in range((cast_value<i32> 0), (cast_value<i32> 1)) block_dim=adaptive {
      $10 = alloca @tmp6
      @tmp6 = atomic_add(@tmp4, #@tmp0 (snode=S2place<f32>)[@tmp5])
    }
    $12 = alloca @tmp7
    @tmp7 = @tmp4
    #@tmp4 (snode=S8place<f32>)[@tmp2, 0] = @tmp7
  }
}
[I 04/20/23 19:15:56.064 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Lowered:
kernel {
  <i32> $0 = const 0
  <i32> $1 = cast_value<i32> $0
  <i32> $2 = const 8192
  <i32> $3 = cast_value<i32> $2
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <i32> $6 = alloca
    <i32> $7 = const 0
    <i64> $8 = global ptr [S6place<i64>], index [$5, $7] activate=true
    <i64> $9 = global load $8
    <i32> $10 = cast_value<i32> $9
    $11 : local store [$6 <- $10]
    <i32> $12 = alloca
    <i32> $13 = local load [$6]
    $14 : local store [$12 <- $13]
    <f32> $15 = alloca
    <f32> $16 = const 0.0
    $17 : local store [$15 <- $16]
    <f32> $18 = alloca
    <f32> $19 = local load [$15]
    $20 : local store [$18 <- $19]
    <i32> $21 = const 0
    <i32> $22 = cast_value<i32> $21
    <i32> $23 = const 1
    <i32> $24 = cast_value<i32> $23
    $25 : for in range($22, $24) block_dim=adaptive {
      <i32> $26 = loop $25 index 0
      <f32> $27 = alloca
      <f32> $28 = global ptr [S2place<f32>], index [$26] activate=true
      <f32> $29 = global load $28
      <f32> $30 = atomic add($18, $29)
      $31 : local store [$27 <- $30]
    }
    <f32> $32 = alloca
    <f32> $33 = local load [$18]
    $34 : local store [$32 <- $33]
    <f32> $35 = local load [$32]
    <i32> $36 = local load [$12]
    <i32> $37 = const 0
    <f32> $38 = global ptr [S8place<f32>], index [$36, $37] activate=true
    $39 : global store [$38 <- $35]
  }
}
[I 04/20/23 19:15:56.064 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Immutable local vars eliminated:
kernel {
  <i32> $0 = const 0
  <i32> $1 = cast_value<i32> $0
  <i32> $2 = const 8192
  <i32> $3 = cast_value<i32> $2
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <i32> $6 = const 0
    <i64> $7 = global ptr [S6place<i64>], index [$5, $6] activate=true
    <i64> $8 = global load $7
    <i32> $9 = cast_value<i32> $8
    <f32> $10 = const 0.0
    <f32> $11 = alloca
    $12 : local store [$11 <- $10]
    <i32> $13 = const 0
    <i32> $14 = cast_value<i32> $13
    <i32> $15 = const 1
    <i32> $16 = cast_value<i32> $15
    $17 : for in range($14, $16) block_dim=adaptive {
      <i32> $18 = loop $17 index 0
      <f32> $19 = global ptr [S2place<f32>], index [$18] activate=true
      <f32> $20 = global load $19
      <f32> $21 = atomic add($11, $20)
    }
    <f32> $22 = local load [$11]
    <i32> $23 = const 0
    <f32> $24 = global ptr [S8place<f32>], index [$9, $23] activate=true
    $25 : global store [$24 <- $22]
  }
}
[I 04/20/23 19:15:56.064 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Typechecked:
kernel {
  <i32> $0 = const 0
  <i32> $1 = cast_value<i32> $0
  <i32> $2 = const 8192
  <i32> $3 = cast_value<i32> $2
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <i32> $6 = const 0
    <*i64> $7 = global ptr [S6place<i64>], index [$5, $6] activate=true
    <i64> $8 = global load $7
    <i32> $9 = cast_value<i32> $8
    <f32> $10 = const 0.0
    <f32> $11 = alloca
    <f32> $12 : local store [$11 <- $10]
    <i32> $13 = const 0
    <i32> $14 = cast_value<i32> $13
    <i32> $15 = const 1
    <i32> $16 = cast_value<i32> $15
    $17 : for in range($14, $16) block_dim=adaptive {
      <i32> $18 = loop $17 index 0
      <*f32> $19 = global ptr [S2place<f32>], index [$18] activate=true
      <f32> $20 = global load $19
      <f32> $21 = atomic add($11, $20)
    }
    <f32> $22 = local load [$11]
    <i32> $23 = const 0
    <*f32> $24 = global ptr [S8place<f32>], index [$9, $23] activate=true
    $25 : global store [$24 <- $22]
  }
}
[I 04/20/23 19:15:56.064 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Bit Loop Vectorized:
kernel {
  <i32> $0 = const 0
  <i32> $1 = cast_value<i32> $0
  <i32> $2 = const 8192
  <i32> $3 = cast_value<i32> $2
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <i32> $6 = const 0
    <*i64> $7 = global ptr [S6place<i64>], index [$5, $6] activate=true
    <i64> $8 = global load $7
    <i32> $9 = cast_value<i32> $8
    <f32> $10 = const 0.0
    <f32> $11 = alloca
    <f32> $12 : local store [$11 <- $10]
    <i32> $13 = const 0
    <i32> $14 = cast_value<i32> $13
    <i32> $15 = const 1
    <i32> $16 = cast_value<i32> $15
    $17 : for in range($14, $16) block_dim=adaptive {
      <i32> $18 = loop $17 index 0
      <*f32> $19 = global ptr [S2place<f32>], index [$18] activate=true
      <f32> $20 = global load $19
      <f32> $21 = atomic add($11, $20)
    }
    <f32> $22 = local load [$11]
    <i32> $23 = const 0
    <*f32> $24 = global ptr [S8place<f32>], index [$9, $23] activate=true
    $25 : global store [$24 <- $22]
  }
}
[I 04/20/23 19:15:56.064 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Matrix ptr lowered:
kernel {
  <i32> $0 = const 0
  <i32> $1 = cast_value<i32> $0
  <i32> $2 = const 8192
  <i32> $3 = cast_value<i32> $2
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <i32> $6 = const 0
    <*i64> $7 = global ptr [S6place<i64>], index [$5, $6] activate=true
    <i64> $8 = global load $7
    <i32> $9 = cast_value<i32> $8
    <f32> $10 = const 0.0
    <f32> $11 = alloca
    <f32> $12 : local store [$11 <- $10]
    <i32> $13 = const 0
    <i32> $14 = cast_value<i32> $13
    <i32> $15 = const 1
    <i32> $16 = cast_value<i32> $15
    $17 : for in range($14, $16) block_dim=adaptive {
      <i32> $18 = loop $17 index 0
      <*f32> $19 = global ptr [S2place<f32>], index [$18] activate=true
      <f32> $20 = global load $19
      <f32> $21 = atomic add($11, $20)
    }
    <f32> $22 = local load [$11]
    <i32> $23 = const 0
    <*f32> $24 = global ptr [S8place<f32>], index [$9, $23] activate=true
    $25 : global store [$24 <- $22]
  }
}
[I 04/20/23 19:15:56.064 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Simplified I:
kernel {
  <i32> $0 = const 0
  <i32> $1 = const 1
  <i32> $2 = const 8192
  $3 : for in range($0, $2) block_dim=256 {
    <i32> $4 = loop $3 index 0
    <*i64> $5 = global ptr [S6place<i64>], index [$4, $0] activate=true
    <i64> $6 = global load $5
    <i32> $7 = cast_value<i32> $6
    <f32> $8 = alloca
    $9 : for in range($0, $1) block_dim=adaptive {
      <i32> $10 = loop $9 index 0
      <*f32> $11 = global ptr [S2place<f32>], index [$10] activate=true
      <f32> $12 = global load $11
      <f32> $13 = atomic add($8, $12)
    }
    <f32> $14 = local load [$8]
    <*f32> $15 = global ptr [S8place<f32>], index [$7, $0] activate=true
    $16 : global store [$15 <- $14]
  }
}
[I 04/20/23 19:15:56.064 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Scalarized:
kernel {
  <i32> $0 = const 0
  <i32> $1 = const 1
  <i32> $2 = const 8192
  $3 : for in range($0, $2) block_dim=256 {
    <i32> $4 = loop $3 index 0
    <*i64> $5 = global ptr [S6place<i64>], index [$4, $0] activate=true
    <i64> $6 = global load $5
    <i32> $7 = cast_value<i32> $6
    <f32> $8 = alloca
    $9 : for in range($0, $1) block_dim=adaptive {
      <i32> $10 = loop $9 index 0
      <*f32> $11 = global ptr [S2place<f32>], index [$10] activate=true
      <f32> $12 = global load $11
      <f32> $13 = atomic add($8, $12)
    }
    <f32> $14 = local load [$8]
    <*f32> $15 = global ptr [S8place<f32>], index [$7, $0] activate=true
    $16 : global store [$15 <- $14]
  }
}
[I 04/20/23 19:15:56.065 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Access flagged I:
kernel {
  <i32> $0 = const 0
  <i32> $1 = const 1
  <i32> $2 = const 8192
  $3 : for in range($0, $2) block_dim=256 {
    <i32> $4 = loop $3 index 0
    <*i64> $5 = global ptr [S6place<i64>], index [$4, $0] activate=false
    <i64> $6 = global load $5
    <i32> $7 = cast_value<i32> $6
    <f32> $8 = alloca
    $9 : for in range($0, $1) block_dim=adaptive {
      <i32> $10 = loop $9 index 0
      <*f32> $11 = global ptr [S2place<f32>], index [$10] activate=false
      <f32> $12 = global load $11
      <f32> $13 = atomic add($8, $12)
    }
    <f32> $14 = local load [$8]
    <*f32> $15 = global ptr [S8place<f32>], index [$7, $0] activate=true
    $16 : global store [$15 <- $14]
  }
}
[I 04/20/23 19:15:56.065 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Simplified II:
kernel {
  <i32> $0 = const 0
  <i32> $1 = const 1
  <i32> $2 = const 8192
  $3 : for in range($0, $2) block_dim=256 {
    <i32> $4 = loop $3 index 0
    <*i64> $5 = global ptr [S6place<i64>], index [$4, $0] activate=false
    <i64> $6 = global load $5
    <i32> $7 = cast_value<i32> $6
    <f32> $8 = alloca
    $9 : for in range($0, $1) block_dim=adaptive {
      <i32> $10 = loop $9 index 0
      <*f32> $11 = global ptr [S2place<f32>], index [$10] activate=false
      <f32> $12 = global load $11
      <f32> $13 = atomic add($8, $12)
    }
    <f32> $14 = local load [$8]
    <*f32> $15 = global ptr [S8place<f32>], index [$7, $0] activate=true
    $16 : global store [$15 <- $14]
  }
}
[I 04/20/23 19:15:56.065 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Offloaded:
kernel {
  $0 = offloaded
  body {
    <i32> $1 = const 0
    <i32> $2 = const 1
    <i32> $3 = const 8192
  }
  $4 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
  body {
    <i32> $5 = loop $4 index 0
    <i32> $6 = const 0
    <*i64> $7 = global ptr [S6place<i64>], index [$5, $6] activate=false
    <i64> $8 = global load $7
    <i32> $9 = cast_value<i32> $8
    <f32> $10 = alloca
    <i32> $11 = const 0
    <i32> $12 = const 1
    $13 : for in range($11, $12) block_dim=adaptive {
      <i32> $14 = loop $13 index 0
      <*f32> $15 = global ptr [S2place<f32>], index [$14] activate=false
      <f32> $16 = global load $15
      <f32> $17 = atomic add($10, $16)
    }
    <f32> $18 = local load [$10]
    <i32> $19 = const 0
    <*f32> $20 = global ptr [S8place<f32>], index [$9, $19] activate=true
    $21 : global store [$20 <- $18]
  }
}
[I 04/20/23 19:15:56.065 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Optimized by CFG:
kernel {
  $0 = offloaded
  body {
  }
  $1 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
  body {
    <i32> $2 = loop $1 index 0
    <i32> $3 = const 0
    <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
    <i64> $5 = global load $4
    <i32> $6 = cast_value<i32> $5
    <f32> $7 = alloca
    <i32> $8 = const 0
    <i32> $9 = const 1
    $10 : for in range($8, $9) block_dim=adaptive {
      <i32> $11 = loop $10 index 0
      <*f32> $12 = global ptr [S2place<f32>], index [$11] activate=false
      <f32> $13 = global load $12
      <f32> $14 = atomic add($7, $13)
    }
    <f32> $15 = local load [$7]
    <i32> $16 = const 0
    <*f32> $17 = global ptr [S8place<f32>], index [$6, $16] activate=true
    $18 : global store [$17 <- $15]
  }
}
[I 04/20/23 19:15:56.065 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Access flagged II:
kernel {
  $0 = offloaded
  body {
  }
  $1 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
  body {
    <i32> $2 = loop $1 index 0
    <i32> $3 = const 0
    <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
    <i64> $5 = global load $4
    <i32> $6 = cast_value<i32> $5
    <f32> $7 = alloca
    <i32> $8 = const 0
    <i32> $9 = const 1
    $10 : for in range($8, $9) block_dim=adaptive {
      <i32> $11 = loop $10 index 0
      <*f32> $12 = global ptr [S2place<f32>], index [$11] activate=false
      <f32> $13 = global load $12
      <f32> $14 = atomic add($7, $13)
    }
    <f32> $15 = local load [$7]
    <i32> $16 = const 0
    <*f32> $17 = global ptr [S8place<f32>], index [$6, $16] activate=true
    $18 : global store [$17 <- $15]
  }
}
[I 04/20/23 19:15:56.065 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Simplified III:
kernel {
  $0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
  body {
    <i32> $1 = loop $0 index 0
    <i32> $2 = const 0
    <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
    <i64> $4 = global load $3
    <i32> $5 = cast_value<i32> $4
    <f32> $6 = alloca
    <i32> $7 = const 1
    $8 : for in range($2, $7) block_dim=adaptive {
      <i32> $9 = loop $8 index 0
      <*f32> $10 = global ptr [S2place<f32>], index [$9] activate=false
      <f32> $11 = global load $10
      <f32> $12 = atomic add($6, $11)
    }
    <f32> $13 = local load [$6]
    <*f32> $14 = global ptr [S8place<f32>], index [$5, $2] activate=true
    $15 : global store [$14 <- $13]
  }
}
[I 04/20/23 19:15:56.065 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Start offload_to_executable:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <f32> $6 = alloca
  <i32> $7 = const 1
  $8 : for in range($2, $7) block_dim=adaptive {
    <i32> $9 = loop $8 index 0
    <*f32> $10 = global ptr [S2place<f32>], index [$9] activate=false
    <f32> $11 = global load $10
    <f32> $12 = atomic add($6, $11)
  }
  <f32> $13 = local load [$6]
  <*f32> $14 = global ptr [S8place<f32>], index [$5, $2] activate=true
  $15 : global store [$14 <- $13]
}
}
[I 04/20/23 19:15:56.065 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Detect read-only accesses:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <f32> $6 = alloca
  <i32> $7 = const 1
  $8 : for in range($2, $7) block_dim=adaptive {
    <i32> $9 = loop $8 index 0
    <*f32> $10 = global ptr [S2place<f32>], index [$9] activate=false
    <f32> $11 = global load $10
    <f32> $12 = atomic add($6, $11)
  }
  <f32> $13 = local load [$6]
  <*f32> $14 = global ptr [S8place<f32>], index [$5, $2] activate=true
  $15 : global store [$14 <- $13]
}
}
[I 04/20/23 19:15:56.066 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Atomics demoted I:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <f32> $6 = alloca
  <i32> $7 = const 1
  $8 : for in range($2, $7) block_dim=adaptive {
    <i32> $9 = loop $8 index 0
    <*f32> $10 = global ptr [S2place<f32>], index [$9] activate=false
    <f32> $11 = global load $10
    <f32> $12 = local load [$6]
    <f32> $13 = add $12 $11
    <f32> $14 : local store [$6 <- $13]
  }
  <f32> $15 = local load [$6]
  <*f32> $16 = global ptr [S8place<f32>], index [$5, $2] activate=true
  $17 : global store [$16 <- $15]
}
}
[I 04/20/23 19:15:56.066 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Cache loop-invariant global vars:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <f32> $6 = alloca
  <i32> $7 = const 1
  $8 : for in range($2, $7) block_dim=adaptive {
    <i32> $9 = loop $8 index 0
    <*f32> $10 = global ptr [S2place<f32>], index [$9] activate=false
    <f32> $11 = global load $10
    <f32> $12 = local load [$6]
    <f32> $13 = add $12 $11
    <f32> $14 : local store [$6 <- $13]
  }
  <f32> $15 = local load [$6]
  <*f32> $16 = global ptr [S8place<f32>], index [$5, $2] activate=true
  $17 : global store [$16 <- $15]
}
}
[I 04/20/23 19:15:56.066 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Dense struct-for demoted:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <f32> $6 = alloca
  <i32> $7 = const 1
  $8 : for in range($2, $7) block_dim=adaptive {
    <i32> $9 = loop $8 index 0
    <*f32> $10 = global ptr [S2place<f32>], index [$9] activate=false
    <f32> $11 = global load $10
    <f32> $12 = local load [$6]
    <f32> $13 = add $12 $11
    <f32> $14 : local store [$6 <- $13]
  }
  <f32> $15 = local load [$6]
  <*f32> $16 = global ptr [S8place<f32>], index [$5, $2] activate=true
  $17 : global store [$16 <- $15]
}
}
[I 04/20/23 19:15:56.066 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] No-access mesh-for demoted:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <f32> $6 = alloca
  <i32> $7 = const 1
  $8 : for in range($2, $7) block_dim=adaptive {
    <i32> $9 = loop $8 index 0
    <*f32> $10 = global ptr [S2place<f32>], index [$9] activate=false
    <f32> $11 = global load $10
    <f32> $12 = local load [$6]
    <f32> $13 = add $12 $11
    <f32> $14 : local store [$6 <- $13]
  }
  <f32> $15 = local load [$6]
  <*f32> $16 = global ptr [S8place<f32>], index [$5, $2] activate=true
  $17 : global store [$16 <- $15]
}
}
[I 04/20/23 19:15:56.066 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Make thread local:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <f32> $6 = alloca
  <i32> $7 = const 1
  $8 : for in range($2, $7) block_dim=adaptive {
    <i32> $9 = loop $8 index 0
    <*f32> $10 = global ptr [S2place<f32>], index [$9] activate=false
    <f32> $11 = global load $10
    <f32> $12 = local load [$6]
    <f32> $13 = add $12 $11
    <f32> $14 : local store [$6 <- $13]
  }
  <f32> $15 = local load [$6]
  <*f32> $16 = global ptr [S8place<f32>], index [$5, $2] activate=true
  $17 : global store [$16 <- $15]
}
}
[I 04/20/23 19:15:56.066 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Make mesh thread local:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <f32> $6 = alloca
  <i32> $7 = const 1
  $8 : for in range($2, $7) block_dim=adaptive {
    <i32> $9 = loop $8 index 0
    <*f32> $10 = global ptr [S2place<f32>], index [$9] activate=false
    <f32> $11 = global load $10
    <f32> $12 = local load [$6]
    <f32> $13 = add $12 $11
    <f32> $14 : local store [$6 <- $13]
  }
  <f32> $15 = local load [$6]
  <*f32> $16 = global ptr [S8place<f32>], index [$5, $2] activate=true
  $17 : global store [$16 <- $15]
}
}
[I 04/20/23 19:15:56.066 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Make mesh block local:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <f32> $6 = alloca
  <i32> $7 = const 1
  $8 : for in range($2, $7) block_dim=adaptive {
    <i32> $9 = loop $8 index 0
    <*f32> $10 = global ptr [S2place<f32>], index [$9] activate=false
    <f32> $11 = global load $10
    <f32> $12 = local load [$6]
    <f32> $13 = add $12 $11
    <f32> $14 : local store [$6 <- $13]
  }
  <f32> $15 = local load [$6]
  <*f32> $16 = global ptr [S8place<f32>], index [$5, $2] activate=true
  $17 : global store [$16 <- $15]
}
}
[I 04/20/23 19:15:56.066 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Simplified X:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <f32> $6 = alloca
  <i32> $7 = const 1
  $8 : for in range($2, $7) block_dim=adaptive {
    <i32> $9 = loop $8 index 0
    <*f32> $10 = global ptr [S2place<f32>], index [$9] activate=false
    <f32> $11 = global load $10
    <f32> $12 = local load [$6]
    <f32> $13 = add $12 $11
    <f32> $14 : local store [$6 <- $13]
  }
  <f32> $15 = local load [$6]
  <*f32> $16 = global ptr [S8place<f32>], index [$5, $2] activate=true
  $17 : global store [$16 <- $15]
}
}
[I 04/20/23 19:15:56.067 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Make block local:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <f32> $6 = alloca
  <i32> $7 = const 1
  $8 : for in range($2, $7) block_dim=adaptive {
    <i32> $9 = loop $8 index 0
    <*f32> $10 = global ptr [S2place<f32>], index [$9] activate=false
    <f32> $11 = global load $10
    <f32> $12 = local load [$6]
    <f32> $13 = add $12 $11
    <f32> $14 : local store [$6 <- $13]
  }
  <f32> $15 = local load [$6]
  <*f32> $16 = global ptr [S8place<f32>], index [$5, $2] activate=true
  $17 : global store [$16 <- $15]
}
}
[I 04/20/23 19:15:56.067 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Demote mesh statements:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <f32> $6 = alloca
  <i32> $7 = const 1
  $8 : for in range($2, $7) block_dim=adaptive {
    <i32> $9 = loop $8 index 0
    <*f32> $10 = global ptr [S2place<f32>], index [$9] activate=false
    <f32> $11 = global load $10
    <f32> $12 = local load [$6]
    <f32> $13 = add $12 $11
    <f32> $14 : local store [$6 <- $13]
  }
  <f32> $15 = local load [$6]
  <*f32> $16 = global ptr [S8place<f32>], index [$5, $2] activate=true
  $17 : global store [$16 <- $15]
}
}
[I 04/20/23 19:15:56.067 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Atomics demoted II:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <f32> $6 = alloca
  <i32> $7 = const 1
  $8 : for in range($2, $7) block_dim=adaptive {
    <i32> $9 = loop $8 index 0
    <*f32> $10 = global ptr [S2place<f32>], index [$9] activate=false
    <f32> $11 = global load $10
    <f32> $12 = local load [$6]
    <f32> $13 = add $12 $11
    <f32> $14 : local store [$6 <- $13]
  }
  <f32> $15 = local load [$6]
  <*f32> $16 = global ptr [S8place<f32>], index [$5, $2] activate=true
  $17 : global store [$16 <- $15]
}
}
[I 04/20/23 19:15:56.067 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Remove range assumption:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <f32> $6 = alloca
  <i32> $7 = const 1
  $8 : for in range($2, $7) block_dim=adaptive {
    <i32> $9 = loop $8 index 0
    <*f32> $10 = global ptr [S2place<f32>], index [$9] activate=false
    <f32> $11 = global load $10
    <f32> $12 = local load [$6]
    <f32> $13 = add $12 $11
    <f32> $14 : local store [$6 <- $13]
  }
  <f32> $15 = local load [$6]
  <*f32> $16 = global ptr [S8place<f32>], index [$5, $2] activate=true
  $17 : global store [$16 <- $15]
}
}
[I 04/20/23 19:15:56.067 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Remove loop_unique:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <f32> $6 = alloca
  <i32> $7 = const 1
  $8 : for in range($2, $7) block_dim=adaptive {
    <i32> $9 = loop $8 index 0
    <*f32> $10 = global ptr [S2place<f32>], index [$9] activate=false
    <f32> $11 = global load $10
    <f32> $12 = local load [$6]
    <f32> $13 = add $12 $11
    <f32> $14 : local store [$6 <- $13]
  }
  <f32> $15 = local load [$6]
  <*f32> $16 = global ptr [S8place<f32>], index [$5, $2] activate=true
  $17 : global store [$16 <- $15]
}
}
[I 04/20/23 19:15:56.067 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Simplified before lower access:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <f32> $6 = alloca
  <i32> $7 = const 1
  $8 : for in range($2, $7) block_dim=adaptive {
    <i32> $9 = loop $8 index 0
    <*f32> $10 = global ptr [S2place<f32>], index [$9] activate=false
    <f32> $11 = global load $10
    <f32> $12 = local load [$6]
    <f32> $13 = add $12 $11
    <f32> $14 : local store [$6 <- $13]
  }
  <f32> $15 = local load [$6]
  <*f32> $16 = global ptr [S8place<f32>], index [$5, $2] activate=true
  $17 : global store [$16 <- $15]
}
}
[I 04/20/23 19:15:56.067 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Access lowered:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <*gen> $4 = get root [S0root][root]
  <i32> $5 = linearized(ind {}, stride {})
  <*gen> $6 = [S0root][root]::lookup($4, $5) activate = false
  <*gen> $7 = get child [S0root->S5dense] $6
  <i32> $8 = const 0
  <i32> $9 = bit_shr $1 $8
  <i32> $10 = const 0
  <i32> $11 = bit_shr $2 $10
  <i32> $12 = linearized(ind {$9, $11}, stride {8192, 3})
  <*gen> $13 = [S5dense][dense]::lookup($7, $12) activate = false
  <*i64> $14 = get child [S5dense->S6place<i64>] $13
  <i64> $15 = global load $14
  <i32> $16 = cast_value<i32> $15
  <f32> $17 = alloca
  <i32> $18 = const 1
  $19 : for in range($2, $18) block_dim=adaptive {
    <i32> $20 = loop $19 index 0
    <*f32> $21 = global ptr [S2place<f32>], index [$20] activate=false
    <*gen> $22 = get root [S0root][root]
    <i32> $23 = linearized(ind {}, stride {})
    <*gen> $24 = [S0root][root]::lookup($22, $23) activate = false
    <*gen> $25 = get child [S0root->S1dense] $24
    <i32> $26 = const 0
    <i32> $27 = bit_shr $20 $26
    <i32> $28 = linearized(ind {$27}, stride {8388608})
    <*gen> $29 = [S1dense][dense]::lookup($25, $28) activate = false
    <*f32> $30 = get child [S1dense->S2place<f32>] $29
    <f32> $31 = global load $30
    <f32> $32 = local load [$17]
    <f32> $33 = add $32 $31
    <f32> $34 : local store [$17 <- $33]
  }
  <f32> $35 = local load [$17]
  <*f32> $36 = global ptr [S8place<f32>], index [$16, $2] activate=true
  <*gen> $37 = get root [S0root][root]
  <i32> $38 = linearized(ind {}, stride {})
  <*gen> $39 = [S0root][root]::lookup($37, $38) activate = false
  <*gen> $40 = get child [S0root->S7dense] $39
  <i32> $41 = const 0
  <i32> $42 = bit_shr $16 $41
  <i32> $43 = const 0
  <i32> $44 = bit_shr $2 $43
  <i32> $45 = linearized(ind {$42, $44}, stride {8192, 3})
  <*gen> $46 = [S7dense][dense]::lookup($40, $45) activate = false
  <*f32> $47 = get child [S7dense->S8place<f32>] $46
  $48 : global store [$47 <- $35]
}
}
[I 04/20/23 19:15:56.068 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] DIE:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*gen> $3 = get root [S0root][root]
  <i32> $4 = linearized(ind {}, stride {})
  <*gen> $5 = [S0root][root]::lookup($3, $4) activate = false
  <*gen> $6 = get child [S0root->S5dense] $5
  <i32> $7 = const 0
  <i32> $8 = bit_shr $1 $7
  <i32> $9 = const 0
  <i32> $10 = bit_shr $2 $9
  <i32> $11 = linearized(ind {$8, $10}, stride {8192, 3})
  <*gen> $12 = [S5dense][dense]::lookup($6, $11) activate = false
  <*i64> $13 = get child [S5dense->S6place<i64>] $12
  <i64> $14 = global load $13
  <i32> $15 = cast_value<i32> $14
  <f32> $16 = alloca
  <i32> $17 = const 1
  $18 : for in range($2, $17) block_dim=adaptive {
    <i32> $19 = loop $18 index 0
    <*gen> $20 = get root [S0root][root]
    <i32> $21 = linearized(ind {}, stride {})
    <*gen> $22 = [S0root][root]::lookup($20, $21) activate = false
    <*gen> $23 = get child [S0root->S1dense] $22
    <i32> $24 = const 0
    <i32> $25 = bit_shr $19 $24
    <i32> $26 = linearized(ind {$25}, stride {8388608})
    <*gen> $27 = [S1dense][dense]::lookup($23, $26) activate = false
    <*f32> $28 = get child [S1dense->S2place<f32>] $27
    <f32> $29 = global load $28
    <f32> $30 = local load [$16]
    <f32> $31 = add $30 $29
    <f32> $32 : local store [$16 <- $31]
  }
  <f32> $33 = local load [$16]
  <*f32> $34 = global ptr [S8place<f32>], index [$15, $2] activate=true
  <*gen> $35 = get root [S0root][root]
  <i32> $36 = linearized(ind {}, stride {})
  <*gen> $37 = [S0root][root]::lookup($35, $36) activate = false
  <*gen> $38 = get child [S0root->S7dense] $37
  <i32> $39 = const 0
  <i32> $40 = bit_shr $15 $39
  <i32> $41 = const 0
  <i32> $42 = bit_shr $2 $41
  <i32> $43 = linearized(ind {$40, $42}, stride {8192, 3})
  <*gen> $44 = [S7dense][dense]::lookup($38, $43) activate = false
  <*f32> $45 = get child [S7dense->S8place<f32>] $44
  $46 : global store [$45 <- $33]
}
}
[I 04/20/23 19:15:56.068 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Access flagged III:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*gen> $3 = get root [S0root][root]
  <i32> $4 = linearized(ind {}, stride {})
  <*gen> $5 = [S0root][root]::lookup($3, $4) activate = false
  <*gen> $6 = get child [S0root->S5dense] $5
  <i32> $7 = const 0
  <i32> $8 = bit_shr $1 $7
  <i32> $9 = const 0
  <i32> $10 = bit_shr $2 $9
  <i32> $11 = linearized(ind {$8, $10}, stride {8192, 3})
  <*gen> $12 = [S5dense][dense]::lookup($6, $11) activate = false
  <*i64> $13 = get child [S5dense->S6place<i64>] $12
  <i64> $14 = global load $13
  <i32> $15 = cast_value<i32> $14
  <f32> $16 = alloca
  <i32> $17 = const 1
  $18 : for in range($2, $17) block_dim=adaptive {
    <i32> $19 = loop $18 index 0
    <*gen> $20 = get root [S0root][root]
    <i32> $21 = linearized(ind {}, stride {})
    <*gen> $22 = [S0root][root]::lookup($20, $21) activate = false
    <*gen> $23 = get child [S0root->S1dense] $22
    <i32> $24 = const 0
    <i32> $25 = bit_shr $19 $24
    <i32> $26 = linearized(ind {$25}, stride {8388608})
    <*gen> $27 = [S1dense][dense]::lookup($23, $26) activate = false
    <*f32> $28 = get child [S1dense->S2place<f32>] $27
    <f32> $29 = global load $28
    <f32> $30 = local load [$16]
    <f32> $31 = add $30 $29
    <f32> $32 : local store [$16 <- $31]
  }
  <f32> $33 = local load [$16]
  <*f32> $34 = global ptr [S8place<f32>], index [$15, $2] activate=false
  <*gen> $35 = get root [S0root][root]
  <i32> $36 = linearized(ind {}, stride {})
  <*gen> $37 = [S0root][root]::lookup($35, $36) activate = false
  <*gen> $38 = get child [S0root->S7dense] $37
  <i32> $39 = const 0
  <i32> $40 = bit_shr $15 $39
  <i32> $41 = const 0
  <i32> $42 = bit_shr $2 $41
  <i32> $43 = linearized(ind {$40, $42}, stride {8192, 3})
  <*gen> $44 = [S7dense][dense]::lookup($38, $43) activate = false
  <*f32> $45 = get child [S7dense->S8place<f32>] $44
  $46 : global store [$45 <- $33]
}
}
[I 04/20/23 19:15:56.068 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Operations demoted:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*gen> $3 = get root [S0root][root]
  <i32> $4 = linearized(ind {}, stride {})
  <*gen> $5 = [S0root][root]::lookup($3, $4) activate = false
  <*gen> $6 = get child [S0root->S5dense] $5
  <i32> $7 = const 0
  <u32> $8 = reinterpret_cast_bits<u32> $1
  <u32> $9 = cast_value<u32> $7
  <u32> $10 = bit_sar $8 $9
  <i32> $11 = reinterpret_cast_bits<i32> $10
  <i32> $12 = const 0
  <u32> $13 = reinterpret_cast_bits<u32> $2
  <u32> $14 = cast_value<u32> $12
  <u32> $15 = bit_sar $13 $14
  <i32> $16 = reinterpret_cast_bits<i32> $15
  <i32> $17 = linearized(ind {$11, $16}, stride {8192, 3})
  <*gen> $18 = [S5dense][dense]::lookup($6, $17) activate = false
  <*i64> $19 = get child [S5dense->S6place<i64>] $18
  <i64> $20 = global load $19
  <i32> $21 = cast_value<i32> $20
  <f32> $22 = alloca
  <i32> $23 = const 1
  $24 : for in range($2, $23) block_dim=adaptive {
    <i32> $25 = loop $24 index 0
    <*gen> $26 = get root [S0root][root]
    <i32> $27 = linearized(ind {}, stride {})
    <*gen> $28 = [S0root][root]::lookup($26, $27) activate = false
    <*gen> $29 = get child [S0root->S1dense] $28
    <i32> $30 = const 0
    <u32> $31 = reinterpret_cast_bits<u32> $25
    <u32> $32 = cast_value<u32> $30
    <u32> $33 = bit_sar $31 $32
    <i32> $34 = reinterpret_cast_bits<i32> $33
    <i32> $35 = linearized(ind {$34}, stride {8388608})
    <*gen> $36 = [S1dense][dense]::lookup($29, $35) activate = false
    <*f32> $37 = get child [S1dense->S2place<f32>] $36
    <f32> $38 = global load $37
    <f32> $39 = local load [$22]
    <f32> $40 = add $39 $38
    <f32> $41 : local store [$22 <- $40]
  }
  <f32> $42 = local load [$22]
  <*f32> $43 = global ptr [S8place<f32>], index [$21, $2] activate=false
  <*gen> $44 = get root [S0root][root]
  <i32> $45 = linearized(ind {}, stride {})
  <*gen> $46 = [S0root][root]::lookup($44, $45) activate = false
  <*gen> $47 = get child [S0root->S7dense] $46
  <i32> $48 = const 0
  <u32> $49 = reinterpret_cast_bits<u32> $21
  <u32> $50 = cast_value<u32> $48
  <u32> $51 = bit_sar $49 $50
  <i32> $52 = reinterpret_cast_bits<i32> $51
  <i32> $53 = const 0
  <u32> $54 = reinterpret_cast_bits<u32> $2
  <u32> $55 = cast_value<u32> $53
  <u32> $56 = bit_sar $54 $55
  <i32> $57 = reinterpret_cast_bits<i32> $56
  <i32> $58 = linearized(ind {$52, $57}, stride {8192, 3})
  <*gen> $59 = [S7dense][dense]::lookup($47, $58) activate = false
  <*f32> $60 = get child [S7dense->S8place<f32>] $59
  $61 : global store [$60 <- $42]
}
}
[I 04/20/23 19:15:56.069 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Simplified IV:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*gen> $3 = get root [S0root][root]
  <*gen> $4 = [S0root][root]::lookup($3, $2) activate = false
  <*gen> $5 = get child [S0root->S5dense] $4
  <i32> $6 = const 1
  <i32> $7 = const 3
  <i32> $8 = mul $1 $7
  <*gen> $9 = [S5dense][dense]::lookup($5, $8) activate = false
  <*i64> $10 = get child [S5dense->S6place<i64>] $9
  <i64> $11 = global load $10
  <i32> $12 = cast_value<i32> $11
  <f32> $13 = alloca
  $14 : for in range($2, $6) block_dim=adaptive {
    <i32> $15 = loop $14 index 0
    <*gen> $16 = get child [S0root->S1dense] $4
    <*gen> $17 = [S1dense][dense]::lookup($16, $15) activate = false
    <*f32> $18 = get child [S1dense->S2place<f32>] $17
    <f32> $19 = global load $18
    <f32> $20 = local load [$13]
    <f32> $21 = add $20 $19
    <f32> $22 : local store [$13 <- $21]
  }
  <f32> $23 = local load [$13]
  <*gen> $24 = get child [S0root->S7dense] $4
  <i32> $25 = mul $12 $7
  <*gen> $26 = [S7dense][dense]::lookup($24, $25) activate = false
  <*f32> $27 = get child [S7dense->S8place<f32>] $26
  $28 : global store [$27 <- $23]
}
}
[I 04/20/23 19:15:56.069 1820179] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c78_0] Bit struct stores optimized:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*gen> $3 = get root [S0root][root]
  <*gen> $4 = [S0root][root]::lookup($3, $2) activate = false
  <*gen> $5 = get child [S0root->S5dense] $4
  <i32> $6 = const 1
  <i32> $7 = const 3
  <i32> $8 = mul $1 $7
  <*gen> $9 = [S5dense][dense]::lookup($5, $8) activate = false
  <*i64> $10 = get child [S5dense->S6place<i64>] $9
  <i64> $11 = global load $10
  <i32> $12 = cast_value<i32> $11
  <f32> $13 = alloca
  $14 : for in range($2, $6) block_dim=adaptive {
    <i32> $15 = loop $14 index 0
    <*gen> $16 = get child [S0root->S1dense] $4
    <*gen> $17 = [S1dense][dense]::lookup($16, $15) activate = false
    <*f32> $18 = get child [S1dense->S2place<f32>] $17
    <f32> $19 = global load $18
    <f32> $20 = local load [$13]
    <f32> $21 = add $20 $19
    <f32> $22 : local store [$13 <- $21]
  }
  <f32> $23 = local load [$13]
  <*gen> $24 = get child [S0root->S7dense] $4
  <i32> $25 = mul $12 $7
  <*gen> $26 = [S7dense][dense]::lookup($24, $25) activate = false
  <*f32> $27 = get child [S7dense->S8place<f32>] $26
  $28 : global store [$27 <- $23]
}
}
[I 04/20/23 19:15:56.107 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Initial IR:
kernel {
  $0 : for @tmp0, @tmp1 in S8place<f32> noneblock_dim=adaptive {
    $1 = alloca @tmp2
    @tmp2 = [@tmp0, @tmp1] (dt=[Tensor (2) i32])
    $3 = alloca @tmp3
    @tmp3 = #@tmp4 (snode=S8place<f32>)[@tmp2[0], @tmp2[1]]
    $5 = alloca @tmp4
    @tmp4 = [@tmp0, @tmp1] (dt=[Tensor (2) i32])
    2d_ext_arr (element_dim=0, dt=f32, grad=false)[@tmp4[0], @tmp4[1]] = @tmp3
  }
}
[I 04/20/23 19:15:56.107 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Lowered:
kernel {
  $0 : struct for in S7dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <[Tensor (2) i32]> $3 = alloca
    <[Tensor (2) i32]> $4 = [$1, $2]
    $5 : local store [$3 <- $4]
    <f32> $6 = alloca
    <i32> $7 = const 0
    <*i32> $8 = shift ptr [$3 + $7]
    <i32> $9 = local load [$8]
    <i32> $10 = const 1
    <*i32> $11 = shift ptr [$3 + $10]
    <i32> $12 = local load [$11]
    <f32> $13 = global ptr [S8place<f32>], index [$9, $12] activate=true
    <f32> $14 = global load $13
    $15 : local store [$6 <- $14]
    <[Tensor (2) i32]> $16 = alloca
    <[Tensor (2) i32]> $17 = [$1, $2]
    $18 : local store [$16 <- $17]
    <f32> $19 = local load [$6]
    <i32> $20 = const 0
    <*i32> $21 = shift ptr [$16 + $20]
    <i32> $22 = local load [$21]
    <i32> $23 = const 1
    <*i32> $24 = shift ptr [$16 + $23]
    <i32> $25 = local load [$24]
    <struct[none]{0(data_ptr, at 0B): *f32}> $26 = argaddr[0]
    <f32> $27 = external_ptr $26, [$22, $25] element_dim=0 layout=AOS is_grad=false
    $28 : global store [$27 <- $19]
  }
}
[I 04/20/23 19:15:56.107 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Immutable local vars eliminated:
kernel {
  $0 : struct for in S7dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <[Tensor (2) i32]> $3 = alloca
    <[Tensor (2) i32]> $4 = [$1, $2]
    $5 : local store [$3 <- $4]
    <i32> $6 = const 0
    <*i32> $7 = shift ptr [$3 + $6]
    <i32> $8 = local load [$7]
    <i32> $9 = const 1
    <*i32> $10 = shift ptr [$3 + $9]
    <i32> $11 = local load [$10]
    <f32> $12 = global ptr [S8place<f32>], index [$8, $11] activate=true
    <f32> $13 = global load $12
    <[Tensor (2) i32]> $14 = alloca
    <[Tensor (2) i32]> $15 = [$1, $2]
    $16 : local store [$14 <- $15]
    <i32> $17 = const 0
    <*i32> $18 = shift ptr [$14 + $17]
    <i32> $19 = local load [$18]
    <i32> $20 = const 1
    <*i32> $21 = shift ptr [$14 + $20]
    <i32> $22 = local load [$21]
    <struct[none]{0(data_ptr, at 0B): *f32}> $23 = argaddr[0]
    <f32> $24 = external_ptr $23, [$19, $22] element_dim=0 layout=AOS is_grad=false
    $25 : global store [$24 <- $13]
  }
}
[I 04/20/23 19:15:56.107 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Typechecked:
kernel {
  $0 : struct for in S7dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <[Tensor (2) i32]> $3 = alloca
    <[Tensor (2) i32]> $4 = [$1, $2]
    <[Tensor (2) i32]> $5 : local store [$3 <- $4]
    <i32> $6 = const 0
    <*i32> $7 = shift ptr [$3 + $6]
    <i32> $8 = local load [$7]
    <i32> $9 = const 1
    <*i32> $10 = shift ptr [$3 + $9]
    <i32> $11 = local load [$10]
    <*f32> $12 = global ptr [S8place<f32>], index [$8, $11] activate=true
    <f32> $13 = global load $12
    <[Tensor (2) i32]> $14 = alloca
    <[Tensor (2) i32]> $15 = [$1, $2]
    <[Tensor (2) i32]> $16 : local store [$14 <- $15]
    <i32> $17 = const 0
    <*i32> $18 = shift ptr [$14 + $17]
    <i32> $19 = local load [$18]
    <i32> $20 = const 1
    <*i32> $21 = shift ptr [$14 + $20]
    <i32> $22 = local load [$21]
    <*struct[none]{0(data_ptr, at 0B): *f32}> $23 = argaddr[0]
    <*f32> $24 = external_ptr $23, [$19, $22] element_dim=0 layout=AOS is_grad=false
    $25 : global store [$24 <- $13]
  }
}
[I 04/20/23 19:15:56.107 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Bit Loop Vectorized:
kernel {
  $0 : struct for in S7dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <[Tensor (2) i32]> $3 = alloca
    <[Tensor (2) i32]> $4 = [$1, $2]
    <[Tensor (2) i32]> $5 : local store [$3 <- $4]
    <i32> $6 = const 0
    <*i32> $7 = shift ptr [$3 + $6]
    <i32> $8 = local load [$7]
    <i32> $9 = const 1
    <*i32> $10 = shift ptr [$3 + $9]
    <i32> $11 = local load [$10]
    <*f32> $12 = global ptr [S8place<f32>], index [$8, $11] activate=true
    <f32> $13 = global load $12
    <[Tensor (2) i32]> $14 = alloca
    <[Tensor (2) i32]> $15 = [$1, $2]
    <[Tensor (2) i32]> $16 : local store [$14 <- $15]
    <i32> $17 = const 0
    <*i32> $18 = shift ptr [$14 + $17]
    <i32> $19 = local load [$18]
    <i32> $20 = const 1
    <*i32> $21 = shift ptr [$14 + $20]
    <i32> $22 = local load [$21]
    <*struct[none]{0(data_ptr, at 0B): *f32}> $23 = argaddr[0]
    <*f32> $24 = external_ptr $23, [$19, $22] element_dim=0 layout=AOS is_grad=false
    $25 : global store [$24 <- $13]
  }
}
[I 04/20/23 19:15:56.108 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Matrix ptr lowered:
kernel {
  $0 : struct for in S7dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <[Tensor (2) i32]> $3 = alloca
    <[Tensor (2) i32]> $4 = [$1, $2]
    <[Tensor (2) i32]> $5 : local store [$3 <- $4]
    <i32> $6 = const 0
    <*i32> $7 = shift ptr [$3 + $6]
    <i32> $8 = local load [$7]
    <i32> $9 = const 1
    <*i32> $10 = shift ptr [$3 + $9]
    <i32> $11 = local load [$10]
    <*f32> $12 = global ptr [S8place<f32>], index [$8, $11] activate=true
    <f32> $13 = global load $12
    <[Tensor (2) i32]> $14 = alloca
    <[Tensor (2) i32]> $15 = [$1, $2]
    <[Tensor (2) i32]> $16 : local store [$14 <- $15]
    <i32> $17 = const 0
    <*i32> $18 = shift ptr [$14 + $17]
    <i32> $19 = local load [$18]
    <i32> $20 = const 1
    <*i32> $21 = shift ptr [$14 + $20]
    <i32> $22 = local load [$21]
    <*struct[none]{0(data_ptr, at 0B): *f32}> $23 = argaddr[0]
    <*f32> $24 = external_ptr $23, [$19, $22] element_dim=0 layout=AOS is_grad=false
    $25 : global store [$24 <- $13]
  }
}
[I 04/20/23 19:15:56.108 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Simplified I:
kernel {
  <i32> $0 = const 1
  <i32> $1 = const 0
  $2 : struct for in S7dense noneblock_dim=adaptive {
    <i32> $3 = loop $2 index 0
    <i32> $4 = loop $2 index 1
    <[Tensor (2) i32]> $5 = alloca
    <[Tensor (2) i32]> $6 = [$3, $4]
    <[Tensor (2) i32]> $7 : local store [$5 <- $6]
    <*i32> $8 = shift ptr [$5 + $1]
    <i32> $9 = local load [$8]
    <*i32> $10 = shift ptr [$5 + $0]
    <i32> $11 = local load [$10]
    <*f32> $12 = global ptr [S8place<f32>], index [$9, $11] activate=true
    <f32> $13 = global load $12
    <[Tensor (2) i32]> $14 = alloca
    <[Tensor (2) i32]> $15 : local store [$14 <- $6]
    <*i32> $16 = shift ptr [$14 + $1]
    <i32> $17 = local load [$16]
    <*i32> $18 = shift ptr [$14 + $0]
    <i32> $19 = local load [$18]
    <*struct[none]{0(data_ptr, at 0B): *f32}> $20 = argaddr[0]
    <*f32> $21 = external_ptr $20, [$17, $19] element_dim=0 layout=AOS is_grad=false
    $22 : global store [$21 <- $13]
  }
}
[I 04/20/23 19:15:56.108 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Scalarized:
kernel {
  $0 : struct for in S7dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <i32> $3 = alloca
    <i32> $4 = alloca
    $5 : local store [$3 <- $1]
    $6 : local store [$4 <- $2]
    <i32> $7 = local load [$3]
    <i32> $8 = local load [$4]
    <*f32> $9 = global ptr [S8place<f32>], index [$7, $8] activate=true
    <f32> $10 = global load $9
    <i32> $11 = alloca
    <i32> $12 = alloca
    $13 : local store [$11 <- $1]
    $14 : local store [$12 <- $2]
    <i32> $15 = local load [$11]
    <i32> $16 = local load [$12]
    <*struct[none]{0(data_ptr, at 0B): *f32}> $17 = argaddr[0]
    <*f32> $18 = external_ptr $17, [$15, $16] element_dim=0 layout=AOS is_grad=false
    $19 : global store [$18 <- $10]
  }
}
[I 04/20/23 19:15:56.108 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Access flagged I:
kernel {
  $0 : struct for in S7dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <i32> $3 = alloca
    <i32> $4 = alloca
    $5 : local store [$3 <- $1]
    $6 : local store [$4 <- $2]
    <i32> $7 = local load [$3]
    <i32> $8 = local load [$4]
    <*f32> $9 = global ptr [S8place<f32>], index [$7, $8] activate=false
    <f32> $10 = global load $9
    <i32> $11 = alloca
    <i32> $12 = alloca
    $13 : local store [$11 <- $1]
    $14 : local store [$12 <- $2]
    <i32> $15 = local load [$11]
    <i32> $16 = local load [$12]
    <*struct[none]{0(data_ptr, at 0B): *f32}> $17 = argaddr[0]
    <*f32> $18 = external_ptr $17, [$15, $16] element_dim=0 layout=AOS is_grad=false
    $19 : global store [$18 <- $10]
  }
}
[I 04/20/23 19:15:56.108 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Simplified II:
kernel {
  $0 : struct for in S7dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <*f32> $3 = global ptr [S8place<f32>], index [$1, $2] activate=false
    <f32> $4 = global load $3
    <*struct[none]{0(data_ptr, at 0B): *f32}> $5 = argaddr[0]
    <*f32> $6 = external_ptr $5, [$1, $2] element_dim=0 layout=AOS is_grad=false
    $7 : global store [$6 <- $4]
  }
}
[I 04/20/23 19:15:56.108 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Offloaded:
kernel {
  $0 = offloaded struct_for(S7dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <*f32> $3 = global ptr [S8place<f32>], index [$1, $2] activate=false
    <f32> $4 = global load $3
    <*struct[none]{0(data_ptr, at 0B): *f32}> $5 = argaddr[0]
    <*f32> $6 = external_ptr $5, [$1, $2] element_dim=0 layout=AOS is_grad=false
    $7 : global store [$6 <- $4]
  }
}
[I 04/20/23 19:15:56.108 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Optimized by CFG:
kernel {
  $0 = offloaded struct_for(S7dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <*f32> $3 = global ptr [S8place<f32>], index [$1, $2] activate=false
    <f32> $4 = global load $3
    <*struct[none]{0(data_ptr, at 0B): *f32}> $5 = argaddr[0]
    <*f32> $6 = external_ptr $5, [$1, $2] element_dim=0 layout=AOS is_grad=false
    $7 : global store [$6 <- $4]
  }
}
[I 04/20/23 19:15:56.108 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Access flagged II:
kernel {
  $0 = offloaded struct_for(S7dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <*f32> $3 = global ptr [S8place<f32>], index [$1, $2] activate=false
    <f32> $4 = global load $3
    <*struct[none]{0(data_ptr, at 0B): *f32}> $5 = argaddr[0]
    <*f32> $6 = external_ptr $5, [$1, $2] element_dim=0 layout=AOS is_grad=false
    $7 : global store [$6 <- $4]
  }
}
[I 04/20/23 19:15:56.108 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Simplified III:
kernel {
  $0 = offloaded struct_for(S7dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $1 = loop $0 index 0
    <i32> $2 = loop $0 index 1
    <*f32> $3 = global ptr [S8place<f32>], index [$1, $2] activate=false
    <f32> $4 = global load $3
    <*struct[none]{0(data_ptr, at 0B): *f32}> $5 = argaddr[0]
    <*f32> $6 = external_ptr $5, [$1, $2] element_dim=0 layout=AOS is_grad=false
    $7 : global store [$6 <- $4]
  }
}
[I 04/20/23 19:15:56.108 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Start offload_to_executable:
kernel {
$0 = offloaded struct_for(S7dense) grid_dim=2624 block_dim=128 bls=none
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = loop $0 index 1
  <*f32> $3 = global ptr [S8place<f32>], index [$1, $2] activate=false
  <f32> $4 = global load $3
  <*struct[none]{0(data_ptr, at 0B): *f32}> $5 = argaddr[0]
  <*f32> $6 = external_ptr $5, [$1, $2] element_dim=0 layout=AOS is_grad=false
  $7 : global store [$6 <- $4]
}
}
[I 04/20/23 19:15:56.109 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Detect read-only accesses:
kernel {
$0 = offloaded struct_for(S7dense) grid_dim=2624 block_dim=128 bls=mem_access_opt [ S8place<f32>:read_only ]
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = loop $0 index 1
  <*f32> $3 = global ptr [S8place<f32>], index [$1, $2] activate=false
  <f32> $4 = global load $3
  <*struct[none]{0(data_ptr, at 0B): *f32}> $5 = argaddr[0]
  <*f32> $6 = external_ptr $5, [$1, $2] element_dim=0 layout=AOS is_grad=false
  $7 : global store [$6 <- $4]
}
}
[I 04/20/23 19:15:56.109 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Atomics demoted I:
kernel {
$0 = offloaded struct_for(S7dense) grid_dim=2624 block_dim=128 bls=mem_access_opt [ S8place<f32>:read_only ]
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = loop $0 index 1
  <*f32> $3 = global ptr [S8place<f32>], index [$1, $2] activate=false
  <f32> $4 = global load $3
  <*struct[none]{0(data_ptr, at 0B): *f32}> $5 = argaddr[0]
  <*f32> $6 = external_ptr $5, [$1, $2] element_dim=0 layout=AOS is_grad=false
  $7 : global store [$6 <- $4]
}
}
[I 04/20/23 19:15:56.109 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Cache loop-invariant global vars:
kernel {
$0 = offloaded struct_for(S7dense) grid_dim=2624 block_dim=128 bls=mem_access_opt [ S8place<f32>:read_only ]
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = loop $0 index 1
  <*f32> $3 = global ptr [S8place<f32>], index [$1, $2] activate=false
  <f32> $4 = global load $3
  <*struct[none]{0(data_ptr, at 0B): *f32}> $5 = argaddr[0]
  <*f32> $6 = external_ptr $5, [$1, $2] element_dim=0 layout=AOS is_grad=false
  $7 : global store [$6 <- $4]
}
}
[I 04/20/23 19:15:56.109 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Dense struct-for demoted:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = const 0
  <i32> $3 = loop $0 index 0
  <i32> $4 = const 0
  <i32> $5 = bit_shr $3 $4
  <i32> $6 = const 3
  <i32> $7 = div $5 $6
  <i32> $8 = const 1
  <i32> $9 = mul $7 $8
  <i32> $10 = add $1 $9
  <i32> $11 = const 3
  <i32> $12 = mod $5 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $12 $13
  <i32> $15 = const 1
  <i32> $16 = mul $14 $15
  <i32> $17 = add $2 $16
  <*f32> $18 = global ptr [S8place<f32>], index [$10, $17] activate=false
  <f32> $19 = global load $18
  <*struct[none]{0(data_ptr, at 0B): *f32}> $20 = argaddr[0]
  <*f32> $21 = external_ptr $20, [$10, $17] element_dim=0 layout=AOS is_grad=false
  $22 : global store [$21 <- $19]
}
}
[I 04/20/23 19:15:56.109 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] No-access mesh-for demoted:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = const 0
  <i32> $3 = loop $0 index 0
  <i32> $4 = const 0
  <i32> $5 = bit_shr $3 $4
  <i32> $6 = const 3
  <i32> $7 = div $5 $6
  <i32> $8 = const 1
  <i32> $9 = mul $7 $8
  <i32> $10 = add $1 $9
  <i32> $11 = const 3
  <i32> $12 = mod $5 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $12 $13
  <i32> $15 = const 1
  <i32> $16 = mul $14 $15
  <i32> $17 = add $2 $16
  <*f32> $18 = global ptr [S8place<f32>], index [$10, $17] activate=false
  <f32> $19 = global load $18
  <*struct[none]{0(data_ptr, at 0B): *f32}> $20 = argaddr[0]
  <*f32> $21 = external_ptr $20, [$10, $17] element_dim=0 layout=AOS is_grad=false
  $22 : global store [$21 <- $19]
}
}
[I 04/20/23 19:15:56.109 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Make thread local:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = const 0
  <i32> $3 = loop $0 index 0
  <i32> $4 = const 0
  <i32> $5 = bit_shr $3 $4
  <i32> $6 = const 3
  <i32> $7 = div $5 $6
  <i32> $8 = const 1
  <i32> $9 = mul $7 $8
  <i32> $10 = add $1 $9
  <i32> $11 = const 3
  <i32> $12 = mod $5 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $12 $13
  <i32> $15 = const 1
  <i32> $16 = mul $14 $15
  <i32> $17 = add $2 $16
  <*f32> $18 = global ptr [S8place<f32>], index [$10, $17] activate=false
  <f32> $19 = global load $18
  <*struct[none]{0(data_ptr, at 0B): *f32}> $20 = argaddr[0]
  <*f32> $21 = external_ptr $20, [$10, $17] element_dim=0 layout=AOS is_grad=false
  $22 : global store [$21 <- $19]
}
}
[I 04/20/23 19:15:56.109 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Make mesh thread local:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = const 0
  <i32> $3 = loop $0 index 0
  <i32> $4 = const 0
  <i32> $5 = bit_shr $3 $4
  <i32> $6 = const 3
  <i32> $7 = div $5 $6
  <i32> $8 = const 1
  <i32> $9 = mul $7 $8
  <i32> $10 = add $1 $9
  <i32> $11 = const 3
  <i32> $12 = mod $5 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $12 $13
  <i32> $15 = const 1
  <i32> $16 = mul $14 $15
  <i32> $17 = add $2 $16
  <*f32> $18 = global ptr [S8place<f32>], index [$10, $17] activate=false
  <f32> $19 = global load $18
  <*struct[none]{0(data_ptr, at 0B): *f32}> $20 = argaddr[0]
  <*f32> $21 = external_ptr $20, [$10, $17] element_dim=0 layout=AOS is_grad=false
  $22 : global store [$21 <- $19]
}
}
[I 04/20/23 19:15:56.109 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Make mesh block local:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = const 0
  <i32> $3 = loop $0 index 0
  <i32> $4 = const 0
  <i32> $5 = bit_shr $3 $4
  <i32> $6 = const 3
  <i32> $7 = div $5 $6
  <i32> $8 = const 1
  <i32> $9 = mul $7 $8
  <i32> $10 = add $1 $9
  <i32> $11 = const 3
  <i32> $12 = mod $5 $11
  <i32> $13 = const 0
  <i32> $14 = bit_shr $12 $13
  <i32> $15 = const 1
  <i32> $16 = mul $14 $15
  <i32> $17 = add $2 $16
  <*f32> $18 = global ptr [S8place<f32>], index [$10, $17] activate=false
  <f32> $19 = global load $18
  <*struct[none]{0(data_ptr, at 0B): *f32}> $20 = argaddr[0]
  <*f32> $21 = external_ptr $20, [$10, $17] element_dim=0 layout=AOS is_grad=false
  $22 : global store [$21 <- $19]
}
}
[I 04/20/23 19:15:56.109 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Simplified X:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S8place<f32>], index [$3, $4] activate=false
  <f32> $6 = global load $5
  <*struct[none]{0(data_ptr, at 0B): *f32}> $7 = argaddr[0]
  <*f32> $8 = external_ptr $7, [$3, $4] element_dim=0 layout=AOS is_grad=false
  $9 : global store [$8 <- $6]
}
}
[I 04/20/23 19:15:56.109 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Make block local:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S8place<f32>], index [$3, $4] activate=false
  <f32> $6 = global load $5
  <*struct[none]{0(data_ptr, at 0B): *f32}> $7 = argaddr[0]
  <*f32> $8 = external_ptr $7, [$3, $4] element_dim=0 layout=AOS is_grad=false
  $9 : global store [$8 <- $6]
}
}
[I 04/20/23 19:15:56.109 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Demote mesh statements:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S8place<f32>], index [$3, $4] activate=false
  <f32> $6 = global load $5
  <*struct[none]{0(data_ptr, at 0B): *f32}> $7 = argaddr[0]
  <*f32> $8 = external_ptr $7, [$3, $4] element_dim=0 layout=AOS is_grad=false
  $9 : global store [$8 <- $6]
}
}
[I 04/20/23 19:15:56.109 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Atomics demoted II:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S8place<f32>], index [$3, $4] activate=false
  <f32> $6 = global load $5
  <*struct[none]{0(data_ptr, at 0B): *f32}> $7 = argaddr[0]
  <*f32> $8 = external_ptr $7, [$3, $4] element_dim=0 layout=AOS is_grad=false
  $9 : global store [$8 <- $6]
}
}
[I 04/20/23 19:15:56.109 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Remove range assumption:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S8place<f32>], index [$3, $4] activate=false
  <f32> $6 = global load $5
  <*struct[none]{0(data_ptr, at 0B): *f32}> $7 = argaddr[0]
  <*f32> $8 = external_ptr $7, [$3, $4] element_dim=0 layout=AOS is_grad=false
  $9 : global store [$8 <- $6]
}
}
[I 04/20/23 19:15:56.109 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Remove loop_unique:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S8place<f32>], index [$3, $4] activate=false
  <f32> $6 = global load $5
  <*struct[none]{0(data_ptr, at 0B): *f32}> $7 = argaddr[0]
  <*f32> $8 = external_ptr $7, [$3, $4] element_dim=0 layout=AOS is_grad=false
  $9 : global store [$8 <- $6]
}
}
[I 04/20/23 19:15:56.110 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Simplified before lower access:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S8place<f32>], index [$3, $4] activate=false
  <f32> $6 = global load $5
  <*struct[none]{0(data_ptr, at 0B): *f32}> $7 = argaddr[0]
  <*f32> $8 = external_ptr $7, [$3, $4] element_dim=0 layout=AOS is_grad=false
  $9 : global store [$8 <- $6]
}
}
[I 04/20/23 19:15:56.110 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Access lowered:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*f32> $5 = global ptr [S8place<f32>], index [$3, $4] activate=false
  <*gen> $6 = get root [S0root][root]
  <i32> $7 = linearized(ind {}, stride {})
  <*gen> $8 = [S0root][root]::lookup($6, $7) activate = false
  <*gen> $9 = get child [S0root->S7dense] $8
  <i32> $10 = const 0
  <i32> $11 = bit_shr $3 $10
  <i32> $12 = const 0
  <i32> $13 = bit_shr $4 $12
  <i32> $14 = linearized(ind {$11, $13}, stride {8192, 3})
  <*gen> $15 = [S7dense][dense]::lookup($9, $14) activate = false
  <*f32> $16 = get child [S7dense->S8place<f32>] $15
  <f32> $17 = global load $16
  <*struct[none]{0(data_ptr, at 0B): *f32}> $18 = argaddr[0]
  <*f32> $19 = external_ptr $18, [$3, $4] element_dim=0 layout=AOS is_grad=false
  $20 : global store [$19 <- $17]
}
}
[I 04/20/23 19:15:56.110 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] DIE:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*gen> $5 = get root [S0root][root]
  <i32> $6 = linearized(ind {}, stride {})
  <*gen> $7 = [S0root][root]::lookup($5, $6) activate = false
  <*gen> $8 = get child [S0root->S7dense] $7
  <i32> $9 = const 0
  <i32> $10 = bit_shr $3 $9
  <i32> $11 = const 0
  <i32> $12 = bit_shr $4 $11
  <i32> $13 = linearized(ind {$10, $12}, stride {8192, 3})
  <*gen> $14 = [S7dense][dense]::lookup($8, $13) activate = false
  <*f32> $15 = get child [S7dense->S8place<f32>] $14
  <f32> $16 = global load $15
  <*struct[none]{0(data_ptr, at 0B): *f32}> $17 = argaddr[0]
  <*f32> $18 = external_ptr $17, [$3, $4] element_dim=0 layout=AOS is_grad=false
  $19 : global store [$18 <- $16]
}
}
[I 04/20/23 19:15:56.110 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Access flagged III:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*gen> $5 = get root [S0root][root]
  <i32> $6 = linearized(ind {}, stride {})
  <*gen> $7 = [S0root][root]::lookup($5, $6) activate = false
  <*gen> $8 = get child [S0root->S7dense] $7
  <i32> $9 = const 0
  <i32> $10 = bit_shr $3 $9
  <i32> $11 = const 0
  <i32> $12 = bit_shr $4 $11
  <i32> $13 = linearized(ind {$10, $12}, stride {8192, 3})
  <*gen> $14 = [S7dense][dense]::lookup($8, $13) activate = false
  <*f32> $15 = get child [S7dense->S8place<f32>] $14
  <f32> $16 = global load $15
  <*struct[none]{0(data_ptr, at 0B): *f32}> $17 = argaddr[0]
  <*f32> $18 = external_ptr $17, [$3, $4] element_dim=0 layout=AOS is_grad=false
  $19 : global store [$18 <- $16]
}
}
[I 04/20/23 19:15:56.110 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Operations demoted:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*gen> $5 = get root [S0root][root]
  <i32> $6 = linearized(ind {}, stride {})
  <*gen> $7 = [S0root][root]::lookup($5, $6) activate = false
  <*gen> $8 = get child [S0root->S7dense] $7
  <i32> $9 = const 0
  <u32> $10 = reinterpret_cast_bits<u32> $3
  <u32> $11 = cast_value<u32> $9
  <u32> $12 = bit_sar $10 $11
  <i32> $13 = reinterpret_cast_bits<i32> $12
  <i32> $14 = const 0
  <u32> $15 = reinterpret_cast_bits<u32> $4
  <u32> $16 = cast_value<u32> $14
  <u32> $17 = bit_sar $15 $16
  <i32> $18 = reinterpret_cast_bits<i32> $17
  <i32> $19 = linearized(ind {$13, $18}, stride {8192, 3})
  <*gen> $20 = [S7dense][dense]::lookup($8, $19) activate = false
  <*f32> $21 = get child [S7dense->S8place<f32>] $20
  <f32> $22 = global load $21
  <*struct[none]{0(data_ptr, at 0B): *f32}> $23 = argaddr[0]
  <*f32> $24 = external_ptr $23, [$3, $4] element_dim=0 layout=AOS is_grad=false
  $25 : global store [$24 <- $22]
}
}
[I 04/20/23 19:15:56.110 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Simplified IV:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*gen> $5 = get root [S0root][root]
  <i32> $6 = const 0
  <*gen> $7 = [S0root][root]::lookup($5, $6) activate = false
  <*gen> $8 = get child [S0root->S7dense] $7
  <i32> $9 = mul $3 $2
  <i32> $10 = add $4 $9
  <*gen> $11 = [S7dense][dense]::lookup($8, $10) activate = false
  <*f32> $12 = get child [S7dense->S8place<f32>] $11
  <f32> $13 = global load $12
  <*struct[none]{0(data_ptr, at 0B): *f32}> $14 = argaddr[0]
  <*f32> $15 = external_ptr $14, [$3, $4] element_dim=0 layout=AOS is_grad=false
  $16 : global store [$15 <- $13]
}
}
[I 04/20/23 19:15:56.110 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_0] Bit struct stores optimized:
kernel {
$0 = offloaded range_for(0, 24576) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 3
  <i32> $3 = div $1 $2
  <i32> $4 = mod $1 $2
  <*gen> $5 = get root [S0root][root]
  <i32> $6 = const 0
  <*gen> $7 = [S0root][root]::lookup($5, $6) activate = false
  <*gen> $8 = get child [S0root->S7dense] $7
  <i32> $9 = mul $3 $2
  <i32> $10 = add $4 $9
  <*gen> $11 = [S7dense][dense]::lookup($8, $10) activate = false
  <*f32> $12 = get child [S7dense->S8place<f32>] $11
  <f32> $13 = global load $12
  <*struct[none]{0(data_ptr, at 0B): *f32}> $14 = argaddr[0]
  <*f32> $15 = external_ptr $14, [$3, $4] element_dim=0 layout=AOS is_grad=false
  $16 : global store [$15 <- $13]
}
}
50674.547
[I 04/20/23 19:15:56.148 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Initial IR:
kernel {
  $0 : for @tmp0 in range((cast_value<i32> 0), (cast_value<i32> 8192)) block_dim=256 {
    $1 = alloca @tmp1
    @tmp1 = (cast_value<i32> #@tmp3 (snode=S6place<i64>)[@tmp0, 0])
    $3 = alloca @tmp2
    @tmp2 = @tmp1
    $5 = alloca @tmp3
    @tmp3 = 0.0
    $7 = alloca @tmp4
    @tmp4 = @tmp3
    $9 : for @tmp5 in range((cast_value<i32> 0), (cast_value<i32> 1)) block_dim=adaptive {
      $10 = alloca @tmp6
      @tmp6 = atomic_add(@tmp4, #@tmp0 (snode=S2place<f32>)[@tmp5])
    }
    $12 = alloca @tmp7
    @tmp7 = @tmp4
    #@tmp4 (snode=S8place<f32>)[@tmp2, 0] = @tmp7
  }
}
[I 04/20/23 19:15:56.148 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Segment reversed (for autodiff):
kernel {
  $0 : for @tmp0 in range((cast_value<i32> 0), (cast_value<i32> 8192)) block_dim=256 {
    $1 = alloca @tmp1
    @tmp1 = (cast_value<i32> #@tmp3 (snode=S6place<i64>)[@tmp0, 0])
    $3 = alloca @tmp2
    @tmp2 = @tmp1
    $5 = alloca @tmp3
    @tmp3 = 0.0
    $7 = alloca @tmp4
    @tmp4 = @tmp3
    $9 : for @tmp5 in range((cast_value<i32> 0), (cast_value<i32> 1)) block_dim=adaptive {
      $10 = alloca @tmp6
      @tmp6 = atomic_add(@tmp4, #@tmp0 (snode=S2place<f32>)[@tmp5])
    }
    $12 = alloca @tmp7
    @tmp7 = @tmp4
    #@tmp4 (snode=S8place<f32>)[@tmp2, 0] = @tmp7
  }
}
[I 04/20/23 19:15:56.148 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Lowered:
kernel {
  <i32> $0 = const 0
  <i32> $1 = cast_value<i32> $0
  <i32> $2 = const 8192
  <i32> $3 = cast_value<i32> $2
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <i32> $6 = alloca
    <i32> $7 = const 0
    <i64> $8 = global ptr [S6place<i64>], index [$5, $7] activate=true
    <i64> $9 = global load $8
    <i32> $10 = cast_value<i32> $9
    $11 : local store [$6 <- $10]
    <i32> $12 = alloca
    <i32> $13 = local load [$6]
    $14 : local store [$12 <- $13]
    <f32> $15 = alloca
    <f32> $16 = const 0.0
    $17 : local store [$15 <- $16]
    <f32> $18 = alloca
    <f32> $19 = local load [$15]
    $20 : local store [$18 <- $19]
    <i32> $21 = const 0
    <i32> $22 = cast_value<i32> $21
    <i32> $23 = const 1
    <i32> $24 = cast_value<i32> $23
    $25 : for in range($22, $24) block_dim=adaptive {
      <i32> $26 = loop $25 index 0
      <f32> $27 = alloca
      <f32> $28 = global ptr [S2place<f32>], index [$26] activate=true
      <f32> $29 = global load $28
      <f32> $30 = atomic add($18, $29)
      $31 : local store [$27 <- $30]
    }
    <f32> $32 = alloca
    <f32> $33 = local load [$18]
    $34 : local store [$32 <- $33]
    <f32> $35 = local load [$32]
    <i32> $36 = local load [$12]
    <i32> $37 = const 0
    <f32> $38 = global ptr [S8place<f32>], index [$36, $37] activate=true
    $39 : global store [$38 <- $35]
  }
}
[I 04/20/23 19:15:56.149 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Immutable local vars eliminated:
kernel {
  <i32> $0 = const 0
  <i32> $1 = cast_value<i32> $0
  <i32> $2 = const 8192
  <i32> $3 = cast_value<i32> $2
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <i32> $6 = const 0
    <i64> $7 = global ptr [S6place<i64>], index [$5, $6] activate=true
    <i64> $8 = global load $7
    <i32> $9 = cast_value<i32> $8
    <f32> $10 = const 0.0
    <f32> $11 = alloca
    $12 : local store [$11 <- $10]
    <i32> $13 = const 0
    <i32> $14 = cast_value<i32> $13
    <i32> $15 = const 1
    <i32> $16 = cast_value<i32> $15
    $17 : for in range($14, $16) block_dim=adaptive {
      <i32> $18 = loop $17 index 0
      <f32> $19 = global ptr [S2place<f32>], index [$18] activate=true
      <f32> $20 = global load $19
      <f32> $21 = atomic add($11, $20)
    }
    <f32> $22 = local load [$11]
    <i32> $23 = const 0
    <f32> $24 = global ptr [S8place<f32>], index [$9, $23] activate=true
    $25 : global store [$24 <- $22]
  }
}
[I 04/20/23 19:15:56.149 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Typechecked:
kernel {
  <i32> $0 = const 0
  <i32> $1 = cast_value<i32> $0
  <i32> $2 = const 8192
  <i32> $3 = cast_value<i32> $2
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <i32> $6 = const 0
    <*i64> $7 = global ptr [S6place<i64>], index [$5, $6] activate=true
    <i64> $8 = global load $7
    <i32> $9 = cast_value<i32> $8
    <f32> $10 = const 0.0
    <f32> $11 = alloca
    <f32> $12 : local store [$11 <- $10]
    <i32> $13 = const 0
    <i32> $14 = cast_value<i32> $13
    <i32> $15 = const 1
    <i32> $16 = cast_value<i32> $15
    $17 : for in range($14, $16) block_dim=adaptive {
      <i32> $18 = loop $17 index 0
      <*f32> $19 = global ptr [S2place<f32>], index [$18] activate=true
      <f32> $20 = global load $19
      <f32> $21 = atomic add($11, $20)
    }
    <f32> $22 = local load [$11]
    <i32> $23 = const 0
    <*f32> $24 = global ptr [S8place<f32>], index [$9, $23] activate=true
    $25 : global store [$24 <- $22]
  }
}
[I 04/20/23 19:15:56.149 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Bit Loop Vectorized:
kernel {
  <i32> $0 = const 0
  <i32> $1 = cast_value<i32> $0
  <i32> $2 = const 8192
  <i32> $3 = cast_value<i32> $2
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <i32> $6 = const 0
    <*i64> $7 = global ptr [S6place<i64>], index [$5, $6] activate=true
    <i64> $8 = global load $7
    <i32> $9 = cast_value<i32> $8
    <f32> $10 = const 0.0
    <f32> $11 = alloca
    <f32> $12 : local store [$11 <- $10]
    <i32> $13 = const 0
    <i32> $14 = cast_value<i32> $13
    <i32> $15 = const 1
    <i32> $16 = cast_value<i32> $15
    $17 : for in range($14, $16) block_dim=adaptive {
      <i32> $18 = loop $17 index 0
      <*f32> $19 = global ptr [S2place<f32>], index [$18] activate=true
      <f32> $20 = global load $19
      <f32> $21 = atomic add($11, $20)
    }
    <f32> $22 = local load [$11]
    <i32> $23 = const 0
    <*f32> $24 = global ptr [S8place<f32>], index [$9, $23] activate=true
    $25 : global store [$24 <- $22]
  }
}
[I 04/20/23 19:15:56.149 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Matrix ptr lowered:
kernel {
  <i32> $0 = const 0
  <i32> $1 = cast_value<i32> $0
  <i32> $2 = const 8192
  <i32> $3 = cast_value<i32> $2
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <i32> $6 = const 0
    <*i64> $7 = global ptr [S6place<i64>], index [$5, $6] activate=true
    <i64> $8 = global load $7
    <i32> $9 = cast_value<i32> $8
    <f32> $10 = const 0.0
    <f32> $11 = alloca
    <f32> $12 : local store [$11 <- $10]
    <i32> $13 = const 0
    <i32> $14 = cast_value<i32> $13
    <i32> $15 = const 1
    <i32> $16 = cast_value<i32> $15
    $17 : for in range($14, $16) block_dim=adaptive {
      <i32> $18 = loop $17 index 0
      <*f32> $19 = global ptr [S2place<f32>], index [$18] activate=true
      <f32> $20 = global load $19
      <f32> $21 = atomic add($11, $20)
    }
    <f32> $22 = local load [$11]
    <i32> $23 = const 0
    <*f32> $24 = global ptr [S8place<f32>], index [$9, $23] activate=true
    $25 : global store [$24 <- $22]
  }
}
[I 04/20/23 19:15:56.149 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Simplified I:
kernel {
  <i32> $0 = const 0
  <i32> $1 = const 1
  <f32> $2 = const 0.0
  <i32> $3 = const 8192
  $4 : for in range($0, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <*i64> $6 = global ptr [S6place<i64>], index [$5, $0] activate=true
    <i64> $7 = global load $6
    <i32> $8 = cast_value<i32> $7
    <f32> $9 = alloca
    <f32> $10 : local store [$9 <- $2]
    $11 : for in range($0, $1) block_dim=adaptive {
      <i32> $12 = loop $11 index 0
      <*f32> $13 = global ptr [S2place<f32>], index [$12] activate=true
      <f32> $14 = global load $13
      <f32> $15 = atomic add($9, $14)
    }
    <f32> $16 = local load [$9]
    <*f32> $17 = global ptr [S8place<f32>], index [$8, $0] activate=true
    $18 : global store [$17 <- $16]
  }
}
[I 04/20/23 19:15:56.149 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Scalarized:
kernel {
  <i32> $0 = const 0
  <i32> $1 = const 1
  <f32> $2 = const 0.0
  <i32> $3 = const 8192
  $4 : for in range($0, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <*i64> $6 = global ptr [S6place<i64>], index [$5, $0] activate=true
    <i64> $7 = global load $6
    <i32> $8 = cast_value<i32> $7
    <f32> $9 = alloca
    <f32> $10 : local store [$9 <- $2]
    $11 : for in range($0, $1) block_dim=adaptive {
      <i32> $12 = loop $11 index 0
      <*f32> $13 = global ptr [S2place<f32>], index [$12] activate=true
      <f32> $14 = global load $13
      <f32> $15 = atomic add($9, $14)
    }
    <f32> $16 = local load [$9]
    <*f32> $17 = global ptr [S8place<f32>], index [$8, $0] activate=true
    $18 : global store [$17 <- $16]
  }
}
[I 04/20/23 19:15:56.150 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Gradient:
kernel {
  <f32> $0 = const 0.0
  <i32> $1 = const 0
  <i32> $2 = const 1
  <i32> $3 = const 8192
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = alloca
    <i32> $6 = loop $4 index 0
    <*i64> $7 = global ptr [S6place<i64>], index [$6, $1] activate=true
    <i64> $8 = global load $7
    <i32> $9 = cast_value<i32> $8
    <f32> $10 = stack alloc (max_size=1024)
    <f32> $11 : stack push $10, val = $0
    <f32> $12 : stack push $10, val = $0
    $13 : for in range($1, $2) block_dim=adaptive {
      <i32> $14 = loop $13 index 0
      <i32> $15 : local store [$5 <- $14]
      <*f32> $16 = global ptr [S2place<f32>], index [$14] activate=true
      <f32> $17 = global load $16
      <f32> $18 = stack load top $10
      <f32> $19 = add $18 $17
      <f32> $20 : stack push $10, val = $19
    }
    <*f32> $21 = global ptr [S8place<f32>], index [$9, $1] activate=true
    <*f32> $22 = global ptr [S10place<f32>], index [$9, $1] activate=true
    <f32> $23 = global load $22
    $24 : global store [$22 <- $0]
    <f32> $25 : stack acc adj $10, val = $23
    $26 : reversed for in range($1, $2) block_dim=adaptive {
      <f32> $27 = stack load top adj $10
      <f32> $28 : stack pop $10
      <f32> $29 : stack acc adj $10, val = $27
      <i32> $30 = local load [$5]
      <*f32> $31 = global ptr [S4place<f32>], index [$30] activate=true
      <f32> $32 = atomic add($31, $27)
    }
    <f32> $33 : stack pop $10
    <f32> $34 : stack pop $10
  }
}
[I 04/20/23 19:15:56.150 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Access flagged I:
kernel {
  <f32> $0 = const 0.0
  <i32> $1 = const 0
  <i32> $2 = const 1
  <i32> $3 = const 8192
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = alloca
    <i32> $6 = loop $4 index 0
    <*i64> $7 = global ptr [S6place<i64>], index [$6, $1] activate=false
    <i64> $8 = global load $7
    <i32> $9 = cast_value<i32> $8
    <f32> $10 = stack alloc (max_size=1024)
    <f32> $11 : stack push $10, val = $0
    <f32> $12 : stack push $10, val = $0
    $13 : for in range($1, $2) block_dim=adaptive {
      <i32> $14 = loop $13 index 0
      <i32> $15 : local store [$5 <- $14]
      <*f32> $16 = global ptr [S2place<f32>], index [$14] activate=false
      <f32> $17 = global load $16
      <f32> $18 = stack load top $10
      <f32> $19 = add $18 $17
      <f32> $20 : stack push $10, val = $19
    }
    <*f32> $21 = global ptr [S8place<f32>], index [$9, $1] activate=false
    <*f32> $22 = global ptr [S10place<f32>], index [$9, $1] activate=true
    <f32> $23 = global load $22
    $24 : global store [$22 <- $0]
    <f32> $25 : stack acc adj $10, val = $23
    $26 : reversed for in range($1, $2) block_dim=adaptive {
      <f32> $27 = stack load top adj $10
      <f32> $28 : stack pop $10
      <f32> $29 : stack acc adj $10, val = $27
      <i32> $30 = local load [$5]
      <*f32> $31 = global ptr [S4place<f32>], index [$30] activate=true
      <f32> $32 = atomic add($31, $27)
    }
    <f32> $33 : stack pop $10
    <f32> $34 : stack pop $10
  }
}
[I 04/20/23 19:15:56.151 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Simplified II:
kernel {
  <f32> $0 = const 0.0
  <i32> $1 = const 0
  <i32> $2 = const 1
  <i32> $3 = const 8192
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = alloca
    <i32> $6 = loop $4 index 0
    <*i64> $7 = global ptr [S6place<i64>], index [$6, $1] activate=false
    <i64> $8 = global load $7
    <i32> $9 = cast_value<i32> $8
    <f32> $10 = stack alloc (max_size=1024)
    <f32> $11 : stack push $10, val = $0
    <f32> $12 : stack push $10, val = $0
    $13 : for in range($1, $2) block_dim=adaptive {
      <i32> $14 = loop $13 index 0
      <i32> $15 : local store [$5 <- $14]
      <*f32> $16 = global ptr [S2place<f32>], index [$14] activate=false
      <f32> $17 = global load $16
      <f32> $18 = stack load top $10
      <f32> $19 = add $18 $17
      <f32> $20 : stack push $10, val = $19
    }
    <*f32> $21 = global ptr [S10place<f32>], index [$9, $1] activate=true
    <f32> $22 = global load $21
    $23 : global store [$21 <- $0]
    <f32> $24 : stack acc adj $10, val = $22
    $25 : reversed for in range($1, $2) block_dim=adaptive {
      <f32> $26 = stack load top adj $10
      <f32> $27 : stack pop $10
      <f32> $28 : stack acc adj $10, val = $26
      <i32> $29 = local load [$5]
      <*f32> $30 = global ptr [S4place<f32>], index [$29] activate=true
      <f32> $31 = atomic add($30, $26)
    }
    <f32> $32 : stack pop $10
    <f32> $33 : stack pop $10
  }
}
[I 04/20/23 19:15:56.151 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Offloaded:
kernel {
  $0 = offloaded
  body {
    <f32> $1 = const 0.0
    <i32> $2 = const 0
    <i32> $3 = const 1
    <i32> $4 = const 8192
  }
  $5 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
  body {
    <i32> $6 = alloca
    <i32> $7 = loop $5 index 0
    <i32> $8 = const 0
    <*i64> $9 = global ptr [S6place<i64>], index [$7, $8] activate=false
    <i64> $10 = global load $9
    <i32> $11 = cast_value<i32> $10
    <f32> $12 = stack alloc (max_size=1024)
    <f32> $13 = const 0.0
    <f32> $14 : stack push $12, val = $13
    <f32> $15 = const 0.0
    <f32> $16 : stack push $12, val = $15
    <i32> $17 = const 0
    <i32> $18 = const 1
    $19 : for in range($17, $18) block_dim=adaptive {
      <i32> $20 = loop $19 index 0
      <i32> $21 : local store [$6 <- $20]
      <*f32> $22 = global ptr [S2place<f32>], index [$20] activate=false
      <f32> $23 = global load $22
      <f32> $24 = stack load top $12
      <f32> $25 = add $24 $23
      <f32> $26 : stack push $12, val = $25
    }
    <i32> $27 = const 0
    <*f32> $28 = global ptr [S10place<f32>], index [$11, $27] activate=true
    <f32> $29 = global load $28
    <f32> $30 = const 0.0
    $31 : global store [$28 <- $30]
    <f32> $32 : stack acc adj $12, val = $29
    <i32> $33 = const 0
    <i32> $34 = const 1
    $35 : reversed for in range($33, $34) block_dim=adaptive {
      <f32> $36 = stack load top adj $12
      <f32> $37 : stack pop $12
      <f32> $38 : stack acc adj $12, val = $36
      <i32> $39 = local load [$6]
      <*f32> $40 = global ptr [S4place<f32>], index [$39] activate=true
      <f32> $41 = atomic add($40, $36)
    }
    <f32> $42 : stack pop $12
    <f32> $43 : stack pop $12
  }
}
[I 04/20/23 19:15:56.151 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Optimized by CFG:
kernel {
  $0 = offloaded
  body {
  }
  $1 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
  body {
    <i32> $2 = alloca
    <i32> $3 = loop $1 index 0
    <i32> $4 = const 0
    <*i64> $5 = global ptr [S6place<i64>], index [$3, $4] activate=false
    <i64> $6 = global load $5
    <i32> $7 = cast_value<i32> $6
    <f32> $8 = stack alloc (max_size=1024)
    <f32> $9 = const 0.0
    <f32> $10 : stack push $8, val = $9
    <f32> $11 = const 0.0
    <f32> $12 : stack push $8, val = $11
    <i32> $13 = const 0
    <i32> $14 = const 1
    $15 : for in range($13, $14) block_dim=adaptive {
      <i32> $16 = loop $15 index 0
      <i32> $17 : local store [$2 <- $16]
      <*f32> $18 = global ptr [S2place<f32>], index [$16] activate=false
      <f32> $19 = global load $18
      <f32> $20 = stack load top $8
      <f32> $21 = add $20 $19
      <f32> $22 : stack push $8, val = $21
    }
    <i32> $23 = const 0
    <*f32> $24 = global ptr [S10place<f32>], index [$7, $23] activate=true
    <f32> $25 = global load $24
    <f32> $26 = const 0.0
    $27 : global store [$24 <- $26]
    <f32> $28 : stack acc adj $8, val = $25
    <i32> $29 = const 0
    <i32> $30 = const 1
    $31 : reversed for in range($29, $30) block_dim=adaptive {
      <f32> $32 = stack load top adj $8
      <f32> $33 : stack pop $8
      <f32> $34 : stack acc adj $8, val = $32
      <i32> $35 = local load [$2]
      <*f32> $36 = global ptr [S4place<f32>], index [$35] activate=true
      <f32> $37 = atomic add($36, $32)
    }
  }
}
[I 04/20/23 19:15:56.151 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Access flagged II:
kernel {
  $0 = offloaded
  body {
  }
  $1 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
  body {
    <i32> $2 = alloca
    <i32> $3 = loop $1 index 0
    <i32> $4 = const 0
    <*i64> $5 = global ptr [S6place<i64>], index [$3, $4] activate=false
    <i64> $6 = global load $5
    <i32> $7 = cast_value<i32> $6
    <f32> $8 = stack alloc (max_size=1024)
    <f32> $9 = const 0.0
    <f32> $10 : stack push $8, val = $9
    <f32> $11 = const 0.0
    <f32> $12 : stack push $8, val = $11
    <i32> $13 = const 0
    <i32> $14 = const 1
    $15 : for in range($13, $14) block_dim=adaptive {
      <i32> $16 = loop $15 index 0
      <i32> $17 : local store [$2 <- $16]
      <*f32> $18 = global ptr [S2place<f32>], index [$16] activate=false
      <f32> $19 = global load $18
      <f32> $20 = stack load top $8
      <f32> $21 = add $20 $19
      <f32> $22 : stack push $8, val = $21
    }
    <i32> $23 = const 0
    <*f32> $24 = global ptr [S10place<f32>], index [$7, $23] activate=true
    <f32> $25 = global load $24
    <f32> $26 = const 0.0
    $27 : global store [$24 <- $26]
    <f32> $28 : stack acc adj $8, val = $25
    <i32> $29 = const 0
    <i32> $30 = const 1
    $31 : reversed for in range($29, $30) block_dim=adaptive {
      <f32> $32 = stack load top adj $8
      <f32> $33 : stack pop $8
      <f32> $34 : stack acc adj $8, val = $32
      <i32> $35 = local load [$2]
      <*f32> $36 = global ptr [S4place<f32>], index [$35] activate=true
      <f32> $37 = atomic add($36, $32)
    }
  }
}
[I 04/20/23 19:15:56.151 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Simplified III:
kernel {
  $0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
  body {
    <i32> $1 = alloca
    <i32> $2 = loop $0 index 0
    <i32> $3 = const 0
    <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
    <i64> $5 = global load $4
    <i32> $6 = cast_value<i32> $5
    <f32> $7 = stack alloc (max_size=1024)
    <f32> $8 = const 0.0
    <f32> $9 : stack push $7, val = $8
    <f32> $10 : stack push $7, val = $8
    <i32> $11 = const 1
    $12 : for in range($3, $11) block_dim=adaptive {
      <i32> $13 = loop $12 index 0
      <i32> $14 : local store [$1 <- $13]
      <*f32> $15 = global ptr [S2place<f32>], index [$13] activate=false
      <f32> $16 = global load $15
      <f32> $17 = stack load top $7
      <f32> $18 = add $17 $16
      <f32> $19 : stack push $7, val = $18
    }
    <*f32> $20 = global ptr [S10place<f32>], index [$6, $3] activate=true
    <f32> $21 = global load $20
    $22 : global store [$20 <- $8]
    <f32> $23 : stack acc adj $7, val = $21
    $24 : reversed for in range($3, $11) block_dim=adaptive {
      <f32> $25 = stack load top adj $7
      <f32> $26 : stack pop $7
      <f32> $27 : stack acc adj $7, val = $25
      <i32> $28 = local load [$1]
      <*f32> $29 = global ptr [S4place<f32>], index [$28] activate=true
      <f32> $30 = atomic add($29, $25)
    }
  }
}
[I 04/20/23 19:15:56.152 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Start offload_to_executable:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
  <i64> $5 = global load $4
  <i32> $6 = cast_value<i32> $5
  <f32> $7 = stack alloc (max_size=1024)
  <f32> $8 = const 0.0
  <f32> $9 : stack push $7, val = $8
  <f32> $10 : stack push $7, val = $8
  <i32> $11 = const 1
  $12 : for in range($3, $11) block_dim=adaptive {
    <i32> $13 = loop $12 index 0
    <i32> $14 : local store [$1 <- $13]
    <*f32> $15 = global ptr [S2place<f32>], index [$13] activate=false
    <f32> $16 = global load $15
    <f32> $17 = stack load top $7
    <f32> $18 = add $17 $16
    <f32> $19 : stack push $7, val = $18
  }
  <*f32> $20 = global ptr [S10place<f32>], index [$6, $3] activate=true
  <f32> $21 = global load $20
  $22 : global store [$20 <- $8]
  <f32> $23 : stack acc adj $7, val = $21
  $24 : reversed for in range($3, $11) block_dim=adaptive {
    <f32> $25 = stack load top adj $7
    <f32> $26 : stack pop $7
    <f32> $27 : stack acc adj $7, val = $25
    <i32> $28 = local load [$1]
    <*f32> $29 = global ptr [S4place<f32>], index [$28] activate=true
    <f32> $30 = atomic add($29, $25)
  }
}
}
[I 04/20/23 19:15:56.152 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Detect read-only accesses:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
  <i64> $5 = global load $4
  <i32> $6 = cast_value<i32> $5
  <f32> $7 = stack alloc (max_size=1024)
  <f32> $8 = const 0.0
  <f32> $9 : stack push $7, val = $8
  <f32> $10 : stack push $7, val = $8
  <i32> $11 = const 1
  $12 : for in range($3, $11) block_dim=adaptive {
    <i32> $13 = loop $12 index 0
    <i32> $14 : local store [$1 <- $13]
    <*f32> $15 = global ptr [S2place<f32>], index [$13] activate=false
    <f32> $16 = global load $15
    <f32> $17 = stack load top $7
    <f32> $18 = add $17 $16
    <f32> $19 : stack push $7, val = $18
  }
  <*f32> $20 = global ptr [S10place<f32>], index [$6, $3] activate=true
  <f32> $21 = global load $20
  $22 : global store [$20 <- $8]
  <f32> $23 : stack acc adj $7, val = $21
  $24 : reversed for in range($3, $11) block_dim=adaptive {
    <f32> $25 = stack load top adj $7
    <f32> $26 : stack pop $7
    <f32> $27 : stack acc adj $7, val = $25
    <i32> $28 = local load [$1]
    <*f32> $29 = global ptr [S4place<f32>], index [$28] activate=true
    <f32> $30 = atomic add($29, $25)
  }
}
}
[I 04/20/23 19:15:56.152 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Atomics demoted I:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
  <i64> $5 = global load $4
  <i32> $6 = cast_value<i32> $5
  <f32> $7 = stack alloc (max_size=1024)
  <f32> $8 = const 0.0
  <f32> $9 : stack push $7, val = $8
  <f32> $10 : stack push $7, val = $8
  <i32> $11 = const 1
  $12 : for in range($3, $11) block_dim=adaptive {
    <i32> $13 = loop $12 index 0
    <i32> $14 : local store [$1 <- $13]
    <*f32> $15 = global ptr [S2place<f32>], index [$13] activate=false
    <f32> $16 = global load $15
    <f32> $17 = stack load top $7
    <f32> $18 = add $17 $16
    <f32> $19 : stack push $7, val = $18
  }
  <*f32> $20 = global ptr [S10place<f32>], index [$6, $3] activate=true
  <f32> $21 = global load $20
  $22 : global store [$20 <- $8]
  <f32> $23 : stack acc adj $7, val = $21
  $24 : reversed for in range($3, $11) block_dim=adaptive {
    <f32> $25 = stack load top adj $7
    <f32> $26 : stack pop $7
    <f32> $27 : stack acc adj $7, val = $25
    <i32> $28 = local load [$1]
    <*f32> $29 = global ptr [S4place<f32>], index [$28] activate=true
    <f32> $30 = atomic add($29, $25)
  }
}
}
[I 04/20/23 19:15:56.152 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Cache loop-invariant global vars:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
  <i64> $5 = global load $4
  <i32> $6 = cast_value<i32> $5
  <f32> $7 = stack alloc (max_size=1024)
  <f32> $8 = const 0.0
  <f32> $9 : stack push $7, val = $8
  <f32> $10 : stack push $7, val = $8
  <i32> $11 = const 1
  $12 : for in range($3, $11) block_dim=adaptive {
    <i32> $13 = loop $12 index 0
    <i32> $14 : local store [$1 <- $13]
    <*f32> $15 = global ptr [S2place<f32>], index [$13] activate=false
    <f32> $16 = global load $15
    <f32> $17 = stack load top $7
    <f32> $18 = add $17 $16
    <f32> $19 : stack push $7, val = $18
  }
  <*f32> $20 = global ptr [S10place<f32>], index [$6, $3] activate=true
  <f32> $21 = global load $20
  $22 : global store [$20 <- $8]
  <f32> $23 : stack acc adj $7, val = $21
  $24 : reversed for in range($3, $11) block_dim=adaptive {
    <f32> $25 = stack load top adj $7
    <f32> $26 : stack pop $7
    <f32> $27 : stack acc adj $7, val = $25
    <i32> $28 = local load [$1]
    <*f32> $29 = global ptr [S4place<f32>], index [$28] activate=true
    <f32> $30 = atomic add($29, $25)
  }
}
}
[I 04/20/23 19:15:56.152 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Dense struct-for demoted:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
  <i64> $5 = global load $4
  <i32> $6 = cast_value<i32> $5
  <f32> $7 = stack alloc (max_size=1024)
  <f32> $8 = const 0.0
  <f32> $9 : stack push $7, val = $8
  <f32> $10 : stack push $7, val = $8
  <i32> $11 = const 1
  $12 : for in range($3, $11) block_dim=adaptive {
    <i32> $13 = loop $12 index 0
    <i32> $14 : local store [$1 <- $13]
    <*f32> $15 = global ptr [S2place<f32>], index [$13] activate=false
    <f32> $16 = global load $15
    <f32> $17 = stack load top $7
    <f32> $18 = add $17 $16
    <f32> $19 : stack push $7, val = $18
  }
  <*f32> $20 = global ptr [S10place<f32>], index [$6, $3] activate=true
  <f32> $21 = global load $20
  $22 : global store [$20 <- $8]
  <f32> $23 : stack acc adj $7, val = $21
  $24 : reversed for in range($3, $11) block_dim=adaptive {
    <f32> $25 = stack load top adj $7
    <f32> $26 : stack pop $7
    <f32> $27 : stack acc adj $7, val = $25
    <i32> $28 = local load [$1]
    <*f32> $29 = global ptr [S4place<f32>], index [$28] activate=true
    <f32> $30 = atomic add($29, $25)
  }
}
}
[I 04/20/23 19:15:56.152 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] No-access mesh-for demoted:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
  <i64> $5 = global load $4
  <i32> $6 = cast_value<i32> $5
  <f32> $7 = stack alloc (max_size=1024)
  <f32> $8 = const 0.0
  <f32> $9 : stack push $7, val = $8
  <f32> $10 : stack push $7, val = $8
  <i32> $11 = const 1
  $12 : for in range($3, $11) block_dim=adaptive {
    <i32> $13 = loop $12 index 0
    <i32> $14 : local store [$1 <- $13]
    <*f32> $15 = global ptr [S2place<f32>], index [$13] activate=false
    <f32> $16 = global load $15
    <f32> $17 = stack load top $7
    <f32> $18 = add $17 $16
    <f32> $19 : stack push $7, val = $18
  }
  <*f32> $20 = global ptr [S10place<f32>], index [$6, $3] activate=true
  <f32> $21 = global load $20
  $22 : global store [$20 <- $8]
  <f32> $23 : stack acc adj $7, val = $21
  $24 : reversed for in range($3, $11) block_dim=adaptive {
    <f32> $25 = stack load top adj $7
    <f32> $26 : stack pop $7
    <f32> $27 : stack acc adj $7, val = $25
    <i32> $28 = local load [$1]
    <*f32> $29 = global ptr [S4place<f32>], index [$28] activate=true
    <f32> $30 = atomic add($29, $25)
  }
}
}
[I 04/20/23 19:15:56.153 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Make thread local:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
  <i64> $5 = global load $4
  <i32> $6 = cast_value<i32> $5
  <f32> $7 = stack alloc (max_size=1024)
  <f32> $8 = const 0.0
  <f32> $9 : stack push $7, val = $8
  <f32> $10 : stack push $7, val = $8
  <i32> $11 = const 1
  $12 : for in range($3, $11) block_dim=adaptive {
    <i32> $13 = loop $12 index 0
    <i32> $14 : local store [$1 <- $13]
    <*f32> $15 = global ptr [S2place<f32>], index [$13] activate=false
    <f32> $16 = global load $15
    <f32> $17 = stack load top $7
    <f32> $18 = add $17 $16
    <f32> $19 : stack push $7, val = $18
  }
  <*f32> $20 = global ptr [S10place<f32>], index [$6, $3] activate=true
  <f32> $21 = global load $20
  $22 : global store [$20 <- $8]
  <f32> $23 : stack acc adj $7, val = $21
  $24 : reversed for in range($3, $11) block_dim=adaptive {
    <f32> $25 = stack load top adj $7
    <f32> $26 : stack pop $7
    <f32> $27 : stack acc adj $7, val = $25
    <i32> $28 = local load [$1]
    <*f32> $29 = global ptr [S4place<f32>], index [$28] activate=true
    <f32> $30 = atomic add($29, $25)
  }
}
}
[I 04/20/23 19:15:56.153 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Make mesh thread local:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
  <i64> $5 = global load $4
  <i32> $6 = cast_value<i32> $5
  <f32> $7 = stack alloc (max_size=1024)
  <f32> $8 = const 0.0
  <f32> $9 : stack push $7, val = $8
  <f32> $10 : stack push $7, val = $8
  <i32> $11 = const 1
  $12 : for in range($3, $11) block_dim=adaptive {
    <i32> $13 = loop $12 index 0
    <i32> $14 : local store [$1 <- $13]
    <*f32> $15 = global ptr [S2place<f32>], index [$13] activate=false
    <f32> $16 = global load $15
    <f32> $17 = stack load top $7
    <f32> $18 = add $17 $16
    <f32> $19 : stack push $7, val = $18
  }
  <*f32> $20 = global ptr [S10place<f32>], index [$6, $3] activate=true
  <f32> $21 = global load $20
  $22 : global store [$20 <- $8]
  <f32> $23 : stack acc adj $7, val = $21
  $24 : reversed for in range($3, $11) block_dim=adaptive {
    <f32> $25 = stack load top adj $7
    <f32> $26 : stack pop $7
    <f32> $27 : stack acc adj $7, val = $25
    <i32> $28 = local load [$1]
    <*f32> $29 = global ptr [S4place<f32>], index [$28] activate=true
    <f32> $30 = atomic add($29, $25)
  }
}
}
[I 04/20/23 19:15:56.153 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Make mesh block local:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
  <i64> $5 = global load $4
  <i32> $6 = cast_value<i32> $5
  <f32> $7 = stack alloc (max_size=1024)
  <f32> $8 = const 0.0
  <f32> $9 : stack push $7, val = $8
  <f32> $10 : stack push $7, val = $8
  <i32> $11 = const 1
  $12 : for in range($3, $11) block_dim=adaptive {
    <i32> $13 = loop $12 index 0
    <i32> $14 : local store [$1 <- $13]
    <*f32> $15 = global ptr [S2place<f32>], index [$13] activate=false
    <f32> $16 = global load $15
    <f32> $17 = stack load top $7
    <f32> $18 = add $17 $16
    <f32> $19 : stack push $7, val = $18
  }
  <*f32> $20 = global ptr [S10place<f32>], index [$6, $3] activate=true
  <f32> $21 = global load $20
  $22 : global store [$20 <- $8]
  <f32> $23 : stack acc adj $7, val = $21
  $24 : reversed for in range($3, $11) block_dim=adaptive {
    <f32> $25 = stack load top adj $7
    <f32> $26 : stack pop $7
    <f32> $27 : stack acc adj $7, val = $25
    <i32> $28 = local load [$1]
    <*f32> $29 = global ptr [S4place<f32>], index [$28] activate=true
    <f32> $30 = atomic add($29, $25)
  }
}
}
[I 04/20/23 19:15:56.153 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Simplified X:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
  <i64> $5 = global load $4
  <i32> $6 = cast_value<i32> $5
  <f32> $7 = stack alloc (max_size=1024)
  <f32> $8 = const 0.0
  <f32> $9 : stack push $7, val = $8
  <f32> $10 : stack push $7, val = $8
  <i32> $11 = const 1
  $12 : for in range($3, $11) block_dim=adaptive {
    <i32> $13 = loop $12 index 0
    <i32> $14 : local store [$1 <- $13]
    <*f32> $15 = global ptr [S2place<f32>], index [$13] activate=false
    <f32> $16 = global load $15
    <f32> $17 = stack load top $7
    <f32> $18 = add $17 $16
    <f32> $19 : stack push $7, val = $18
  }
  <*f32> $20 = global ptr [S10place<f32>], index [$6, $3] activate=true
  <f32> $21 = global load $20
  $22 : global store [$20 <- $8]
  <f32> $23 : stack acc adj $7, val = $21
  $24 : reversed for in range($3, $11) block_dim=adaptive {
    <f32> $25 = stack load top adj $7
    <f32> $26 : stack pop $7
    <f32> $27 : stack acc adj $7, val = $25
    <i32> $28 = local load [$1]
    <*f32> $29 = global ptr [S4place<f32>], index [$28] activate=true
    <f32> $30 = atomic add($29, $25)
  }
}
}
[I 04/20/23 19:15:56.153 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Make block local:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
  <i64> $5 = global load $4
  <i32> $6 = cast_value<i32> $5
  <f32> $7 = stack alloc (max_size=1024)
  <f32> $8 = const 0.0
  <f32> $9 : stack push $7, val = $8
  <f32> $10 : stack push $7, val = $8
  <i32> $11 = const 1
  $12 : for in range($3, $11) block_dim=adaptive {
    <i32> $13 = loop $12 index 0
    <i32> $14 : local store [$1 <- $13]
    <*f32> $15 = global ptr [S2place<f32>], index [$13] activate=false
    <f32> $16 = global load $15
    <f32> $17 = stack load top $7
    <f32> $18 = add $17 $16
    <f32> $19 : stack push $7, val = $18
  }
  <*f32> $20 = global ptr [S10place<f32>], index [$6, $3] activate=true
  <f32> $21 = global load $20
  $22 : global store [$20 <- $8]
  <f32> $23 : stack acc adj $7, val = $21
  $24 : reversed for in range($3, $11) block_dim=adaptive {
    <f32> $25 = stack load top adj $7
    <f32> $26 : stack pop $7
    <f32> $27 : stack acc adj $7, val = $25
    <i32> $28 = local load [$1]
    <*f32> $29 = global ptr [S4place<f32>], index [$28] activate=true
    <f32> $30 = atomic add($29, $25)
  }
}
}
[I 04/20/23 19:15:56.153 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Demote mesh statements:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
  <i64> $5 = global load $4
  <i32> $6 = cast_value<i32> $5
  <f32> $7 = stack alloc (max_size=1024)
  <f32> $8 = const 0.0
  <f32> $9 : stack push $7, val = $8
  <f32> $10 : stack push $7, val = $8
  <i32> $11 = const 1
  $12 : for in range($3, $11) block_dim=adaptive {
    <i32> $13 = loop $12 index 0
    <i32> $14 : local store [$1 <- $13]
    <*f32> $15 = global ptr [S2place<f32>], index [$13] activate=false
    <f32> $16 = global load $15
    <f32> $17 = stack load top $7
    <f32> $18 = add $17 $16
    <f32> $19 : stack push $7, val = $18
  }
  <*f32> $20 = global ptr [S10place<f32>], index [$6, $3] activate=true
  <f32> $21 = global load $20
  $22 : global store [$20 <- $8]
  <f32> $23 : stack acc adj $7, val = $21
  $24 : reversed for in range($3, $11) block_dim=adaptive {
    <f32> $25 = stack load top adj $7
    <f32> $26 : stack pop $7
    <f32> $27 : stack acc adj $7, val = $25
    <i32> $28 = local load [$1]
    <*f32> $29 = global ptr [S4place<f32>], index [$28] activate=true
    <f32> $30 = atomic add($29, $25)
  }
}
}
[I 04/20/23 19:15:56.153 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Atomics demoted II:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
  <i64> $5 = global load $4
  <i32> $6 = cast_value<i32> $5
  <f32> $7 = stack alloc (max_size=1024)
  <f32> $8 = const 0.0
  <f32> $9 : stack push $7, val = $8
  <f32> $10 : stack push $7, val = $8
  <i32> $11 = const 1
  $12 : for in range($3, $11) block_dim=adaptive {
    <i32> $13 = loop $12 index 0
    <i32> $14 : local store [$1 <- $13]
    <*f32> $15 = global ptr [S2place<f32>], index [$13] activate=false
    <f32> $16 = global load $15
    <f32> $17 = stack load top $7
    <f32> $18 = add $17 $16
    <f32> $19 : stack push $7, val = $18
  }
  <*f32> $20 = global ptr [S10place<f32>], index [$6, $3] activate=true
  <f32> $21 = global load $20
  $22 : global store [$20 <- $8]
  <f32> $23 : stack acc adj $7, val = $21
  $24 : reversed for in range($3, $11) block_dim=adaptive {
    <f32> $25 = stack load top adj $7
    <f32> $26 : stack pop $7
    <f32> $27 : stack acc adj $7, val = $25
    <i32> $28 = local load [$1]
    <*f32> $29 = global ptr [S4place<f32>], index [$28] activate=true
    <f32> $30 = atomic add($29, $25)
  }
}
}
[I 04/20/23 19:15:56.154 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Remove range assumption:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
  <i64> $5 = global load $4
  <i32> $6 = cast_value<i32> $5
  <f32> $7 = stack alloc (max_size=1024)
  <f32> $8 = const 0.0
  <f32> $9 : stack push $7, val = $8
  <f32> $10 : stack push $7, val = $8
  <i32> $11 = const 1
  $12 : for in range($3, $11) block_dim=adaptive {
    <i32> $13 = loop $12 index 0
    <i32> $14 : local store [$1 <- $13]
    <*f32> $15 = global ptr [S2place<f32>], index [$13] activate=false
    <f32> $16 = global load $15
    <f32> $17 = stack load top $7
    <f32> $18 = add $17 $16
    <f32> $19 : stack push $7, val = $18
  }
  <*f32> $20 = global ptr [S10place<f32>], index [$6, $3] activate=true
  <f32> $21 = global load $20
  $22 : global store [$20 <- $8]
  <f32> $23 : stack acc adj $7, val = $21
  $24 : reversed for in range($3, $11) block_dim=adaptive {
    <f32> $25 = stack load top adj $7
    <f32> $26 : stack pop $7
    <f32> $27 : stack acc adj $7, val = $25
    <i32> $28 = local load [$1]
    <*f32> $29 = global ptr [S4place<f32>], index [$28] activate=true
    <f32> $30 = atomic add($29, $25)
  }
}
}
[I 04/20/23 19:15:56.154 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Remove loop_unique:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
  <i64> $5 = global load $4
  <i32> $6 = cast_value<i32> $5
  <f32> $7 = stack alloc (max_size=1024)
  <f32> $8 = const 0.0
  <f32> $9 : stack push $7, val = $8
  <f32> $10 : stack push $7, val = $8
  <i32> $11 = const 1
  $12 : for in range($3, $11) block_dim=adaptive {
    <i32> $13 = loop $12 index 0
    <i32> $14 : local store [$1 <- $13]
    <*f32> $15 = global ptr [S2place<f32>], index [$13] activate=false
    <f32> $16 = global load $15
    <f32> $17 = stack load top $7
    <f32> $18 = add $17 $16
    <f32> $19 : stack push $7, val = $18
  }
  <*f32> $20 = global ptr [S10place<f32>], index [$6, $3] activate=true
  <f32> $21 = global load $20
  $22 : global store [$20 <- $8]
  <f32> $23 : stack acc adj $7, val = $21
  $24 : reversed for in range($3, $11) block_dim=adaptive {
    <f32> $25 = stack load top adj $7
    <f32> $26 : stack pop $7
    <f32> $27 : stack acc adj $7, val = $25
    <i32> $28 = local load [$1]
    <*f32> $29 = global ptr [S4place<f32>], index [$28] activate=true
    <f32> $30 = atomic add($29, $25)
  }
}
}
[I 04/20/23 19:15:56.154 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Simplified before lower access:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
  <i64> $5 = global load $4
  <i32> $6 = cast_value<i32> $5
  <f32> $7 = stack alloc (max_size=1024)
  <f32> $8 = const 0.0
  <f32> $9 : stack push $7, val = $8
  <f32> $10 : stack push $7, val = $8
  <i32> $11 = const 1
  $12 : for in range($3, $11) block_dim=adaptive {
    <i32> $13 = loop $12 index 0
    <i32> $14 : local store [$1 <- $13]
    <*f32> $15 = global ptr [S2place<f32>], index [$13] activate=false
    <f32> $16 = global load $15
    <f32> $17 = stack load top $7
    <f32> $18 = add $17 $16
    <f32> $19 : stack push $7, val = $18
  }
  <*f32> $20 = global ptr [S10place<f32>], index [$6, $3] activate=true
  <f32> $21 = global load $20
  $22 : global store [$20 <- $8]
  <f32> $23 : stack acc adj $7, val = $21
  $24 : reversed for in range($3, $11) block_dim=adaptive {
    <f32> $25 = stack load top adj $7
    <f32> $26 : stack pop $7
    <f32> $27 : stack acc adj $7, val = $25
    <i32> $28 = local load [$1]
    <*f32> $29 = global ptr [S4place<f32>], index [$28] activate=true
    <f32> $30 = atomic add($29, $25)
  }
}
}
[I 04/20/23 19:15:56.154 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Access lowered:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
  <*gen> $5 = get root [S0root][root]
  <i32> $6 = linearized(ind {}, stride {})
  <*gen> $7 = [S0root][root]::lookup($5, $6) activate = false
  <*gen> $8 = get child [S0root->S5dense] $7
  <i32> $9 = const 0
  <i32> $10 = bit_shr $2 $9
  <i32> $11 = const 0
  <i32> $12 = bit_shr $3 $11
  <i32> $13 = linearized(ind {$10, $12}, stride {8192, 3})
  <*gen> $14 = [S5dense][dense]::lookup($8, $13) activate = false
  <*i64> $15 = get child [S5dense->S6place<i64>] $14
  <i64> $16 = global load $15
  <i32> $17 = cast_value<i32> $16
  <f32> $18 = stack alloc (max_size=1024)
  <f32> $19 = const 0.0
  <f32> $20 : stack push $18, val = $19
  <f32> $21 : stack push $18, val = $19
  <i32> $22 = const 1
  $23 : for in range($3, $22) block_dim=adaptive {
    <i32> $24 = loop $23 index 0
    <i32> $25 : local store [$1 <- $24]
    <*f32> $26 = global ptr [S2place<f32>], index [$24] activate=false
    <*gen> $27 = get root [S0root][root]
    <i32> $28 = linearized(ind {}, stride {})
    <*gen> $29 = [S0root][root]::lookup($27, $28) activate = false
    <*gen> $30 = get child [S0root->S1dense] $29
    <i32> $31 = const 0
    <i32> $32 = bit_shr $24 $31
    <i32> $33 = linearized(ind {$32}, stride {8388608})
    <*gen> $34 = [S1dense][dense]::lookup($30, $33) activate = false
    <*f32> $35 = get child [S1dense->S2place<f32>] $34
    <f32> $36 = global load $35
    <f32> $37 = stack load top $18
    <f32> $38 = add $37 $36
    <f32> $39 : stack push $18, val = $38
  }
  <*f32> $40 = global ptr [S10place<f32>], index [$17, $3] activate=true
  <*gen> $41 = get root [S0root][root]
  <i32> $42 = linearized(ind {}, stride {})
  <*gen> $43 = [S0root][root]::lookup($41, $42) activate = false
  <*gen> $44 = get child [S0root->S9dense] $43
  <i32> $45 = const 0
  <i32> $46 = bit_shr $17 $45
  <i32> $47 = const 0
  <i32> $48 = bit_shr $3 $47
  <i32> $49 = linearized(ind {$46, $48}, stride {8192, 3})
  <*gen> $50 = [S9dense][dense]::lookup($44, $49) activate = false
  <*f32> $51 = get child [S9dense->S10place<f32>] $50
  <f32> $52 = global load $51
  <*gen> $53 = get root [S0root][root]
  <i32> $54 = linearized(ind {}, stride {})
  <*gen> $55 = [S0root][root]::lookup($53, $54) activate = false
  <*gen> $56 = get child [S0root->S9dense] $55
  <i32> $57 = const 0
  <i32> $58 = bit_shr $17 $57
  <i32> $59 = const 0
  <i32> $60 = bit_shr $3 $59
  <i32> $61 = linearized(ind {$58, $60}, stride {8192, 3})
  <*gen> $62 = [S9dense][dense]::lookup($56, $61) activate = false
  <*f32> $63 = get child [S9dense->S10place<f32>] $62
  $64 : global store [$63 <- $19]
  <f32> $65 : stack acc adj $18, val = $52
  $66 : reversed for in range($3, $22) block_dim=adaptive {
    <f32> $67 = stack load top adj $18
    <f32> $68 : stack pop $18
    <f32> $69 : stack acc adj $18, val = $67
    <i32> $70 = local load [$1]
    <*f32> $71 = global ptr [S4place<f32>], index [$70] activate=true
    <*gen> $72 = get root [S0root][root]
    <i32> $73 = linearized(ind {}, stride {})
    <*gen> $74 = [S0root][root]::lookup($72, $73) activate = false
    <*gen> $75 = get child [S0root->S3dense] $74
    <i32> $76 = const 0
    <i32> $77 = bit_shr $70 $76
    <i32> $78 = linearized(ind {$77}, stride {8388608})
    <*gen> $79 = [S3dense][dense]::lookup($75, $78) activate = false
    <*f32> $80 = get child [S3dense->S4place<f32>] $79
    <f32> $81 = atomic add($80, $67)
  }
}
}
[I 04/20/23 19:15:56.154 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] DIE:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*gen> $4 = get root [S0root][root]
  <i32> $5 = linearized(ind {}, stride {})
  <*gen> $6 = [S0root][root]::lookup($4, $5) activate = false
  <*gen> $7 = get child [S0root->S5dense] $6
  <i32> $8 = const 0
  <i32> $9 = bit_shr $2 $8
  <i32> $10 = const 0
  <i32> $11 = bit_shr $3 $10
  <i32> $12 = linearized(ind {$9, $11}, stride {8192, 3})
  <*gen> $13 = [S5dense][dense]::lookup($7, $12) activate = false
  <*i64> $14 = get child [S5dense->S6place<i64>] $13
  <i64> $15 = global load $14
  <i32> $16 = cast_value<i32> $15
  <f32> $17 = stack alloc (max_size=1024)
  <f32> $18 = const 0.0
  <f32> $19 : stack push $17, val = $18
  <f32> $20 : stack push $17, val = $18
  <i32> $21 = const 1
  $22 : for in range($3, $21) block_dim=adaptive {
    <i32> $23 = loop $22 index 0
    <i32> $24 : local store [$1 <- $23]
    <*gen> $25 = get root [S0root][root]
    <i32> $26 = linearized(ind {}, stride {})
    <*gen> $27 = [S0root][root]::lookup($25, $26) activate = false
    <*gen> $28 = get child [S0root->S1dense] $27
    <i32> $29 = const 0
    <i32> $30 = bit_shr $23 $29
    <i32> $31 = linearized(ind {$30}, stride {8388608})
    <*gen> $32 = [S1dense][dense]::lookup($28, $31) activate = false
    <*f32> $33 = get child [S1dense->S2place<f32>] $32
    <f32> $34 = global load $33
    <f32> $35 = stack load top $17
    <f32> $36 = add $35 $34
    <f32> $37 : stack push $17, val = $36
  }
  <*f32> $38 = global ptr [S10place<f32>], index [$16, $3] activate=true
  <*gen> $39 = get root [S0root][root]
  <i32> $40 = linearized(ind {}, stride {})
  <*gen> $41 = [S0root][root]::lookup($39, $40) activate = false
  <*gen> $42 = get child [S0root->S9dense] $41
  <i32> $43 = const 0
  <i32> $44 = bit_shr $16 $43
  <i32> $45 = const 0
  <i32> $46 = bit_shr $3 $45
  <i32> $47 = linearized(ind {$44, $46}, stride {8192, 3})
  <*gen> $48 = [S9dense][dense]::lookup($42, $47) activate = false
  <*f32> $49 = get child [S9dense->S10place<f32>] $48
  <f32> $50 = global load $49
  <*gen> $51 = get root [S0root][root]
  <i32> $52 = linearized(ind {}, stride {})
  <*gen> $53 = [S0root][root]::lookup($51, $52) activate = false
  <*gen> $54 = get child [S0root->S9dense] $53
  <i32> $55 = const 0
  <i32> $56 = bit_shr $16 $55
  <i32> $57 = const 0
  <i32> $58 = bit_shr $3 $57
  <i32> $59 = linearized(ind {$56, $58}, stride {8192, 3})
  <*gen> $60 = [S9dense][dense]::lookup($54, $59) activate = false
  <*f32> $61 = get child [S9dense->S10place<f32>] $60
  $62 : global store [$61 <- $18]
  <f32> $63 : stack acc adj $17, val = $50
  $64 : reversed for in range($3, $21) block_dim=adaptive {
    <f32> $65 = stack load top adj $17
    <f32> $66 : stack pop $17
    <f32> $67 : stack acc adj $17, val = $65
    <i32> $68 = local load [$1]
    <*f32> $69 = global ptr [S4place<f32>], index [$68] activate=true
    <*gen> $70 = get root [S0root][root]
    <i32> $71 = linearized(ind {}, stride {})
    <*gen> $72 = [S0root][root]::lookup($70, $71) activate = false
    <*gen> $73 = get child [S0root->S3dense] $72
    <i32> $74 = const 0
    <i32> $75 = bit_shr $68 $74
    <i32> $76 = linearized(ind {$75}, stride {8388608})
    <*gen> $77 = [S3dense][dense]::lookup($73, $76) activate = false
    <*f32> $78 = get child [S3dense->S4place<f32>] $77
    <f32> $79 = atomic add($78, $65)
  }
}
}
[I 04/20/23 19:15:56.155 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Access flagged III:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*gen> $4 = get root [S0root][root]
  <i32> $5 = linearized(ind {}, stride {})
  <*gen> $6 = [S0root][root]::lookup($4, $5) activate = false
  <*gen> $7 = get child [S0root->S5dense] $6
  <i32> $8 = const 0
  <i32> $9 = bit_shr $2 $8
  <i32> $10 = const 0
  <i32> $11 = bit_shr $3 $10
  <i32> $12 = linearized(ind {$9, $11}, stride {8192, 3})
  <*gen> $13 = [S5dense][dense]::lookup($7, $12) activate = false
  <*i64> $14 = get child [S5dense->S6place<i64>] $13
  <i64> $15 = global load $14
  <i32> $16 = cast_value<i32> $15
  <f32> $17 = stack alloc (max_size=1024)
  <f32> $18 = const 0.0
  <f32> $19 : stack push $17, val = $18
  <f32> $20 : stack push $17, val = $18
  <i32> $21 = const 1
  $22 : for in range($3, $21) block_dim=adaptive {
    <i32> $23 = loop $22 index 0
    <i32> $24 : local store [$1 <- $23]
    <*gen> $25 = get root [S0root][root]
    <i32> $26 = linearized(ind {}, stride {})
    <*gen> $27 = [S0root][root]::lookup($25, $26) activate = false
    <*gen> $28 = get child [S0root->S1dense] $27
    <i32> $29 = const 0
    <i32> $30 = bit_shr $23 $29
    <i32> $31 = linearized(ind {$30}, stride {8388608})
    <*gen> $32 = [S1dense][dense]::lookup($28, $31) activate = false
    <*f32> $33 = get child [S1dense->S2place<f32>] $32
    <f32> $34 = global load $33
    <f32> $35 = stack load top $17
    <f32> $36 = add $35 $34
    <f32> $37 : stack push $17, val = $36
  }
  <*f32> $38 = global ptr [S10place<f32>], index [$16, $3] activate=false
  <*gen> $39 = get root [S0root][root]
  <i32> $40 = linearized(ind {}, stride {})
  <*gen> $41 = [S0root][root]::lookup($39, $40) activate = false
  <*gen> $42 = get child [S0root->S9dense] $41
  <i32> $43 = const 0
  <i32> $44 = bit_shr $16 $43
  <i32> $45 = const 0
  <i32> $46 = bit_shr $3 $45
  <i32> $47 = linearized(ind {$44, $46}, stride {8192, 3})
  <*gen> $48 = [S9dense][dense]::lookup($42, $47) activate = false
  <*f32> $49 = get child [S9dense->S10place<f32>] $48
  <f32> $50 = global load $49
  <*gen> $51 = get root [S0root][root]
  <i32> $52 = linearized(ind {}, stride {})
  <*gen> $53 = [S0root][root]::lookup($51, $52) activate = false
  <*gen> $54 = get child [S0root->S9dense] $53
  <i32> $55 = const 0
  <i32> $56 = bit_shr $16 $55
  <i32> $57 = const 0
  <i32> $58 = bit_shr $3 $57
  <i32> $59 = linearized(ind {$56, $58}, stride {8192, 3})
  <*gen> $60 = [S9dense][dense]::lookup($54, $59) activate = false
  <*f32> $61 = get child [S9dense->S10place<f32>] $60
  $62 : global store [$61 <- $18]
  <f32> $63 : stack acc adj $17, val = $50
  $64 : reversed for in range($3, $21) block_dim=adaptive {
    <f32> $65 = stack load top adj $17
    <f32> $66 : stack pop $17
    <f32> $67 : stack acc adj $17, val = $65
    <i32> $68 = local load [$1]
    <*f32> $69 = global ptr [S4place<f32>], index [$68] activate=false
    <*gen> $70 = get root [S0root][root]
    <i32> $71 = linearized(ind {}, stride {})
    <*gen> $72 = [S0root][root]::lookup($70, $71) activate = false
    <*gen> $73 = get child [S0root->S3dense] $72
    <i32> $74 = const 0
    <i32> $75 = bit_shr $68 $74
    <i32> $76 = linearized(ind {$75}, stride {8388608})
    <*gen> $77 = [S3dense][dense]::lookup($73, $76) activate = false
    <*f32> $78 = get child [S3dense->S4place<f32>] $77
    <f32> $79 = atomic add($78, $65)
  }
}
}
[I 04/20/23 19:15:56.155 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Operations demoted:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*gen> $4 = get root [S0root][root]
  <i32> $5 = linearized(ind {}, stride {})
  <*gen> $6 = [S0root][root]::lookup($4, $5) activate = false
  <*gen> $7 = get child [S0root->S5dense] $6
  <i32> $8 = const 0
  <u32> $9 = reinterpret_cast_bits<u32> $2
  <u32> $10 = cast_value<u32> $8
  <u32> $11 = bit_sar $9 $10
  <i32> $12 = reinterpret_cast_bits<i32> $11
  <i32> $13 = const 0
  <u32> $14 = reinterpret_cast_bits<u32> $3
  <u32> $15 = cast_value<u32> $13
  <u32> $16 = bit_sar $14 $15
  <i32> $17 = reinterpret_cast_bits<i32> $16
  <i32> $18 = linearized(ind {$12, $17}, stride {8192, 3})
  <*gen> $19 = [S5dense][dense]::lookup($7, $18) activate = false
  <*i64> $20 = get child [S5dense->S6place<i64>] $19
  <i64> $21 = global load $20
  <i32> $22 = cast_value<i32> $21
  <f32> $23 = stack alloc (max_size=1024)
  <f32> $24 = const 0.0
  <f32> $25 : stack push $23, val = $24
  <f32> $26 : stack push $23, val = $24
  <i32> $27 = const 1
  $28 : for in range($3, $27) block_dim=adaptive {
    <i32> $29 = loop $28 index 0
    <i32> $30 : local store [$1 <- $29]
    <*gen> $31 = get root [S0root][root]
    <i32> $32 = linearized(ind {}, stride {})
    <*gen> $33 = [S0root][root]::lookup($31, $32) activate = false
    <*gen> $34 = get child [S0root->S1dense] $33
    <i32> $35 = const 0
    <u32> $36 = reinterpret_cast_bits<u32> $29
    <u32> $37 = cast_value<u32> $35
    <u32> $38 = bit_sar $36 $37
    <i32> $39 = reinterpret_cast_bits<i32> $38
    <i32> $40 = linearized(ind {$39}, stride {8388608})
    <*gen> $41 = [S1dense][dense]::lookup($34, $40) activate = false
    <*f32> $42 = get child [S1dense->S2place<f32>] $41
    <f32> $43 = global load $42
    <f32> $44 = stack load top $23
    <f32> $45 = add $44 $43
    <f32> $46 : stack push $23, val = $45
  }
  <*f32> $47 = global ptr [S10place<f32>], index [$22, $3] activate=false
  <*gen> $48 = get root [S0root][root]
  <i32> $49 = linearized(ind {}, stride {})
  <*gen> $50 = [S0root][root]::lookup($48, $49) activate = false
  <*gen> $51 = get child [S0root->S9dense] $50
  <i32> $52 = const 0
  <u32> $53 = reinterpret_cast_bits<u32> $22
  <u32> $54 = cast_value<u32> $52
  <u32> $55 = bit_sar $53 $54
  <i32> $56 = reinterpret_cast_bits<i32> $55
  <i32> $57 = const 0
  <u32> $58 = reinterpret_cast_bits<u32> $3
  <u32> $59 = cast_value<u32> $57
  <u32> $60 = bit_sar $58 $59
  <i32> $61 = reinterpret_cast_bits<i32> $60
  <i32> $62 = linearized(ind {$56, $61}, stride {8192, 3})
  <*gen> $63 = [S9dense][dense]::lookup($51, $62) activate = false
  <*f32> $64 = get child [S9dense->S10place<f32>] $63
  <f32> $65 = global load $64
  <*gen> $66 = get root [S0root][root]
  <i32> $67 = linearized(ind {}, stride {})
  <*gen> $68 = [S0root][root]::lookup($66, $67) activate = false
  <*gen> $69 = get child [S0root->S9dense] $68
  <i32> $70 = const 0
  <u32> $71 = reinterpret_cast_bits<u32> $22
  <u32> $72 = cast_value<u32> $70
  <u32> $73 = bit_sar $71 $72
  <i32> $74 = reinterpret_cast_bits<i32> $73
  <i32> $75 = const 0
  <u32> $76 = reinterpret_cast_bits<u32> $3
  <u32> $77 = cast_value<u32> $75
  <u32> $78 = bit_sar $76 $77
  <i32> $79 = reinterpret_cast_bits<i32> $78
  <i32> $80 = linearized(ind {$74, $79}, stride {8192, 3})
  <*gen> $81 = [S9dense][dense]::lookup($69, $80) activate = false
  <*f32> $82 = get child [S9dense->S10place<f32>] $81
  $83 : global store [$82 <- $24]
  <f32> $84 : stack acc adj $23, val = $65
  $85 : reversed for in range($3, $27) block_dim=adaptive {
    <f32> $86 = stack load top adj $23
    <f32> $87 : stack pop $23
    <f32> $88 : stack acc adj $23, val = $86
    <i32> $89 = local load [$1]
    <*f32> $90 = global ptr [S4place<f32>], index [$89] activate=false
    <*gen> $91 = get root [S0root][root]
    <i32> $92 = linearized(ind {}, stride {})
    <*gen> $93 = [S0root][root]::lookup($91, $92) activate = false
    <*gen> $94 = get child [S0root->S3dense] $93
    <i32> $95 = const 0
    <u32> $96 = reinterpret_cast_bits<u32> $89
    <u32> $97 = cast_value<u32> $95
    <u32> $98 = bit_sar $96 $97
    <i32> $99 = reinterpret_cast_bits<i32> $98
    <i32> $100 = linearized(ind {$99}, stride {8388608})
    <*gen> $101 = [S3dense][dense]::lookup($94, $100) activate = false
    <*f32> $102 = get child [S3dense->S4place<f32>] $101
    <f32> $103 = atomic add($102, $86)
  }
}
}
[I 04/20/23 19:15:56.156 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Simplified IV:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*gen> $4 = get root [S0root][root]
  <*gen> $5 = [S0root][root]::lookup($4, $3) activate = false
  <*gen> $6 = get child [S0root->S5dense] $5
  <i32> $7 = const 1
  <i32> $8 = const 3
  <i32> $9 = mul $2 $8
  <*gen> $10 = [S5dense][dense]::lookup($6, $9) activate = false
  <*i64> $11 = get child [S5dense->S6place<i64>] $10
  <i64> $12 = global load $11
  <i32> $13 = cast_value<i32> $12
  <f32> $14 = stack alloc (max_size=1024)
  <f32> $15 = const 0.0
  <f32> $16 : stack push $14, val = $15
  <f32> $17 : stack push $14, val = $15
  $18 : for in range($3, $7) block_dim=adaptive {
    <i32> $19 = loop $18 index 0
    <i32> $20 : local store [$1 <- $19]
    <*gen> $21 = get child [S0root->S1dense] $5
    <*gen> $22 = [S1dense][dense]::lookup($21, $19) activate = false
    <*f32> $23 = get child [S1dense->S2place<f32>] $22
    <f32> $24 = global load $23
    <f32> $25 = stack load top $14
    <f32> $26 = add $25 $24
    <f32> $27 : stack push $14, val = $26
  }
  <*gen> $28 = get child [S0root->S9dense] $5
  <i32> $29 = mul $13 $8
  <*gen> $30 = [S9dense][dense]::lookup($28, $29) activate = false
  <*f32> $31 = get child [S9dense->S10place<f32>] $30
  <f32> $32 = global load $31
  $33 : global store [$31 <- $15]
  <f32> $34 : stack acc adj $14, val = $32
  $35 : reversed for in range($3, $7) block_dim=adaptive {
    <f32> $36 = stack load top adj $14
    <f32> $37 : stack pop $14
    <f32> $38 : stack acc adj $14, val = $36
    <i32> $39 = local load [$1]
    <*gen> $40 = get child [S0root->S3dense] $5
    <*gen> $41 = [S3dense][dense]::lookup($40, $39) activate = false
    <*f32> $42 = get child [S3dense->S4place<f32>] $41
    <f32> $43 = atomic add($42, $36)
  }
}
}
[I 04/20/23 19:15:56.156 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Autodiff stack size determined:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*gen> $4 = get root [S0root][root]
  <*gen> $5 = [S0root][root]::lookup($4, $3) activate = false
  <*gen> $6 = get child [S0root->S5dense] $5
  <i32> $7 = const 1
  <i32> $8 = const 3
  <i32> $9 = mul $2 $8
  <*gen> $10 = [S5dense][dense]::lookup($6, $9) activate = false
  <*i64> $11 = get child [S5dense->S6place<i64>] $10
  <i64> $12 = global load $11
  <i32> $13 = cast_value<i32> $12
  <f32> $14 = stack alloc (max_size=1024)
  <f32> $15 = const 0.0
  <f32> $16 : stack push $14, val = $15
  <f32> $17 : stack push $14, val = $15
  $18 : for in range($3, $7) block_dim=adaptive {
    <i32> $19 = loop $18 index 0
    <i32> $20 : local store [$1 <- $19]
    <*gen> $21 = get child [S0root->S1dense] $5
    <*gen> $22 = [S1dense][dense]::lookup($21, $19) activate = false
    <*f32> $23 = get child [S1dense->S2place<f32>] $22
    <f32> $24 = global load $23
    <f32> $25 = stack load top $14
    <f32> $26 = add $25 $24
    <f32> $27 : stack push $14, val = $26
  }
  <*gen> $28 = get child [S0root->S9dense] $5
  <i32> $29 = mul $13 $8
  <*gen> $30 = [S9dense][dense]::lookup($28, $29) activate = false
  <*f32> $31 = get child [S9dense->S10place<f32>] $30
  <f32> $32 = global load $31
  $33 : global store [$31 <- $15]
  <f32> $34 : stack acc adj $14, val = $32
  $35 : reversed for in range($3, $7) block_dim=adaptive {
    <f32> $36 = stack load top adj $14
    <f32> $37 : stack pop $14
    <f32> $38 : stack acc adj $14, val = $36
    <i32> $39 = local load [$1]
    <*gen> $40 = get child [S0root->S3dense] $5
    <*gen> $41 = [S3dense][dense]::lookup($40, $39) activate = false
    <*f32> $42 = get child [S3dense->S4place<f32>] $41
    <f32> $43 = atomic add($42, $36)
  }
}
}
[I 04/20/23 19:15:56.156 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_stack_c79_0_reverse_grad] Bit struct stores optimized:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = alloca
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <*gen> $4 = get root [S0root][root]
  <*gen> $5 = [S0root][root]::lookup($4, $3) activate = false
  <*gen> $6 = get child [S0root->S5dense] $5
  <i32> $7 = const 1
  <i32> $8 = const 3
  <i32> $9 = mul $2 $8
  <*gen> $10 = [S5dense][dense]::lookup($6, $9) activate = false
  <*i64> $11 = get child [S5dense->S6place<i64>] $10
  <i64> $12 = global load $11
  <i32> $13 = cast_value<i32> $12
  <f32> $14 = stack alloc (max_size=1024)
  <f32> $15 = const 0.0
  <f32> $16 : stack push $14, val = $15
  <f32> $17 : stack push $14, val = $15
  $18 : for in range($3, $7) block_dim=adaptive {
    <i32> $19 = loop $18 index 0
    <i32> $20 : local store [$1 <- $19]
    <*gen> $21 = get child [S0root->S1dense] $5
    <*gen> $22 = [S1dense][dense]::lookup($21, $19) activate = false
    <*f32> $23 = get child [S1dense->S2place<f32>] $22
    <f32> $24 = global load $23
    <f32> $25 = stack load top $14
    <f32> $26 = add $25 $24
    <f32> $27 : stack push $14, val = $26
  }
  <*gen> $28 = get child [S0root->S9dense] $5
  <i32> $29 = mul $13 $8
  <*gen> $30 = [S9dense][dense]::lookup($28, $29) activate = false
  <*f32> $31 = get child [S9dense->S10place<f32>] $30
  <f32> $32 = global load $31
  $33 : global store [$31 <- $15]
  <f32> $34 : stack acc adj $14, val = $32
  $35 : reversed for in range($3, $7) block_dim=adaptive {
    <f32> $36 = stack load top adj $14
    <f32> $37 : stack pop $14
    <f32> $38 : stack acc adj $14, val = $36
    <i32> $39 = local load [$1]
    <*gen> $40 = get child [S0root->S3dense] $5
    <*gen> $41 = [S3dense][dense]::lookup($40, $39) activate = false
    <*f32> $42 = get child [S3dense->S4place<f32>] $41
    <f32> $43 = atomic add($42, $36)
  }
}
}
[I 04/20/23 19:15:56.199 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Initial IR:
kernel {
  $0 : for @tmp0 in S4place<f32> noneblock_dim=adaptive {
    $1 = alloca @tmp1
    @tmp1 = [@tmp0] (dt=[Tensor (1) i32])
    $3 = alloca @tmp2
    @tmp2 = #@tmp1 (snode=S4place<f32>)[@tmp1[0]]
    $5 = alloca @tmp3
    @tmp3 = [@tmp0] (dt=[Tensor (1) i32])
    1d_ext_arr (element_dim=0, dt=f32, grad=false)[@tmp3[0]] = @tmp2
  }
}
[I 04/20/23 19:15:56.199 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Lowered:
kernel {
  $0 : struct for in S3dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <[Tensor (1) i32]> $2 = alloca
    <[Tensor (1) i32]> $3 = [$1]
    $4 : local store [$2 <- $3]
    <f32> $5 = alloca
    <i32> $6 = const 0
    <*i32> $7 = shift ptr [$2 + $6]
    <i32> $8 = local load [$7]
    <f32> $9 = global ptr [S4place<f32>], index [$8] activate=true
    <f32> $10 = global load $9
    $11 : local store [$5 <- $10]
    <[Tensor (1) i32]> $12 = alloca
    <[Tensor (1) i32]> $13 = [$1]
    $14 : local store [$12 <- $13]
    <f32> $15 = local load [$5]
    <i32> $16 = const 0
    <*i32> $17 = shift ptr [$12 + $16]
    <i32> $18 = local load [$17]
    <struct[none]{0(data_ptr, at 0B): *f32}> $19 = argaddr[0]
    <f32> $20 = external_ptr $19, [$18] element_dim=0 layout=AOS is_grad=false
    $21 : global store [$20 <- $15]
  }
}
[I 04/20/23 19:15:56.199 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Immutable local vars eliminated:
kernel {
  $0 : struct for in S3dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <[Tensor (1) i32]> $2 = alloca
    <[Tensor (1) i32]> $3 = [$1]
    $4 : local store [$2 <- $3]
    <i32> $5 = const 0
    <*i32> $6 = shift ptr [$2 + $5]
    <i32> $7 = local load [$6]
    <f32> $8 = global ptr [S4place<f32>], index [$7] activate=true
    <f32> $9 = global load $8
    <[Tensor (1) i32]> $10 = alloca
    <[Tensor (1) i32]> $11 = [$1]
    $12 : local store [$10 <- $11]
    <i32> $13 = const 0
    <*i32> $14 = shift ptr [$10 + $13]
    <i32> $15 = local load [$14]
    <struct[none]{0(data_ptr, at 0B): *f32}> $16 = argaddr[0]
    <f32> $17 = external_ptr $16, [$15] element_dim=0 layout=AOS is_grad=false
    $18 : global store [$17 <- $9]
  }
}
[I 04/20/23 19:15:56.199 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Typechecked:
kernel {
  $0 : struct for in S3dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <[Tensor (1) i32]> $2 = alloca
    <[Tensor (1) i32]> $3 = [$1]
    <[Tensor (1) i32]> $4 : local store [$2 <- $3]
    <i32> $5 = const 0
    <*i32> $6 = shift ptr [$2 + $5]
    <i32> $7 = local load [$6]
    <*f32> $8 = global ptr [S4place<f32>], index [$7] activate=true
    <f32> $9 = global load $8
    <[Tensor (1) i32]> $10 = alloca
    <[Tensor (1) i32]> $11 = [$1]
    <[Tensor (1) i32]> $12 : local store [$10 <- $11]
    <i32> $13 = const 0
    <*i32> $14 = shift ptr [$10 + $13]
    <i32> $15 = local load [$14]
    <*struct[none]{0(data_ptr, at 0B): *f32}> $16 = argaddr[0]
    <*f32> $17 = external_ptr $16, [$15] element_dim=0 layout=AOS is_grad=false
    $18 : global store [$17 <- $9]
  }
}
[I 04/20/23 19:15:56.199 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Bit Loop Vectorized:
kernel {
  $0 : struct for in S3dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <[Tensor (1) i32]> $2 = alloca
    <[Tensor (1) i32]> $3 = [$1]
    <[Tensor (1) i32]> $4 : local store [$2 <- $3]
    <i32> $5 = const 0
    <*i32> $6 = shift ptr [$2 + $5]
    <i32> $7 = local load [$6]
    <*f32> $8 = global ptr [S4place<f32>], index [$7] activate=true
    <f32> $9 = global load $8
    <[Tensor (1) i32]> $10 = alloca
    <[Tensor (1) i32]> $11 = [$1]
    <[Tensor (1) i32]> $12 : local store [$10 <- $11]
    <i32> $13 = const 0
    <*i32> $14 = shift ptr [$10 + $13]
    <i32> $15 = local load [$14]
    <*struct[none]{0(data_ptr, at 0B): *f32}> $16 = argaddr[0]
    <*f32> $17 = external_ptr $16, [$15] element_dim=0 layout=AOS is_grad=false
    $18 : global store [$17 <- $9]
  }
}
[I 04/20/23 19:15:56.199 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Matrix ptr lowered:
kernel {
  $0 : struct for in S3dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <[Tensor (1) i32]> $2 = alloca
    <[Tensor (1) i32]> $3 = [$1]
    <[Tensor (1) i32]> $4 : local store [$2 <- $3]
    <i32> $5 = const 0
    <*i32> $6 = shift ptr [$2 + $5]
    <i32> $7 = local load [$6]
    <*f32> $8 = global ptr [S4place<f32>], index [$7] activate=true
    <f32> $9 = global load $8
    <[Tensor (1) i32]> $10 = alloca
    <[Tensor (1) i32]> $11 = [$1]
    <[Tensor (1) i32]> $12 : local store [$10 <- $11]
    <i32> $13 = const 0
    <*i32> $14 = shift ptr [$10 + $13]
    <i32> $15 = local load [$14]
    <*struct[none]{0(data_ptr, at 0B): *f32}> $16 = argaddr[0]
    <*f32> $17 = external_ptr $16, [$15] element_dim=0 layout=AOS is_grad=false
    $18 : global store [$17 <- $9]
  }
}
[I 04/20/23 19:15:56.199 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Simplified I:
kernel {
  <i32> $0 = const 0
  $1 : struct for in S3dense noneblock_dim=adaptive {
    <i32> $2 = loop $1 index 0
    <[Tensor (1) i32]> $3 = alloca
    <[Tensor (1) i32]> $4 = [$2]
    <[Tensor (1) i32]> $5 : local store [$3 <- $4]
    <*i32> $6 = shift ptr [$3 + $0]
    <i32> $7 = local load [$6]
    <*f32> $8 = global ptr [S4place<f32>], index [$7] activate=true
    <f32> $9 = global load $8
    <[Tensor (1) i32]> $10 = alloca
    <[Tensor (1) i32]> $11 : local store [$10 <- $4]
    <*i32> $12 = shift ptr [$10 + $0]
    <i32> $13 = local load [$12]
    <*struct[none]{0(data_ptr, at 0B): *f32}> $14 = argaddr[0]
    <*f32> $15 = external_ptr $14, [$13] element_dim=0 layout=AOS is_grad=false
    $16 : global store [$15 <- $9]
  }
}
[I 04/20/23 19:15:56.199 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Scalarized:
kernel {
  $0 : struct for in S3dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = alloca
    $3 : local store [$2 <- $1]
    <i32> $4 = local load [$2]
    <*f32> $5 = global ptr [S4place<f32>], index [$4] activate=true
    <f32> $6 = global load $5
    <i32> $7 = alloca
    $8 : local store [$7 <- $1]
    <i32> $9 = local load [$7]
    <*struct[none]{0(data_ptr, at 0B): *f32}> $10 = argaddr[0]
    <*f32> $11 = external_ptr $10, [$9] element_dim=0 layout=AOS is_grad=false
    $12 : global store [$11 <- $6]
  }
}
[I 04/20/23 19:15:56.199 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Access flagged I:
kernel {
  $0 : struct for in S3dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <i32> $2 = alloca
    $3 : local store [$2 <- $1]
    <i32> $4 = local load [$2]
    <*f32> $5 = global ptr [S4place<f32>], index [$4] activate=false
    <f32> $6 = global load $5
    <i32> $7 = alloca
    $8 : local store [$7 <- $1]
    <i32> $9 = local load [$7]
    <*struct[none]{0(data_ptr, at 0B): *f32}> $10 = argaddr[0]
    <*f32> $11 = external_ptr $10, [$9] element_dim=0 layout=AOS is_grad=false
    $12 : global store [$11 <- $6]
  }
}
[I 04/20/23 19:15:56.199 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Simplified II:
kernel {
  $0 : struct for in S3dense noneblock_dim=adaptive {
    <i32> $1 = loop $0 index 0
    <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
    <f32> $3 = global load $2
    <*struct[none]{0(data_ptr, at 0B): *f32}> $4 = argaddr[0]
    <*f32> $5 = external_ptr $4, [$1] element_dim=0 layout=AOS is_grad=false
    $6 : global store [$5 <- $3]
  }
}
[I 04/20/23 19:15:56.199 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Offloaded:
kernel {
  $0 = offloaded struct_for(S3dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $1 = loop $0 index 0
    <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
    <f32> $3 = global load $2
    <*struct[none]{0(data_ptr, at 0B): *f32}> $4 = argaddr[0]
    <*f32> $5 = external_ptr $4, [$1] element_dim=0 layout=AOS is_grad=false
    $6 : global store [$5 <- $3]
  }
}
[I 04/20/23 19:15:56.200 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Optimized by CFG:
kernel {
  $0 = offloaded struct_for(S3dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $1 = loop $0 index 0
    <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
    <f32> $3 = global load $2
    <*struct[none]{0(data_ptr, at 0B): *f32}> $4 = argaddr[0]
    <*f32> $5 = external_ptr $4, [$1] element_dim=0 layout=AOS is_grad=false
    $6 : global store [$5 <- $3]
  }
}
[I 04/20/23 19:15:56.200 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Access flagged II:
kernel {
  $0 = offloaded struct_for(S3dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $1 = loop $0 index 0
    <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
    <f32> $3 = global load $2
    <*struct[none]{0(data_ptr, at 0B): *f32}> $4 = argaddr[0]
    <*f32> $5 = external_ptr $4, [$1] element_dim=0 layout=AOS is_grad=false
    $6 : global store [$5 <- $3]
  }
}
[I 04/20/23 19:15:56.200 1820159] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Simplified III:
kernel {
  $0 = offloaded struct_for(S3dense) grid_dim=2624 block_dim=128 bls=none
  body {
    <i32> $1 = loop $0 index 0
    <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
    <f32> $3 = global load $2
    <*struct[none]{0(data_ptr, at 0B): *f32}> $4 = argaddr[0]
    <*f32> $5 = external_ptr $4, [$1] element_dim=0 layout=AOS is_grad=false
    $6 : global store [$5 <- $3]
  }
}
[I 04/20/23 19:15:56.200 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Start offload_to_executable:
kernel {
$0 = offloaded struct_for(S3dense) grid_dim=2624 block_dim=128 bls=none
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = global load $2
  <*struct[none]{0(data_ptr, at 0B): *f32}> $4 = argaddr[0]
  <*f32> $5 = external_ptr $4, [$1] element_dim=0 layout=AOS is_grad=false
  $6 : global store [$5 <- $3]
}
}
[I 04/20/23 19:15:56.200 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Detect read-only accesses:
kernel {
$0 = offloaded struct_for(S3dense) grid_dim=2624 block_dim=128 bls=mem_access_opt [ S4place<f32>:read_only ]
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = global load $2
  <*struct[none]{0(data_ptr, at 0B): *f32}> $4 = argaddr[0]
  <*f32> $5 = external_ptr $4, [$1] element_dim=0 layout=AOS is_grad=false
  $6 : global store [$5 <- $3]
}
}
[I 04/20/23 19:15:56.200 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Atomics demoted I:
kernel {
$0 = offloaded struct_for(S3dense) grid_dim=2624 block_dim=128 bls=mem_access_opt [ S4place<f32>:read_only ]
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = global load $2
  <*struct[none]{0(data_ptr, at 0B): *f32}> $4 = argaddr[0]
  <*f32> $5 = external_ptr $4, [$1] element_dim=0 layout=AOS is_grad=false
  $6 : global store [$5 <- $3]
}
}
[I 04/20/23 19:15:56.200 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Cache loop-invariant global vars:
kernel {
$0 = offloaded struct_for(S3dense) grid_dim=2624 block_dim=128 bls=mem_access_opt [ S4place<f32>:read_only ]
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = global load $2
  <*struct[none]{0(data_ptr, at 0B): *f32}> $4 = argaddr[0]
  <*f32> $5 = external_ptr $4, [$1] element_dim=0 layout=AOS is_grad=false
  $6 : global store [$5 <- $3]
}
}
[I 04/20/23 19:15:56.200 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Dense struct-for demoted:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <i32> $4 = bit_shr $2 $3
  <i32> $5 = const 0
  <i32> $6 = bit_shr $4 $5
  <i32> $7 = const 1
  <i32> $8 = mul $6 $7
  <i32> $9 = add $1 $8
  <*f32> $10 = global ptr [S4place<f32>], index [$9] activate=false
  <f32> $11 = global load $10
  <*struct[none]{0(data_ptr, at 0B): *f32}> $12 = argaddr[0]
  <*f32> $13 = external_ptr $12, [$9] element_dim=0 layout=AOS is_grad=false
  $14 : global store [$13 <- $11]
}
}
[I 04/20/23 19:15:56.200 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] No-access mesh-for demoted:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <i32> $4 = bit_shr $2 $3
  <i32> $5 = const 0
  <i32> $6 = bit_shr $4 $5
  <i32> $7 = const 1
  <i32> $8 = mul $6 $7
  <i32> $9 = add $1 $8
  <*f32> $10 = global ptr [S4place<f32>], index [$9] activate=false
  <f32> $11 = global load $10
  <*struct[none]{0(data_ptr, at 0B): *f32}> $12 = argaddr[0]
  <*f32> $13 = external_ptr $12, [$9] element_dim=0 layout=AOS is_grad=false
  $14 : global store [$13 <- $11]
}
}
[I 04/20/23 19:15:56.200 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Make thread local:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <i32> $4 = bit_shr $2 $3
  <i32> $5 = const 0
  <i32> $6 = bit_shr $4 $5
  <i32> $7 = const 1
  <i32> $8 = mul $6 $7
  <i32> $9 = add $1 $8
  <*f32> $10 = global ptr [S4place<f32>], index [$9] activate=false
  <f32> $11 = global load $10
  <*struct[none]{0(data_ptr, at 0B): *f32}> $12 = argaddr[0]
  <*f32> $13 = external_ptr $12, [$9] element_dim=0 layout=AOS is_grad=false
  $14 : global store [$13 <- $11]
}
}
[I 04/20/23 19:15:56.200 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Make mesh thread local:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <i32> $4 = bit_shr $2 $3
  <i32> $5 = const 0
  <i32> $6 = bit_shr $4 $5
  <i32> $7 = const 1
  <i32> $8 = mul $6 $7
  <i32> $9 = add $1 $8
  <*f32> $10 = global ptr [S4place<f32>], index [$9] activate=false
  <f32> $11 = global load $10
  <*struct[none]{0(data_ptr, at 0B): *f32}> $12 = argaddr[0]
  <*f32> $13 = external_ptr $12, [$9] element_dim=0 layout=AOS is_grad=false
  $14 : global store [$13 <- $11]
}
}
[I 04/20/23 19:15:56.200 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Make mesh block local:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = const 0
  <i32> $2 = loop $0 index 0
  <i32> $3 = const 0
  <i32> $4 = bit_shr $2 $3
  <i32> $5 = const 0
  <i32> $6 = bit_shr $4 $5
  <i32> $7 = const 1
  <i32> $8 = mul $6 $7
  <i32> $9 = add $1 $8
  <*f32> $10 = global ptr [S4place<f32>], index [$9] activate=false
  <f32> $11 = global load $10
  <*struct[none]{0(data_ptr, at 0B): *f32}> $12 = argaddr[0]
  <*f32> $13 = external_ptr $12, [$9] element_dim=0 layout=AOS is_grad=false
  $14 : global store [$13 <- $11]
}
}
[I 04/20/23 19:15:56.200 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Simplified X:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = global load $2
  <*struct[none]{0(data_ptr, at 0B): *f32}> $4 = argaddr[0]
  <*f32> $5 = external_ptr $4, [$1] element_dim=0 layout=AOS is_grad=false
  $6 : global store [$5 <- $3]
}
}
[I 04/20/23 19:15:56.200 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Make block local:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = global load $2
  <*struct[none]{0(data_ptr, at 0B): *f32}> $4 = argaddr[0]
  <*f32> $5 = external_ptr $4, [$1] element_dim=0 layout=AOS is_grad=false
  $6 : global store [$5 <- $3]
}
}
[I 04/20/23 19:15:56.200 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Demote mesh statements:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = global load $2
  <*struct[none]{0(data_ptr, at 0B): *f32}> $4 = argaddr[0]
  <*f32> $5 = external_ptr $4, [$1] element_dim=0 layout=AOS is_grad=false
  $6 : global store [$5 <- $3]
}
}
[I 04/20/23 19:15:56.200 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Atomics demoted II:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = global load $2
  <*struct[none]{0(data_ptr, at 0B): *f32}> $4 = argaddr[0]
  <*f32> $5 = external_ptr $4, [$1] element_dim=0 layout=AOS is_grad=false
  $6 : global store [$5 <- $3]
}
}
[I 04/20/23 19:15:56.200 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Remove range assumption:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = global load $2
  <*struct[none]{0(data_ptr, at 0B): *f32}> $4 = argaddr[0]
  <*f32> $5 = external_ptr $4, [$1] element_dim=0 layout=AOS is_grad=false
  $6 : global store [$5 <- $3]
}
}
[I 04/20/23 19:15:56.200 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Remove loop_unique:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = global load $2
  <*struct[none]{0(data_ptr, at 0B): *f32}> $4 = argaddr[0]
  <*f32> $5 = external_ptr $4, [$1] element_dim=0 layout=AOS is_grad=false
  $6 : global store [$5 <- $3]
}
}
[I 04/20/23 19:15:56.201 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Simplified before lower access:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <f32> $3 = global load $2
  <*struct[none]{0(data_ptr, at 0B): *f32}> $4 = argaddr[0]
  <*f32> $5 = external_ptr $4, [$1] element_dim=0 layout=AOS is_grad=false
  $6 : global store [$5 <- $3]
}
}
[I 04/20/23 19:15:56.201 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Access lowered:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*f32> $2 = global ptr [S4place<f32>], index [$1] activate=false
  <*gen> $3 = get root [S0root][root]
  <i32> $4 = linearized(ind {}, stride {})
  <*gen> $5 = [S0root][root]::lookup($3, $4) activate = false
  <*gen> $6 = get child [S0root->S3dense] $5
  <i32> $7 = const 0
  <i32> $8 = bit_shr $1 $7
  <i32> $9 = linearized(ind {$8}, stride {8388608})
  <*gen> $10 = [S3dense][dense]::lookup($6, $9) activate = false
  <*f32> $11 = get child [S3dense->S4place<f32>] $10
  <f32> $12 = global load $11
  <*struct[none]{0(data_ptr, at 0B): *f32}> $13 = argaddr[0]
  <*f32> $14 = external_ptr $13, [$1] element_dim=0 layout=AOS is_grad=false
  $15 : global store [$14 <- $12]
}
}
[I 04/20/23 19:15:56.201 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] DIE:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*gen> $2 = get root [S0root][root]
  <i32> $3 = linearized(ind {}, stride {})
  <*gen> $4 = [S0root][root]::lookup($2, $3) activate = false
  <*gen> $5 = get child [S0root->S3dense] $4
  <i32> $6 = const 0
  <i32> $7 = bit_shr $1 $6
  <i32> $8 = linearized(ind {$7}, stride {8388608})
  <*gen> $9 = [S3dense][dense]::lookup($5, $8) activate = false
  <*f32> $10 = get child [S3dense->S4place<f32>] $9
  <f32> $11 = global load $10
  <*struct[none]{0(data_ptr, at 0B): *f32}> $12 = argaddr[0]
  <*f32> $13 = external_ptr $12, [$1] element_dim=0 layout=AOS is_grad=false
  $14 : global store [$13 <- $11]
}
}
[I 04/20/23 19:15:56.201 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Access flagged III:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*gen> $2 = get root [S0root][root]
  <i32> $3 = linearized(ind {}, stride {})
  <*gen> $4 = [S0root][root]::lookup($2, $3) activate = false
  <*gen> $5 = get child [S0root->S3dense] $4
  <i32> $6 = const 0
  <i32> $7 = bit_shr $1 $6
  <i32> $8 = linearized(ind {$7}, stride {8388608})
  <*gen> $9 = [S3dense][dense]::lookup($5, $8) activate = false
  <*f32> $10 = get child [S3dense->S4place<f32>] $9
  <f32> $11 = global load $10
  <*struct[none]{0(data_ptr, at 0B): *f32}> $12 = argaddr[0]
  <*f32> $13 = external_ptr $12, [$1] element_dim=0 layout=AOS is_grad=false
  $14 : global store [$13 <- $11]
}
}
[I 04/20/23 19:15:56.201 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Operations demoted:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*gen> $2 = get root [S0root][root]
  <i32> $3 = linearized(ind {}, stride {})
  <*gen> $4 = [S0root][root]::lookup($2, $3) activate = false
  <*gen> $5 = get child [S0root->S3dense] $4
  <i32> $6 = const 0
  <u32> $7 = reinterpret_cast_bits<u32> $1
  <u32> $8 = cast_value<u32> $6
  <u32> $9 = bit_sar $7 $8
  <i32> $10 = reinterpret_cast_bits<i32> $9
  <i32> $11 = linearized(ind {$10}, stride {8388608})
  <*gen> $12 = [S3dense][dense]::lookup($5, $11) activate = false
  <*f32> $13 = get child [S3dense->S4place<f32>] $12
  <f32> $14 = global load $13
  <*struct[none]{0(data_ptr, at 0B): *f32}> $15 = argaddr[0]
  <*f32> $16 = external_ptr $15, [$1] element_dim=0 layout=AOS is_grad=false
  $17 : global store [$16 <- $14]
}
}
[I 04/20/23 19:15:56.201 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Simplified IV:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*gen> $2 = get root [S0root][root]
  <i32> $3 = const 0
  <*gen> $4 = [S0root][root]::lookup($2, $3) activate = false
  <*gen> $5 = get child [S0root->S3dense] $4
  <*gen> $6 = [S3dense][dense]::lookup($5, $1) activate = false
  <*f32> $7 = get child [S3dense->S4place<f32>] $6
  <f32> $8 = global load $7
  <*struct[none]{0(data_ptr, at 0B): *f32}> $9 = argaddr[0]
  <*f32> $10 = external_ptr $9, [$1] element_dim=0 layout=AOS is_grad=false
  $11 : global store [$10 <- $8]
}
}
[I 04/20/23 19:15:56.201 1820178] [compile_to_offloads.cpp:operator()@23] [tensor_to_ext_arr_c6_1] Bit struct stores optimized:
kernel {
$0 = offloaded range_for(0, 8388608) grid_dim=2624 block_dim=128
body {
  <i32> $1 = loop $0 index 0
  <*gen> $2 = get root [S0root][root]
  <i32> $3 = const 0
  <*gen> $4 = [S0root][root]::lookup($2, $3) activate = false
  <*gen> $5 = get child [S0root->S3dense] $4
  <*gen> $6 = [S3dense][dense]::lookup($5, $1) activate = false
  <*f32> $7 = get child [S3dense->S4place<f32>] $6
  <f32> $8 = global load $7
  <*struct[none]{0(data_ptr, at 0B): *f32}> $9 = argaddr[0]
  <*f32> $10 = external_ptr $9, [$1] element_dim=0 layout=AOS is_grad=false
  $11 : global store [$10 <- $8]
}
}
with stack 8192.0
[I 04/20/23 19:15:56.258 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Initial IR:
kernel {
  $0 : for @tmp0 in range((cast_value<i32> 0), (cast_value<i32> 8192)) block_dim=256 {
    $1 = alloca @tmp1
    @tmp1 = (cast_value<i32> #@tmp3 (snode=S6place<i64>)[@tmp0, 0])
    $3 = alloca @tmp2
    @tmp2 = @tmp1
    $5 = alloca @tmp3
    @tmp3 = 0.0
    #@tmp4 (snode=S8place<f32>)[@tmp2, 0] = @tmp3
    $8 : for @tmp4 in range((cast_value<i32> 0), (cast_value<i32> 1)) block_dim=adaptive {
      $9 = alloca @tmp5
      @tmp5 = atomic_add(#@tmp4 (snode=S8place<f32>)[@tmp2, 0], #@tmp0 (snode=S2place<f32>)[@tmp4])
    }
  }
}
[I 04/20/23 19:15:56.258 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Lowered:
kernel {
  <i32> $0 = const 0
  <i32> $1 = cast_value<i32> $0
  <i32> $2 = const 8192
  <i32> $3 = cast_value<i32> $2
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <i32> $6 = alloca
    <i32> $7 = const 0
    <i64> $8 = global ptr [S6place<i64>], index [$5, $7] activate=true
    <i64> $9 = global load $8
    <i32> $10 = cast_value<i32> $9
    $11 : local store [$6 <- $10]
    <i32> $12 = alloca
    <i32> $13 = local load [$6]
    $14 : local store [$12 <- $13]
    <f32> $15 = alloca
    <f32> $16 = const 0.0
    $17 : local store [$15 <- $16]
    <f32> $18 = local load [$15]
    <i32> $19 = local load [$12]
    <i32> $20 = const 0
    <f32> $21 = global ptr [S8place<f32>], index [$19, $20] activate=true
    $22 : global store [$21 <- $18]
    <i32> $23 = const 0
    <i32> $24 = cast_value<i32> $23
    <i32> $25 = const 1
    <i32> $26 = cast_value<i32> $25
    $27 : for in range($24, $26) block_dim=adaptive {
      <i32> $28 = loop $27 index 0
      <f32> $29 = alloca
      <f32> $30 = global ptr [S2place<f32>], index [$28] activate=true
      <f32> $31 = global load $30
      <i32> $32 = local load [$12]
      <i32> $33 = const 0
      <f32> $34 = global ptr [S8place<f32>], index [$32, $33] activate=true
      <f32> $35 = atomic add($34, $31)
      $36 : local store [$29 <- $35]
    }
  }
}
[I 04/20/23 19:15:56.258 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Immutable local vars eliminated:
kernel {
  <i32> $0 = const 0
  <i32> $1 = cast_value<i32> $0
  <i32> $2 = const 8192
  <i32> $3 = cast_value<i32> $2
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <i32> $6 = const 0
    <i64> $7 = global ptr [S6place<i64>], index [$5, $6] activate=true
    <i64> $8 = global load $7
    <i32> $9 = cast_value<i32> $8
    <f32> $10 = const 0.0
    <i32> $11 = const 0
    <f32> $12 = global ptr [S8place<f32>], index [$9, $11] activate=true
    $13 : global store [$12 <- $10]
    <i32> $14 = const 0
    <i32> $15 = cast_value<i32> $14
    <i32> $16 = const 1
    <i32> $17 = cast_value<i32> $16
    $18 : for in range($15, $17) block_dim=adaptive {
      <i32> $19 = loop $18 index 0
      <f32> $20 = global ptr [S2place<f32>], index [$19] activate=true
      <f32> $21 = global load $20
      <i32> $22 = const 0
      <f32> $23 = global ptr [S8place<f32>], index [$9, $22] activate=true
      <f32> $24 = atomic add($23, $21)
    }
  }
}
[I 04/20/23 19:15:56.258 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Typechecked:
kernel {
  <i32> $0 = const 0
  <i32> $1 = cast_value<i32> $0
  <i32> $2 = const 8192
  <i32> $3 = cast_value<i32> $2
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <i32> $6 = const 0
    <*i64> $7 = global ptr [S6place<i64>], index [$5, $6] activate=true
    <i64> $8 = global load $7
    <i32> $9 = cast_value<i32> $8
    <f32> $10 = const 0.0
    <i32> $11 = const 0
    <*f32> $12 = global ptr [S8place<f32>], index [$9, $11] activate=true
    $13 : global store [$12 <- $10]
    <i32> $14 = const 0
    <i32> $15 = cast_value<i32> $14
    <i32> $16 = const 1
    <i32> $17 = cast_value<i32> $16
    $18 : for in range($15, $17) block_dim=adaptive {
      <i32> $19 = loop $18 index 0
      <*f32> $20 = global ptr [S2place<f32>], index [$19] activate=true
      <f32> $21 = global load $20
      <i32> $22 = const 0
      <*f32> $23 = global ptr [S8place<f32>], index [$9, $22] activate=true
      <f32> $24 = atomic add($23, $21)
    }
  }
}
[I 04/20/23 19:15:56.258 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Bit Loop Vectorized:
kernel {
  <i32> $0 = const 0
  <i32> $1 = cast_value<i32> $0
  <i32> $2 = const 8192
  <i32> $3 = cast_value<i32> $2
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <i32> $6 = const 0
    <*i64> $7 = global ptr [S6place<i64>], index [$5, $6] activate=true
    <i64> $8 = global load $7
    <i32> $9 = cast_value<i32> $8
    <f32> $10 = const 0.0
    <i32> $11 = const 0
    <*f32> $12 = global ptr [S8place<f32>], index [$9, $11] activate=true
    $13 : global store [$12 <- $10]
    <i32> $14 = const 0
    <i32> $15 = cast_value<i32> $14
    <i32> $16 = const 1
    <i32> $17 = cast_value<i32> $16
    $18 : for in range($15, $17) block_dim=adaptive {
      <i32> $19 = loop $18 index 0
      <*f32> $20 = global ptr [S2place<f32>], index [$19] activate=true
      <f32> $21 = global load $20
      <i32> $22 = const 0
      <*f32> $23 = global ptr [S8place<f32>], index [$9, $22] activate=true
      <f32> $24 = atomic add($23, $21)
    }
  }
}
[I 04/20/23 19:15:56.258 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Matrix ptr lowered:
kernel {
  <i32> $0 = const 0
  <i32> $1 = cast_value<i32> $0
  <i32> $2 = const 8192
  <i32> $3 = cast_value<i32> $2
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <i32> $6 = const 0
    <*i64> $7 = global ptr [S6place<i64>], index [$5, $6] activate=true
    <i64> $8 = global load $7
    <i32> $9 = cast_value<i32> $8
    <f32> $10 = const 0.0
    <i32> $11 = const 0
    <*f32> $12 = global ptr [S8place<f32>], index [$9, $11] activate=true
    $13 : global store [$12 <- $10]
    <i32> $14 = const 0
    <i32> $15 = cast_value<i32> $14
    <i32> $16 = const 1
    <i32> $17 = cast_value<i32> $16
    $18 : for in range($15, $17) block_dim=adaptive {
      <i32> $19 = loop $18 index 0
      <*f32> $20 = global ptr [S2place<f32>], index [$19] activate=true
      <f32> $21 = global load $20
      <i32> $22 = const 0
      <*f32> $23 = global ptr [S8place<f32>], index [$9, $22] activate=true
      <f32> $24 = atomic add($23, $21)
    }
  }
}
[I 04/20/23 19:15:56.259 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Simplified I:
kernel {
  <i32> $0 = const 0
  <i32> $1 = const 1
  <f32> $2 = const 0.0
  <i32> $3 = const 8192
  $4 : for in range($0, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <*i64> $6 = global ptr [S6place<i64>], index [$5, $0] activate=true
    <i64> $7 = global load $6
    <i32> $8 = cast_value<i32> $7
    <*f32> $9 = global ptr [S8place<f32>], index [$8, $0] activate=true
    $10 : global store [$9 <- $2]
    $11 : for in range($0, $1) block_dim=adaptive {
      <i32> $12 = loop $11 index 0
      <*f32> $13 = global ptr [S2place<f32>], index [$12] activate=true
      <f32> $14 = global load $13
      <f32> $15 = atomic add($9, $14)
    }
  }
}
[I 04/20/23 19:15:56.259 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Scalarized:
kernel {
  <i32> $0 = const 0
  <i32> $1 = const 1
  <f32> $2 = const 0.0
  <i32> $3 = const 8192
  $4 : for in range($0, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <*i64> $6 = global ptr [S6place<i64>], index [$5, $0] activate=true
    <i64> $7 = global load $6
    <i32> $8 = cast_value<i32> $7
    <*f32> $9 = global ptr [S8place<f32>], index [$8, $0] activate=true
    $10 : global store [$9 <- $2]
    $11 : for in range($0, $1) block_dim=adaptive {
      <i32> $12 = loop $11 index 0
      <*f32> $13 = global ptr [S2place<f32>], index [$12] activate=true
      <f32> $14 = global load $13
      <f32> $15 = atomic add($9, $14)
    }
  }
}
[I 04/20/23 19:15:56.259 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Access flagged I:
kernel {
  <i32> $0 = const 0
  <i32> $1 = const 1
  <f32> $2 = const 0.0
  <i32> $3 = const 8192
  $4 : for in range($0, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <*i64> $6 = global ptr [S6place<i64>], index [$5, $0] activate=false
    <i64> $7 = global load $6
    <i32> $8 = cast_value<i32> $7
    <*f32> $9 = global ptr [S8place<f32>], index [$8, $0] activate=true
    $10 : global store [$9 <- $2]
    $11 : for in range($0, $1) block_dim=adaptive {
      <i32> $12 = loop $11 index 0
      <*f32> $13 = global ptr [S2place<f32>], index [$12] activate=false
      <f32> $14 = global load $13
      <f32> $15 = atomic add($9, $14)
    }
  }
}
[I 04/20/23 19:15:56.259 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Simplified II:
kernel {
  <i32> $0 = const 0
  <i32> $1 = const 1
  <f32> $2 = const 0.0
  <i32> $3 = const 8192
  $4 : for in range($0, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <*i64> $6 = global ptr [S6place<i64>], index [$5, $0] activate=false
    <i64> $7 = global load $6
    <i32> $8 = cast_value<i32> $7
    <*f32> $9 = global ptr [S8place<f32>], index [$8, $0] activate=true
    $10 : global store [$9 <- $2]
    $11 : for in range($0, $1) block_dim=adaptive {
      <i32> $12 = loop $11 index 0
      <*f32> $13 = global ptr [S2place<f32>], index [$12] activate=false
      <f32> $14 = global load $13
      <f32> $15 = atomic add($9, $14)
    }
  }
}
[I 04/20/23 19:15:56.259 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Offloaded:
kernel {
  $0 = offloaded
  body {
    <i32> $1 = const 0
    <i32> $2 = const 1
    <f32> $3 = const 0.0
    <i32> $4 = const 8192
  }
  $5 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
  body {
    <i32> $6 = loop $5 index 0
    <i32> $7 = const 0
    <*i64> $8 = global ptr [S6place<i64>], index [$6, $7] activate=false
    <i64> $9 = global load $8
    <i32> $10 = cast_value<i32> $9
    <i32> $11 = const 0
    <*f32> $12 = global ptr [S8place<f32>], index [$10, $11] activate=true
    <f32> $13 = const 0.0
    $14 : global store [$12 <- $13]
    <i32> $15 = const 0
    <i32> $16 = const 1
    $17 : for in range($15, $16) block_dim=adaptive {
      <i32> $18 = loop $17 index 0
      <*f32> $19 = global ptr [S2place<f32>], index [$18] activate=false
      <f32> $20 = global load $19
      <f32> $21 = atomic add($12, $20)
    }
  }
}
[I 04/20/23 19:15:56.259 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Optimized by CFG:
kernel {
  $0 = offloaded
  body {
  }
  $1 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
  body {
    <i32> $2 = loop $1 index 0
    <i32> $3 = const 0
    <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
    <i64> $5 = global load $4
    <i32> $6 = cast_value<i32> $5
    <i32> $7 = const 0
    <*f32> $8 = global ptr [S8place<f32>], index [$6, $7] activate=true
    <f32> $9 = const 0.0
    $10 : global store [$8 <- $9]
    <i32> $11 = const 0
    <i32> $12 = const 1
    $13 : for in range($11, $12) block_dim=adaptive {
      <i32> $14 = loop $13 index 0
      <*f32> $15 = global ptr [S2place<f32>], index [$14] activate=false
      <f32> $16 = global load $15
      <f32> $17 = atomic add($8, $16)
    }
  }
}
[I 04/20/23 19:15:56.259 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Access flagged II:
kernel {
  $0 = offloaded
  body {
  }
  $1 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
  body {
    <i32> $2 = loop $1 index 0
    <i32> $3 = const 0
    <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
    <i64> $5 = global load $4
    <i32> $6 = cast_value<i32> $5
    <i32> $7 = const 0
    <*f32> $8 = global ptr [S8place<f32>], index [$6, $7] activate=true
    <f32> $9 = const 0.0
    $10 : global store [$8 <- $9]
    <i32> $11 = const 0
    <i32> $12 = const 1
    $13 : for in range($11, $12) block_dim=adaptive {
      <i32> $14 = loop $13 index 0
      <*f32> $15 = global ptr [S2place<f32>], index [$14] activate=false
      <f32> $16 = global load $15
      <f32> $17 = atomic add($8, $16)
    }
  }
}
[I 04/20/23 19:15:56.259 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Simplified III:
kernel {
  $0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
  body {
    <i32> $1 = loop $0 index 0
    <i32> $2 = const 0
    <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
    <i64> $4 = global load $3
    <i32> $5 = cast_value<i32> $4
    <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
    <f32> $7 = const 0.0
    $8 : global store [$6 <- $7]
    <i32> $9 = const 1
    $10 : for in range($2, $9) block_dim=adaptive {
      <i32> $11 = loop $10 index 0
      <*f32> $12 = global ptr [S2place<f32>], index [$11] activate=false
      <f32> $13 = global load $12
      <f32> $14 = atomic add($6, $13)
    }
  }
}
[I 04/20/23 19:15:56.259 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Start offload_to_executable:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <i32> $9 = const 1
  $10 : for in range($2, $9) block_dim=adaptive {
    <i32> $11 = loop $10 index 0
    <*f32> $12 = global ptr [S2place<f32>], index [$11] activate=false
    <f32> $13 = global load $12
    <f32> $14 = atomic add($6, $13)
  }
}
}
[I 04/20/23 19:15:56.260 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Detect read-only accesses:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <i32> $9 = const 1
  $10 : for in range($2, $9) block_dim=adaptive {
    <i32> $11 = loop $10 index 0
    <*f32> $12 = global ptr [S2place<f32>], index [$11] activate=false
    <f32> $13 = global load $12
    <f32> $14 = atomic add($6, $13)
  }
}
}
[I 04/20/23 19:15:56.260 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Atomics demoted I:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <i32> $9 = const 1
  $10 : for in range($2, $9) block_dim=adaptive {
    <i32> $11 = loop $10 index 0
    <*f32> $12 = global ptr [S2place<f32>], index [$11] activate=false
    <f32> $13 = global load $12
    <f32> $14 = atomic add($6, $13)
  }
}
}
[I 04/20/23 19:15:56.260 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Cache loop-invariant global vars:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <i32> $9 = const 1
  $10 : for in range($2, $9) block_dim=adaptive {
    <i32> $11 = loop $10 index 0
    <*f32> $12 = global ptr [S2place<f32>], index [$11] activate=false
    <f32> $13 = global load $12
    <f32> $14 = atomic add($6, $13)
  }
}
}
[I 04/20/23 19:15:56.260 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Dense struct-for demoted:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <i32> $9 = const 1
  $10 : for in range($2, $9) block_dim=adaptive {
    <i32> $11 = loop $10 index 0
    <*f32> $12 = global ptr [S2place<f32>], index [$11] activate=false
    <f32> $13 = global load $12
    <f32> $14 = atomic add($6, $13)
  }
}
}
[I 04/20/23 19:15:56.260 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] No-access mesh-for demoted:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <i32> $9 = const 1
  $10 : for in range($2, $9) block_dim=adaptive {
    <i32> $11 = loop $10 index 0
    <*f32> $12 = global ptr [S2place<f32>], index [$11] activate=false
    <f32> $13 = global load $12
    <f32> $14 = atomic add($6, $13)
  }
}
}
[I 04/20/23 19:15:56.260 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Make thread local:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <i32> $9 = const 1
  $10 : for in range($2, $9) block_dim=adaptive {
    <i32> $11 = loop $10 index 0
    <*f32> $12 = global ptr [S2place<f32>], index [$11] activate=false
    <f32> $13 = global load $12
    <f32> $14 = atomic add($6, $13)
  }
}
}
[I 04/20/23 19:15:56.260 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Make mesh thread local:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <i32> $9 = const 1
  $10 : for in range($2, $9) block_dim=adaptive {
    <i32> $11 = loop $10 index 0
    <*f32> $12 = global ptr [S2place<f32>], index [$11] activate=false
    <f32> $13 = global load $12
    <f32> $14 = atomic add($6, $13)
  }
}
}
[I 04/20/23 19:15:56.260 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Make mesh block local:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <i32> $9 = const 1
  $10 : for in range($2, $9) block_dim=adaptive {
    <i32> $11 = loop $10 index 0
    <*f32> $12 = global ptr [S2place<f32>], index [$11] activate=false
    <f32> $13 = global load $12
    <f32> $14 = atomic add($6, $13)
  }
}
}
[I 04/20/23 19:15:56.260 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Simplified X:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <i32> $9 = const 1
  $10 : for in range($2, $9) block_dim=adaptive {
    <i32> $11 = loop $10 index 0
    <*f32> $12 = global ptr [S2place<f32>], index [$11] activate=false
    <f32> $13 = global load $12
    <f32> $14 = atomic add($6, $13)
  }
}
}
[I 04/20/23 19:15:56.260 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Make block local:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <i32> $9 = const 1
  $10 : for in range($2, $9) block_dim=adaptive {
    <i32> $11 = loop $10 index 0
    <*f32> $12 = global ptr [S2place<f32>], index [$11] activate=false
    <f32> $13 = global load $12
    <f32> $14 = atomic add($6, $13)
  }
}
}
[I 04/20/23 19:15:56.260 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Demote mesh statements:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <i32> $9 = const 1
  $10 : for in range($2, $9) block_dim=adaptive {
    <i32> $11 = loop $10 index 0
    <*f32> $12 = global ptr [S2place<f32>], index [$11] activate=false
    <f32> $13 = global load $12
    <f32> $14 = atomic add($6, $13)
  }
}
}
[I 04/20/23 19:15:56.260 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Atomics demoted II:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <i32> $9 = const 1
  $10 : for in range($2, $9) block_dim=adaptive {
    <i32> $11 = loop $10 index 0
    <*f32> $12 = global ptr [S2place<f32>], index [$11] activate=false
    <f32> $13 = global load $12
    <f32> $14 = atomic add($6, $13)
  }
}
}
[I 04/20/23 19:15:56.260 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Remove range assumption:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <i32> $9 = const 1
  $10 : for in range($2, $9) block_dim=adaptive {
    <i32> $11 = loop $10 index 0
    <*f32> $12 = global ptr [S2place<f32>], index [$11] activate=false
    <f32> $13 = global load $12
    <f32> $14 = atomic add($6, $13)
  }
}
}
[I 04/20/23 19:15:56.260 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Remove loop_unique:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <i32> $9 = const 1
  $10 : for in range($2, $9) block_dim=adaptive {
    <i32> $11 = loop $10 index 0
    <*f32> $12 = global ptr [S2place<f32>], index [$11] activate=false
    <f32> $13 = global load $12
    <f32> $14 = atomic add($6, $13)
  }
}
}
[I 04/20/23 19:15:56.261 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Simplified before lower access:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <i32> $9 = const 1
  $10 : for in range($2, $9) block_dim=adaptive {
    <i32> $11 = loop $10 index 0
    <*f32> $12 = global ptr [S2place<f32>], index [$11] activate=false
    <f32> $13 = global load $12
    <f32> $14 = atomic add($6, $13)
  }
}
}
[I 04/20/23 19:15:56.261 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Access lowered:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <*gen> $4 = get root [S0root][root]
  <i32> $5 = linearized(ind {}, stride {})
  <*gen> $6 = [S0root][root]::lookup($4, $5) activate = false
  <*gen> $7 = get child [S0root->S5dense] $6
  <i32> $8 = const 0
  <i32> $9 = bit_shr $1 $8
  <i32> $10 = const 0
  <i32> $11 = bit_shr $2 $10
  <i32> $12 = linearized(ind {$9, $11}, stride {8192, 3})
  <*gen> $13 = [S5dense][dense]::lookup($7, $12) activate = false
  <*i64> $14 = get child [S5dense->S6place<i64>] $13
  <i64> $15 = global load $14
  <i32> $16 = cast_value<i32> $15
  <*f32> $17 = global ptr [S8place<f32>], index [$16, $2] activate=true
  <f32> $18 = const 0.0
  <*gen> $19 = get root [S0root][root]
  <i32> $20 = linearized(ind {}, stride {})
  <*gen> $21 = [S0root][root]::lookup($19, $20) activate = false
  <*gen> $22 = get child [S0root->S7dense] $21
  <i32> $23 = const 0
  <i32> $24 = bit_shr $16 $23
  <i32> $25 = const 0
  <i32> $26 = bit_shr $2 $25
  <i32> $27 = linearized(ind {$24, $26}, stride {8192, 3})
  <*gen> $28 = [S7dense][dense]::lookup($22, $27) activate = false
  <*f32> $29 = get child [S7dense->S8place<f32>] $28
  $30 : global store [$29 <- $18]
  <i32> $31 = const 1
  $32 : for in range($2, $31) block_dim=adaptive {
    <i32> $33 = loop $32 index 0
    <*f32> $34 = global ptr [S2place<f32>], index [$33] activate=false
    <*gen> $35 = get root [S0root][root]
    <i32> $36 = linearized(ind {}, stride {})
    <*gen> $37 = [S0root][root]::lookup($35, $36) activate = false
    <*gen> $38 = get child [S0root->S1dense] $37
    <i32> $39 = const 0
    <i32> $40 = bit_shr $33 $39
    <i32> $41 = linearized(ind {$40}, stride {8388608})
    <*gen> $42 = [S1dense][dense]::lookup($38, $41) activate = false
    <*f32> $43 = get child [S1dense->S2place<f32>] $42
    <f32> $44 = global load $43
    <*gen> $45 = get root [S0root][root]
    <i32> $46 = linearized(ind {}, stride {})
    <*gen> $47 = [S0root][root]::lookup($45, $46) activate = false
    <*gen> $48 = get child [S0root->S7dense] $47
    <i32> $49 = const 0
    <i32> $50 = bit_shr $16 $49
    <i32> $51 = const 0
    <i32> $52 = bit_shr $2 $51
    <i32> $53 = linearized(ind {$50, $52}, stride {8192, 3})
    <*gen> $54 = [S7dense][dense]::lookup($48, $53) activate = false
    <*f32> $55 = get child [S7dense->S8place<f32>] $54
    <f32> $56 = atomic add($55, $44)
  }
}
}
[I 04/20/23 19:15:56.261 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] DIE:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*gen> $3 = get root [S0root][root]
  <i32> $4 = linearized(ind {}, stride {})
  <*gen> $5 = [S0root][root]::lookup($3, $4) activate = false
  <*gen> $6 = get child [S0root->S5dense] $5
  <i32> $7 = const 0
  <i32> $8 = bit_shr $1 $7
  <i32> $9 = const 0
  <i32> $10 = bit_shr $2 $9
  <i32> $11 = linearized(ind {$8, $10}, stride {8192, 3})
  <*gen> $12 = [S5dense][dense]::lookup($6, $11) activate = false
  <*i64> $13 = get child [S5dense->S6place<i64>] $12
  <i64> $14 = global load $13
  <i32> $15 = cast_value<i32> $14
  <*f32> $16 = global ptr [S8place<f32>], index [$15, $2] activate=true
  <f32> $17 = const 0.0
  <*gen> $18 = get root [S0root][root]
  <i32> $19 = linearized(ind {}, stride {})
  <*gen> $20 = [S0root][root]::lookup($18, $19) activate = false
  <*gen> $21 = get child [S0root->S7dense] $20
  <i32> $22 = const 0
  <i32> $23 = bit_shr $15 $22
  <i32> $24 = const 0
  <i32> $25 = bit_shr $2 $24
  <i32> $26 = linearized(ind {$23, $25}, stride {8192, 3})
  <*gen> $27 = [S7dense][dense]::lookup($21, $26) activate = false
  <*f32> $28 = get child [S7dense->S8place<f32>] $27
  $29 : global store [$28 <- $17]
  <i32> $30 = const 1
  $31 : for in range($2, $30) block_dim=adaptive {
    <i32> $32 = loop $31 index 0
    <*gen> $33 = get root [S0root][root]
    <i32> $34 = linearized(ind {}, stride {})
    <*gen> $35 = [S0root][root]::lookup($33, $34) activate = false
    <*gen> $36 = get child [S0root->S1dense] $35
    <i32> $37 = const 0
    <i32> $38 = bit_shr $32 $37
    <i32> $39 = linearized(ind {$38}, stride {8388608})
    <*gen> $40 = [S1dense][dense]::lookup($36, $39) activate = false
    <*f32> $41 = get child [S1dense->S2place<f32>] $40
    <f32> $42 = global load $41
    <*gen> $43 = get root [S0root][root]
    <i32> $44 = linearized(ind {}, stride {})
    <*gen> $45 = [S0root][root]::lookup($43, $44) activate = false
    <*gen> $46 = get child [S0root->S7dense] $45
    <i32> $47 = const 0
    <i32> $48 = bit_shr $15 $47
    <i32> $49 = const 0
    <i32> $50 = bit_shr $2 $49
    <i32> $51 = linearized(ind {$48, $50}, stride {8192, 3})
    <*gen> $52 = [S7dense][dense]::lookup($46, $51) activate = false
    <*f32> $53 = get child [S7dense->S8place<f32>] $52
    <f32> $54 = atomic add($53, $42)
  }
}
}
[I 04/20/23 19:15:56.261 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Access flagged III:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*gen> $3 = get root [S0root][root]
  <i32> $4 = linearized(ind {}, stride {})
  <*gen> $5 = [S0root][root]::lookup($3, $4) activate = false
  <*gen> $6 = get child [S0root->S5dense] $5
  <i32> $7 = const 0
  <i32> $8 = bit_shr $1 $7
  <i32> $9 = const 0
  <i32> $10 = bit_shr $2 $9
  <i32> $11 = linearized(ind {$8, $10}, stride {8192, 3})
  <*gen> $12 = [S5dense][dense]::lookup($6, $11) activate = false
  <*i64> $13 = get child [S5dense->S6place<i64>] $12
  <i64> $14 = global load $13
  <i32> $15 = cast_value<i32> $14
  <*f32> $16 = global ptr [S8place<f32>], index [$15, $2] activate=false
  <f32> $17 = const 0.0
  <*gen> $18 = get root [S0root][root]
  <i32> $19 = linearized(ind {}, stride {})
  <*gen> $20 = [S0root][root]::lookup($18, $19) activate = false
  <*gen> $21 = get child [S0root->S7dense] $20
  <i32> $22 = const 0
  <i32> $23 = bit_shr $15 $22
  <i32> $24 = const 0
  <i32> $25 = bit_shr $2 $24
  <i32> $26 = linearized(ind {$23, $25}, stride {8192, 3})
  <*gen> $27 = [S7dense][dense]::lookup($21, $26) activate = false
  <*f32> $28 = get child [S7dense->S8place<f32>] $27
  $29 : global store [$28 <- $17]
  <i32> $30 = const 1
  $31 : for in range($2, $30) block_dim=adaptive {
    <i32> $32 = loop $31 index 0
    <*gen> $33 = get root [S0root][root]
    <i32> $34 = linearized(ind {}, stride {})
    <*gen> $35 = [S0root][root]::lookup($33, $34) activate = false
    <*gen> $36 = get child [S0root->S1dense] $35
    <i32> $37 = const 0
    <i32> $38 = bit_shr $32 $37
    <i32> $39 = linearized(ind {$38}, stride {8388608})
    <*gen> $40 = [S1dense][dense]::lookup($36, $39) activate = false
    <*f32> $41 = get child [S1dense->S2place<f32>] $40
    <f32> $42 = global load $41
    <*gen> $43 = get root [S0root][root]
    <i32> $44 = linearized(ind {}, stride {})
    <*gen> $45 = [S0root][root]::lookup($43, $44) activate = false
    <*gen> $46 = get child [S0root->S7dense] $45
    <i32> $47 = const 0
    <i32> $48 = bit_shr $15 $47
    <i32> $49 = const 0
    <i32> $50 = bit_shr $2 $49
    <i32> $51 = linearized(ind {$48, $50}, stride {8192, 3})
    <*gen> $52 = [S7dense][dense]::lookup($46, $51) activate = false
    <*f32> $53 = get child [S7dense->S8place<f32>] $52
    <f32> $54 = atomic add($53, $42)
  }
}
}
[I 04/20/23 19:15:56.261 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Operations demoted:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*gen> $3 = get root [S0root][root]
  <i32> $4 = linearized(ind {}, stride {})
  <*gen> $5 = [S0root][root]::lookup($3, $4) activate = false
  <*gen> $6 = get child [S0root->S5dense] $5
  <i32> $7 = const 0
  <u32> $8 = reinterpret_cast_bits<u32> $1
  <u32> $9 = cast_value<u32> $7
  <u32> $10 = bit_sar $8 $9
  <i32> $11 = reinterpret_cast_bits<i32> $10
  <i32> $12 = const 0
  <u32> $13 = reinterpret_cast_bits<u32> $2
  <u32> $14 = cast_value<u32> $12
  <u32> $15 = bit_sar $13 $14
  <i32> $16 = reinterpret_cast_bits<i32> $15
  <i32> $17 = linearized(ind {$11, $16}, stride {8192, 3})
  <*gen> $18 = [S5dense][dense]::lookup($6, $17) activate = false
  <*i64> $19 = get child [S5dense->S6place<i64>] $18
  <i64> $20 = global load $19
  <i32> $21 = cast_value<i32> $20
  <*f32> $22 = global ptr [S8place<f32>], index [$21, $2] activate=false
  <f32> $23 = const 0.0
  <*gen> $24 = get root [S0root][root]
  <i32> $25 = linearized(ind {}, stride {})
  <*gen> $26 = [S0root][root]::lookup($24, $25) activate = false
  <*gen> $27 = get child [S0root->S7dense] $26
  <i32> $28 = const 0
  <u32> $29 = reinterpret_cast_bits<u32> $21
  <u32> $30 = cast_value<u32> $28
  <u32> $31 = bit_sar $29 $30
  <i32> $32 = reinterpret_cast_bits<i32> $31
  <i32> $33 = const 0
  <u32> $34 = reinterpret_cast_bits<u32> $2
  <u32> $35 = cast_value<u32> $33
  <u32> $36 = bit_sar $34 $35
  <i32> $37 = reinterpret_cast_bits<i32> $36
  <i32> $38 = linearized(ind {$32, $37}, stride {8192, 3})
  <*gen> $39 = [S7dense][dense]::lookup($27, $38) activate = false
  <*f32> $40 = get child [S7dense->S8place<f32>] $39
  $41 : global store [$40 <- $23]
  <i32> $42 = const 1
  $43 : for in range($2, $42) block_dim=adaptive {
    <i32> $44 = loop $43 index 0
    <*gen> $45 = get root [S0root][root]
    <i32> $46 = linearized(ind {}, stride {})
    <*gen> $47 = [S0root][root]::lookup($45, $46) activate = false
    <*gen> $48 = get child [S0root->S1dense] $47
    <i32> $49 = const 0
    <u32> $50 = reinterpret_cast_bits<u32> $44
    <u32> $51 = cast_value<u32> $49
    <u32> $52 = bit_sar $50 $51
    <i32> $53 = reinterpret_cast_bits<i32> $52
    <i32> $54 = linearized(ind {$53}, stride {8388608})
    <*gen> $55 = [S1dense][dense]::lookup($48, $54) activate = false
    <*f32> $56 = get child [S1dense->S2place<f32>] $55
    <f32> $57 = global load $56
    <*gen> $58 = get root [S0root][root]
    <i32> $59 = linearized(ind {}, stride {})
    <*gen> $60 = [S0root][root]::lookup($58, $59) activate = false
    <*gen> $61 = get child [S0root->S7dense] $60
    <i32> $62 = const 0
    <u32> $63 = reinterpret_cast_bits<u32> $21
    <u32> $64 = cast_value<u32> $62
    <u32> $65 = bit_sar $63 $64
    <i32> $66 = reinterpret_cast_bits<i32> $65
    <i32> $67 = const 0
    <u32> $68 = reinterpret_cast_bits<u32> $2
    <u32> $69 = cast_value<u32> $67
    <u32> $70 = bit_sar $68 $69
    <i32> $71 = reinterpret_cast_bits<i32> $70
    <i32> $72 = linearized(ind {$66, $71}, stride {8192, 3})
    <*gen> $73 = [S7dense][dense]::lookup($61, $72) activate = false
    <*f32> $74 = get child [S7dense->S8place<f32>] $73
    <f32> $75 = atomic add($74, $57)
  }
}
}
[I 04/20/23 19:15:56.262 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Simplified IV:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*gen> $3 = get root [S0root][root]
  <*gen> $4 = [S0root][root]::lookup($3, $2) activate = false
  <*gen> $5 = get child [S0root->S5dense] $4
  <i32> $6 = const 1
  <i32> $7 = const 3
  <i32> $8 = mul $1 $7
  <*gen> $9 = [S5dense][dense]::lookup($5, $8) activate = false
  <*i64> $10 = get child [S5dense->S6place<i64>] $9
  <i64> $11 = global load $10
  <i32> $12 = cast_value<i32> $11
  <f32> $13 = const 0.0
  <*gen> $14 = get child [S0root->S7dense] $4
  <i32> $15 = mul $12 $7
  <*gen> $16 = [S7dense][dense]::lookup($14, $15) activate = false
  <*f32> $17 = get child [S7dense->S8place<f32>] $16
  $18 : global store [$17 <- $13]
  $19 : for in range($2, $6) block_dim=adaptive {
    <i32> $20 = loop $19 index 0
    <*gen> $21 = get child [S0root->S1dense] $4
    <*gen> $22 = [S1dense][dense]::lookup($21, $20) activate = false
    <*f32> $23 = get child [S1dense->S2place<f32>] $22
    <f32> $24 = global load $23
    <f32> $25 = atomic add($17, $24)
  }
}
}
[I 04/20/23 19:15:56.262 1820180] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c76_0] Bit struct stores optimized:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*gen> $3 = get root [S0root][root]
  <*gen> $4 = [S0root][root]::lookup($3, $2) activate = false
  <*gen> $5 = get child [S0root->S5dense] $4
  <i32> $6 = const 1
  <i32> $7 = const 3
  <i32> $8 = mul $1 $7
  <*gen> $9 = [S5dense][dense]::lookup($5, $8) activate = false
  <*i64> $10 = get child [S5dense->S6place<i64>] $9
  <i64> $11 = global load $10
  <i32> $12 = cast_value<i32> $11
  <f32> $13 = const 0.0
  <*gen> $14 = get child [S0root->S7dense] $4
  <i32> $15 = mul $12 $7
  <*gen> $16 = [S7dense][dense]::lookup($14, $15) activate = false
  <*f32> $17 = get child [S7dense->S8place<f32>] $16
  $18 : global store [$17 <- $13]
  $19 : for in range($2, $6) block_dim=adaptive {
    <i32> $20 = loop $19 index 0
    <*gen> $21 = get child [S0root->S1dense] $4
    <*gen> $22 = [S1dense][dense]::lookup($21, $20) activate = false
    <*f32> $23 = get child [S1dense->S2place<f32>] $22
    <f32> $24 = global load $23
    <f32> $25 = atomic add($17, $24)
  }
}
}
50674.547
[I 04/20/23 19:15:56.302 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Initial IR:
kernel {
  $0 : for @tmp0 in range((cast_value<i32> 0), (cast_value<i32> 8192)) block_dim=256 {
    $1 = alloca @tmp1
    @tmp1 = (cast_value<i32> #@tmp3 (snode=S6place<i64>)[@tmp0, 0])
    $3 = alloca @tmp2
    @tmp2 = @tmp1
    $5 = alloca @tmp3
    @tmp3 = 0.0
    #@tmp4 (snode=S8place<f32>)[@tmp2, 0] = @tmp3
    $8 : for @tmp4 in range((cast_value<i32> 0), (cast_value<i32> 1)) block_dim=adaptive {
      $9 = alloca @tmp5
      @tmp5 = atomic_add(#@tmp4 (snode=S8place<f32>)[@tmp2, 0], #@tmp0 (snode=S2place<f32>)[@tmp4])
    }
  }
}
[I 04/20/23 19:15:56.303 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Segment reversed (for autodiff):
kernel {
  $0 : for @tmp0 in range((cast_value<i32> 0), (cast_value<i32> 8192)) block_dim=256 {
    $1 = alloca @tmp1
    @tmp1 = (cast_value<i32> #@tmp3 (snode=S6place<i64>)[@tmp0, 0])
    $3 = alloca @tmp2
    @tmp2 = @tmp1
    $5 = alloca @tmp3
    @tmp3 = 0.0
    #@tmp4 (snode=S8place<f32>)[@tmp2, 0] = @tmp3
    $8 : for @tmp4 in range((cast_value<i32> 0), (cast_value<i32> 1)) block_dim=adaptive {
      $9 = alloca @tmp5
      @tmp5 = atomic_add(#@tmp4 (snode=S8place<f32>)[@tmp2, 0], #@tmp0 (snode=S2place<f32>)[@tmp4])
    }
  }
}
[I 04/20/23 19:15:56.303 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Lowered:
kernel {
  <i32> $0 = const 0
  <i32> $1 = cast_value<i32> $0
  <i32> $2 = const 8192
  <i32> $3 = cast_value<i32> $2
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <i32> $6 = alloca
    <i32> $7 = const 0
    <i64> $8 = global ptr [S6place<i64>], index [$5, $7] activate=true
    <i64> $9 = global load $8
    <i32> $10 = cast_value<i32> $9
    $11 : local store [$6 <- $10]
    <i32> $12 = alloca
    <i32> $13 = local load [$6]
    $14 : local store [$12 <- $13]
    <f32> $15 = alloca
    <f32> $16 = const 0.0
    $17 : local store [$15 <- $16]
    <f32> $18 = local load [$15]
    <i32> $19 = local load [$12]
    <i32> $20 = const 0
    <f32> $21 = global ptr [S8place<f32>], index [$19, $20] activate=true
    $22 : global store [$21 <- $18]
    <i32> $23 = const 0
    <i32> $24 = cast_value<i32> $23
    <i32> $25 = const 1
    <i32> $26 = cast_value<i32> $25
    $27 : for in range($24, $26) block_dim=adaptive {
      <i32> $28 = loop $27 index 0
      <f32> $29 = alloca
      <f32> $30 = global ptr [S2place<f32>], index [$28] activate=true
      <f32> $31 = global load $30
      <i32> $32 = local load [$12]
      <i32> $33 = const 0
      <f32> $34 = global ptr [S8place<f32>], index [$32, $33] activate=true
      <f32> $35 = atomic add($34, $31)
      $36 : local store [$29 <- $35]
    }
  }
}
[I 04/20/23 19:15:56.303 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Immutable local vars eliminated:
kernel {
  <i32> $0 = const 0
  <i32> $1 = cast_value<i32> $0
  <i32> $2 = const 8192
  <i32> $3 = cast_value<i32> $2
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <i32> $6 = const 0
    <i64> $7 = global ptr [S6place<i64>], index [$5, $6] activate=true
    <i64> $8 = global load $7
    <i32> $9 = cast_value<i32> $8
    <f32> $10 = const 0.0
    <i32> $11 = const 0
    <f32> $12 = global ptr [S8place<f32>], index [$9, $11] activate=true
    $13 : global store [$12 <- $10]
    <i32> $14 = const 0
    <i32> $15 = cast_value<i32> $14
    <i32> $16 = const 1
    <i32> $17 = cast_value<i32> $16
    $18 : for in range($15, $17) block_dim=adaptive {
      <i32> $19 = loop $18 index 0
      <f32> $20 = global ptr [S2place<f32>], index [$19] activate=true
      <f32> $21 = global load $20
      <i32> $22 = const 0
      <f32> $23 = global ptr [S8place<f32>], index [$9, $22] activate=true
      <f32> $24 = atomic add($23, $21)
    }
  }
}
[I 04/20/23 19:15:56.303 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Typechecked:
kernel {
  <i32> $0 = const 0
  <i32> $1 = cast_value<i32> $0
  <i32> $2 = const 8192
  <i32> $3 = cast_value<i32> $2
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <i32> $6 = const 0
    <*i64> $7 = global ptr [S6place<i64>], index [$5, $6] activate=true
    <i64> $8 = global load $7
    <i32> $9 = cast_value<i32> $8
    <f32> $10 = const 0.0
    <i32> $11 = const 0
    <*f32> $12 = global ptr [S8place<f32>], index [$9, $11] activate=true
    $13 : global store [$12 <- $10]
    <i32> $14 = const 0
    <i32> $15 = cast_value<i32> $14
    <i32> $16 = const 1
    <i32> $17 = cast_value<i32> $16
    $18 : for in range($15, $17) block_dim=adaptive {
      <i32> $19 = loop $18 index 0
      <*f32> $20 = global ptr [S2place<f32>], index [$19] activate=true
      <f32> $21 = global load $20
      <i32> $22 = const 0
      <*f32> $23 = global ptr [S8place<f32>], index [$9, $22] activate=true
      <f32> $24 = atomic add($23, $21)
    }
  }
}
[I 04/20/23 19:15:56.303 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Bit Loop Vectorized:
kernel {
  <i32> $0 = const 0
  <i32> $1 = cast_value<i32> $0
  <i32> $2 = const 8192
  <i32> $3 = cast_value<i32> $2
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <i32> $6 = const 0
    <*i64> $7 = global ptr [S6place<i64>], index [$5, $6] activate=true
    <i64> $8 = global load $7
    <i32> $9 = cast_value<i32> $8
    <f32> $10 = const 0.0
    <i32> $11 = const 0
    <*f32> $12 = global ptr [S8place<f32>], index [$9, $11] activate=true
    $13 : global store [$12 <- $10]
    <i32> $14 = const 0
    <i32> $15 = cast_value<i32> $14
    <i32> $16 = const 1
    <i32> $17 = cast_value<i32> $16
    $18 : for in range($15, $17) block_dim=adaptive {
      <i32> $19 = loop $18 index 0
      <*f32> $20 = global ptr [S2place<f32>], index [$19] activate=true
      <f32> $21 = global load $20
      <i32> $22 = const 0
      <*f32> $23 = global ptr [S8place<f32>], index [$9, $22] activate=true
      <f32> $24 = atomic add($23, $21)
    }
  }
}
[I 04/20/23 19:15:56.303 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Matrix ptr lowered:
kernel {
  <i32> $0 = const 0
  <i32> $1 = cast_value<i32> $0
  <i32> $2 = const 8192
  <i32> $3 = cast_value<i32> $2
  $4 : for in range($1, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <i32> $6 = const 0
    <*i64> $7 = global ptr [S6place<i64>], index [$5, $6] activate=true
    <i64> $8 = global load $7
    <i32> $9 = cast_value<i32> $8
    <f32> $10 = const 0.0
    <i32> $11 = const 0
    <*f32> $12 = global ptr [S8place<f32>], index [$9, $11] activate=true
    $13 : global store [$12 <- $10]
    <i32> $14 = const 0
    <i32> $15 = cast_value<i32> $14
    <i32> $16 = const 1
    <i32> $17 = cast_value<i32> $16
    $18 : for in range($15, $17) block_dim=adaptive {
      <i32> $19 = loop $18 index 0
      <*f32> $20 = global ptr [S2place<f32>], index [$19] activate=true
      <f32> $21 = global load $20
      <i32> $22 = const 0
      <*f32> $23 = global ptr [S8place<f32>], index [$9, $22] activate=true
      <f32> $24 = atomic add($23, $21)
    }
  }
}
[I 04/20/23 19:15:56.304 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Simplified I:
kernel {
  <i32> $0 = const 0
  <i32> $1 = const 1
  <f32> $2 = const 0.0
  <i32> $3 = const 8192
  $4 : for in range($0, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <*i64> $6 = global ptr [S6place<i64>], index [$5, $0] activate=true
    <i64> $7 = global load $6
    <i32> $8 = cast_value<i32> $7
    <*f32> $9 = global ptr [S8place<f32>], index [$8, $0] activate=true
    $10 : global store [$9 <- $2]
    $11 : for in range($0, $1) block_dim=adaptive {
      <i32> $12 = loop $11 index 0
      <*f32> $13 = global ptr [S2place<f32>], index [$12] activate=true
      <f32> $14 = global load $13
      <f32> $15 = atomic add($9, $14)
    }
  }
}
[I 04/20/23 19:15:56.304 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Scalarized:
kernel {
  <i32> $0 = const 0
  <i32> $1 = const 1
  <f32> $2 = const 0.0
  <i32> $3 = const 8192
  $4 : for in range($0, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <*i64> $6 = global ptr [S6place<i64>], index [$5, $0] activate=true
    <i64> $7 = global load $6
    <i32> $8 = cast_value<i32> $7
    <*f32> $9 = global ptr [S8place<f32>], index [$8, $0] activate=true
    $10 : global store [$9 <- $2]
    $11 : for in range($0, $1) block_dim=adaptive {
      <i32> $12 = loop $11 index 0
      <*f32> $13 = global ptr [S2place<f32>], index [$12] activate=true
      <f32> $14 = global load $13
      <f32> $15 = atomic add($9, $14)
    }
  }
}
[I 04/20/23 19:15:56.304 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Gradient:
kernel {
  <i32> $0 = const 0
  <i32> $1 = const 1
  <f32> $2 = const 0.0
  <i32> $3 = const 8192
  $4 : for in range($0, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <*i64> $6 = global ptr [S6place<i64>], index [$5, $0] activate=true
    <i64> $7 = global load $6
    <i32> $8 = cast_value<i32> $7
    <*f32> $9 = global ptr [S8place<f32>], index [$8, $0] activate=true
    $10 : global store [$9 <- $2]
    <*f32> $11 = global ptr [S10place<f32>], index [$8, $0] activate=true
    $12 : reversed for in range($0, $1) block_dim=adaptive {
      <i32> $13 = loop $12 index 0
      <*f32> $14 = global ptr [S2place<f32>], index [$13] activate=true
      <f32> $15 = global load $11
      <*f32> $16 = global ptr [S4place<f32>], index [$13] activate=true
      <f32> $17 = atomic add($16, $15)
    }
  }
}
[I 04/20/23 19:15:56.304 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Access flagged I:
kernel {
  <i32> $0 = const 0
  <i32> $1 = const 1
  <f32> $2 = const 0.0
  <i32> $3 = const 8192
  $4 : for in range($0, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <*i64> $6 = global ptr [S6place<i64>], index [$5, $0] activate=false
    <i64> $7 = global load $6
    <i32> $8 = cast_value<i32> $7
    <*f32> $9 = global ptr [S8place<f32>], index [$8, $0] activate=true
    $10 : global store [$9 <- $2]
    <*f32> $11 = global ptr [S10place<f32>], index [$8, $0] activate=false
    $12 : reversed for in range($0, $1) block_dim=adaptive {
      <i32> $13 = loop $12 index 0
      <*f32> $14 = global ptr [S2place<f32>], index [$13] activate=false
      <f32> $15 = global load $11
      <*f32> $16 = global ptr [S4place<f32>], index [$13] activate=true
      <f32> $17 = atomic add($16, $15)
    }
  }
}
[I 04/20/23 19:15:56.304 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Simplified II:
kernel {
  <i32> $0 = const 0
  <i32> $1 = const 1
  <f32> $2 = const 0.0
  <i32> $3 = const 8192
  $4 : for in range($0, $3) block_dim=256 {
    <i32> $5 = loop $4 index 0
    <*i64> $6 = global ptr [S6place<i64>], index [$5, $0] activate=false
    <i64> $7 = global load $6
    <i32> $8 = cast_value<i32> $7
    <*f32> $9 = global ptr [S8place<f32>], index [$8, $0] activate=true
    $10 : global store [$9 <- $2]
    <*f32> $11 = global ptr [S10place<f32>], index [$8, $0] activate=false
    $12 : reversed for in range($0, $1) block_dim=adaptive {
      <i32> $13 = loop $12 index 0
      <f32> $14 = global load $11
      <*f32> $15 = global ptr [S4place<f32>], index [$13] activate=true
      <f32> $16 = atomic add($15, $14)
    }
  }
}
[I 04/20/23 19:15:56.304 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Offloaded:
kernel {
  $0 = offloaded
  body {
    <i32> $1 = const 0
    <i32> $2 = const 1
    <f32> $3 = const 0.0
    <i32> $4 = const 8192
  }
  $5 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
  body {
    <i32> $6 = loop $5 index 0
    <i32> $7 = const 0
    <*i64> $8 = global ptr [S6place<i64>], index [$6, $7] activate=false
    <i64> $9 = global load $8
    <i32> $10 = cast_value<i32> $9
    <i32> $11 = const 0
    <*f32> $12 = global ptr [S8place<f32>], index [$10, $11] activate=true
    <f32> $13 = const 0.0
    $14 : global store [$12 <- $13]
    <i32> $15 = const 0
    <*f32> $16 = global ptr [S10place<f32>], index [$10, $15] activate=false
    <i32> $17 = const 0
    <i32> $18 = const 1
    $19 : reversed for in range($17, $18) block_dim=adaptive {
      <i32> $20 = loop $19 index 0
      <f32> $21 = global load $16
      <*f32> $22 = global ptr [S4place<f32>], index [$20] activate=true
      <f32> $23 = atomic add($22, $21)
    }
  }
}
[I 04/20/23 19:15:56.305 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Optimized by CFG:
kernel {
  $0 = offloaded
  body {
  }
  $1 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
  body {
    <i32> $2 = loop $1 index 0
    <i32> $3 = const 0
    <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
    <i64> $5 = global load $4
    <i32> $6 = cast_value<i32> $5
    <i32> $7 = const 0
    <*f32> $8 = global ptr [S8place<f32>], index [$6, $7] activate=true
    <f32> $9 = const 0.0
    $10 : global store [$8 <- $9]
    <i32> $11 = const 0
    <*f32> $12 = global ptr [S10place<f32>], index [$6, $11] activate=false
    <i32> $13 = const 0
    <i32> $14 = const 1
    $15 : reversed for in range($13, $14) block_dim=adaptive {
      <i32> $16 = loop $15 index 0
      <f32> $17 = global load $12
      <*f32> $18 = global ptr [S4place<f32>], index [$16] activate=true
      <f32> $19 = atomic add($18, $17)
    }
  }
}
[I 04/20/23 19:15:56.305 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Access flagged II:
kernel {
  $0 = offloaded
  body {
  }
  $1 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
  body {
    <i32> $2 = loop $1 index 0
    <i32> $3 = const 0
    <*i64> $4 = global ptr [S6place<i64>], index [$2, $3] activate=false
    <i64> $5 = global load $4
    <i32> $6 = cast_value<i32> $5
    <i32> $7 = const 0
    <*f32> $8 = global ptr [S8place<f32>], index [$6, $7] activate=true
    <f32> $9 = const 0.0
    $10 : global store [$8 <- $9]
    <i32> $11 = const 0
    <*f32> $12 = global ptr [S10place<f32>], index [$6, $11] activate=false
    <i32> $13 = const 0
    <i32> $14 = const 1
    $15 : reversed for in range($13, $14) block_dim=adaptive {
      <i32> $16 = loop $15 index 0
      <f32> $17 = global load $12
      <*f32> $18 = global ptr [S4place<f32>], index [$16] activate=true
      <f32> $19 = atomic add($18, $17)
    }
  }
}
[I 04/20/23 19:15:56.305 1820159] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Simplified III:
kernel {
  $0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
  body {
    <i32> $1 = loop $0 index 0
    <i32> $2 = const 0
    <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
    <i64> $4 = global load $3
    <i32> $5 = cast_value<i32> $4
    <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
    <f32> $7 = const 0.0
    $8 : global store [$6 <- $7]
    <*f32> $9 = global ptr [S10place<f32>], index [$5, $2] activate=false
    <i32> $10 = const 1
    $11 : reversed for in range($2, $10) block_dim=adaptive {
      <i32> $12 = loop $11 index 0
      <f32> $13 = global load $9
      <*f32> $14 = global ptr [S4place<f32>], index [$12] activate=true
      <f32> $15 = atomic add($14, $13)
    }
  }
}
[I 04/20/23 19:15:56.305 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Start offload_to_executable:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <*f32> $9 = global ptr [S10place<f32>], index [$5, $2] activate=false
  <i32> $10 = const 1
  $11 : reversed for in range($2, $10) block_dim=adaptive {
    <i32> $12 = loop $11 index 0
    <f32> $13 = global load $9
    <*f32> $14 = global ptr [S4place<f32>], index [$12] activate=true
    <f32> $15 = atomic add($14, $13)
  }
}
}
[I 04/20/23 19:15:56.305 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Detect read-only accesses:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <*f32> $9 = global ptr [S10place<f32>], index [$5, $2] activate=false
  <i32> $10 = const 1
  $11 : reversed for in range($2, $10) block_dim=adaptive {
    <i32> $12 = loop $11 index 0
    <f32> $13 = global load $9
    <*f32> $14 = global ptr [S4place<f32>], index [$12] activate=true
    <f32> $15 = atomic add($14, $13)
  }
}
}
[I 04/20/23 19:15:56.305 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Atomics demoted I:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <*f32> $9 = global ptr [S10place<f32>], index [$5, $2] activate=false
  <i32> $10 = const 1
  $11 : reversed for in range($2, $10) block_dim=adaptive {
    <i32> $12 = loop $11 index 0
    <f32> $13 = global load $9
    <*f32> $14 = global ptr [S4place<f32>], index [$12] activate=true
    <f32> $15 = atomic add($14, $13)
  }
}
}
[I 04/20/23 19:15:56.305 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Cache loop-invariant global vars:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <*f32> $9 = global ptr [S10place<f32>], index [$5, $2] activate=false
  <i32> $10 = const 1
  $11 : reversed for in range($2, $10) block_dim=adaptive {
    <i32> $12 = loop $11 index 0
    <f32> $13 = global load $9
    <*f32> $14 = global ptr [S4place<f32>], index [$12] activate=true
    <f32> $15 = atomic add($14, $13)
  }
}
}
[I 04/20/23 19:15:56.305 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Dense struct-for demoted:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <*f32> $9 = global ptr [S10place<f32>], index [$5, $2] activate=false
  <i32> $10 = const 1
  $11 : reversed for in range($2, $10) block_dim=adaptive {
    <i32> $12 = loop $11 index 0
    <f32> $13 = global load $9
    <*f32> $14 = global ptr [S4place<f32>], index [$12] activate=true
    <f32> $15 = atomic add($14, $13)
  }
}
}
[I 04/20/23 19:15:56.305 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] No-access mesh-for demoted:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <*f32> $9 = global ptr [S10place<f32>], index [$5, $2] activate=false
  <i32> $10 = const 1
  $11 : reversed for in range($2, $10) block_dim=adaptive {
    <i32> $12 = loop $11 index 0
    <f32> $13 = global load $9
    <*f32> $14 = global ptr [S4place<f32>], index [$12] activate=true
    <f32> $15 = atomic add($14, $13)
  }
}
}
[I 04/20/23 19:15:56.305 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Make thread local:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <*f32> $9 = global ptr [S10place<f32>], index [$5, $2] activate=false
  <i32> $10 = const 1
  $11 : reversed for in range($2, $10) block_dim=adaptive {
    <i32> $12 = loop $11 index 0
    <f32> $13 = global load $9
    <*f32> $14 = global ptr [S4place<f32>], index [$12] activate=true
    <f32> $15 = atomic add($14, $13)
  }
}
}
[I 04/20/23 19:15:56.305 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Make mesh thread local:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <*f32> $9 = global ptr [S10place<f32>], index [$5, $2] activate=false
  <i32> $10 = const 1
  $11 : reversed for in range($2, $10) block_dim=adaptive {
    <i32> $12 = loop $11 index 0
    <f32> $13 = global load $9
    <*f32> $14 = global ptr [S4place<f32>], index [$12] activate=true
    <f32> $15 = atomic add($14, $13)
  }
}
}
[I 04/20/23 19:15:56.306 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Make mesh block local:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <*f32> $9 = global ptr [S10place<f32>], index [$5, $2] activate=false
  <i32> $10 = const 1
  $11 : reversed for in range($2, $10) block_dim=adaptive {
    <i32> $12 = loop $11 index 0
    <f32> $13 = global load $9
    <*f32> $14 = global ptr [S4place<f32>], index [$12] activate=true
    <f32> $15 = atomic add($14, $13)
  }
}
}
[I 04/20/23 19:15:56.306 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Simplified X:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <*f32> $9 = global ptr [S10place<f32>], index [$5, $2] activate=false
  <i32> $10 = const 1
  $11 : reversed for in range($2, $10) block_dim=adaptive {
    <i32> $12 = loop $11 index 0
    <f32> $13 = global load $9
    <*f32> $14 = global ptr [S4place<f32>], index [$12] activate=true
    <f32> $15 = atomic add($14, $13)
  }
}
}
[I 04/20/23 19:15:56.306 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Make block local:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <*f32> $9 = global ptr [S10place<f32>], index [$5, $2] activate=false
  <i32> $10 = const 1
  $11 : reversed for in range($2, $10) block_dim=adaptive {
    <i32> $12 = loop $11 index 0
    <f32> $13 = global load $9
    <*f32> $14 = global ptr [S4place<f32>], index [$12] activate=true
    <f32> $15 = atomic add($14, $13)
  }
}
}
[I 04/20/23 19:15:56.306 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Demote mesh statements:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <*f32> $9 = global ptr [S10place<f32>], index [$5, $2] activate=false
  <i32> $10 = const 1
  $11 : reversed for in range($2, $10) block_dim=adaptive {
    <i32> $12 = loop $11 index 0
    <f32> $13 = global load $9
    <*f32> $14 = global ptr [S4place<f32>], index [$12] activate=true
    <f32> $15 = atomic add($14, $13)
  }
}
}
[I 04/20/23 19:15:56.306 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Atomics demoted II:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <*f32> $9 = global ptr [S10place<f32>], index [$5, $2] activate=false
  <i32> $10 = const 1
  $11 : reversed for in range($2, $10) block_dim=adaptive {
    <i32> $12 = loop $11 index 0
    <f32> $13 = global load $9
    <*f32> $14 = global ptr [S4place<f32>], index [$12] activate=true
    <f32> $15 = atomic add($14, $13)
  }
}
}
[I 04/20/23 19:15:56.306 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Remove range assumption:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <*f32> $9 = global ptr [S10place<f32>], index [$5, $2] activate=false
  <i32> $10 = const 1
  $11 : reversed for in range($2, $10) block_dim=adaptive {
    <i32> $12 = loop $11 index 0
    <f32> $13 = global load $9
    <*f32> $14 = global ptr [S4place<f32>], index [$12] activate=true
    <f32> $15 = atomic add($14, $13)
  }
}
}
[I 04/20/23 19:15:56.306 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Remove loop_unique:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <*f32> $9 = global ptr [S10place<f32>], index [$5, $2] activate=false
  <i32> $10 = const 1
  $11 : reversed for in range($2, $10) block_dim=adaptive {
    <i32> $12 = loop $11 index 0
    <f32> $13 = global load $9
    <*f32> $14 = global ptr [S4place<f32>], index [$12] activate=true
    <f32> $15 = atomic add($14, $13)
  }
}
}
[I 04/20/23 19:15:56.306 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Simplified before lower access:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <i64> $4 = global load $3
  <i32> $5 = cast_value<i32> $4
  <*f32> $6 = global ptr [S8place<f32>], index [$5, $2] activate=true
  <f32> $7 = const 0.0
  $8 : global store [$6 <- $7]
  <*f32> $9 = global ptr [S10place<f32>], index [$5, $2] activate=false
  <i32> $10 = const 1
  $11 : reversed for in range($2, $10) block_dim=adaptive {
    <i32> $12 = loop $11 index 0
    <f32> $13 = global load $9
    <*f32> $14 = global ptr [S4place<f32>], index [$12] activate=true
    <f32> $15 = atomic add($14, $13)
  }
}
}
[I 04/20/23 19:15:56.306 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Access lowered:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*i64> $3 = global ptr [S6place<i64>], index [$1, $2] activate=false
  <*gen> $4 = get root [S0root][root]
  <i32> $5 = linearized(ind {}, stride {})
  <*gen> $6 = [S0root][root]::lookup($4, $5) activate = false
  <*gen> $7 = get child [S0root->S5dense] $6
  <i32> $8 = const 0
  <i32> $9 = bit_shr $1 $8
  <i32> $10 = const 0
  <i32> $11 = bit_shr $2 $10
  <i32> $12 = linearized(ind {$9, $11}, stride {8192, 3})
  <*gen> $13 = [S5dense][dense]::lookup($7, $12) activate = false
  <*i64> $14 = get child [S5dense->S6place<i64>] $13
  <i64> $15 = global load $14
  <i32> $16 = cast_value<i32> $15
  <*f32> $17 = global ptr [S8place<f32>], index [$16, $2] activate=true
  <f32> $18 = const 0.0
  <*gen> $19 = get root [S0root][root]
  <i32> $20 = linearized(ind {}, stride {})
  <*gen> $21 = [S0root][root]::lookup($19, $20) activate = false
  <*gen> $22 = get child [S0root->S7dense] $21
  <i32> $23 = const 0
  <i32> $24 = bit_shr $16 $23
  <i32> $25 = const 0
  <i32> $26 = bit_shr $2 $25
  <i32> $27 = linearized(ind {$24, $26}, stride {8192, 3})
  <*gen> $28 = [S7dense][dense]::lookup($22, $27) activate = false
  <*f32> $29 = get child [S7dense->S8place<f32>] $28
  $30 : global store [$29 <- $18]
  <*f32> $31 = global ptr [S10place<f32>], index [$16, $2] activate=false
  <i32> $32 = const 1
  $33 : reversed for in range($2, $32) block_dim=adaptive {
    <i32> $34 = loop $33 index 0
    <*gen> $35 = get root [S0root][root]
    <i32> $36 = linearized(ind {}, stride {})
    <*gen> $37 = [S0root][root]::lookup($35, $36) activate = false
    <*gen> $38 = get child [S0root->S9dense] $37
    <i32> $39 = const 0
    <i32> $40 = bit_shr $16 $39
    <i32> $41 = const 0
    <i32> $42 = bit_shr $2 $41
    <i32> $43 = linearized(ind {$40, $42}, stride {8192, 3})
    <*gen> $44 = [S9dense][dense]::lookup($38, $43) activate = false
    <*f32> $45 = get child [S9dense->S10place<f32>] $44
    <f32> $46 = global load $45
    <*f32> $47 = global ptr [S4place<f32>], index [$34] activate=true
    <*gen> $48 = get root [S0root][root]
    <i32> $49 = linearized(ind {}, stride {})
    <*gen> $50 = [S0root][root]::lookup($48, $49) activate = false
    <*gen> $51 = get child [S0root->S3dense] $50
    <i32> $52 = const 0
    <i32> $53 = bit_shr $34 $52
    <i32> $54 = linearized(ind {$53}, stride {8388608})
    <*gen> $55 = [S3dense][dense]::lookup($51, $54) activate = false
    <*f32> $56 = get child [S3dense->S4place<f32>] $55
    <f32> $57 = atomic add($56, $46)
  }
}
}
[I 04/20/23 19:15:56.306 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] DIE:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*gen> $3 = get root [S0root][root]
  <i32> $4 = linearized(ind {}, stride {})
  <*gen> $5 = [S0root][root]::lookup($3, $4) activate = false
  <*gen> $6 = get child [S0root->S5dense] $5
  <i32> $7 = const 0
  <i32> $8 = bit_shr $1 $7
  <i32> $9 = const 0
  <i32> $10 = bit_shr $2 $9
  <i32> $11 = linearized(ind {$8, $10}, stride {8192, 3})
  <*gen> $12 = [S5dense][dense]::lookup($6, $11) activate = false
  <*i64> $13 = get child [S5dense->S6place<i64>] $12
  <i64> $14 = global load $13
  <i32> $15 = cast_value<i32> $14
  <*f32> $16 = global ptr [S8place<f32>], index [$15, $2] activate=true
  <f32> $17 = const 0.0
  <*gen> $18 = get root [S0root][root]
  <i32> $19 = linearized(ind {}, stride {})
  <*gen> $20 = [S0root][root]::lookup($18, $19) activate = false
  <*gen> $21 = get child [S0root->S7dense] $20
  <i32> $22 = const 0
  <i32> $23 = bit_shr $15 $22
  <i32> $24 = const 0
  <i32> $25 = bit_shr $2 $24
  <i32> $26 = linearized(ind {$23, $25}, stride {8192, 3})
  <*gen> $27 = [S7dense][dense]::lookup($21, $26) activate = false
  <*f32> $28 = get child [S7dense->S8place<f32>] $27
  $29 : global store [$28 <- $17]
  <i32> $30 = const 1
  $31 : reversed for in range($2, $30) block_dim=adaptive {
    <i32> $32 = loop $31 index 0
    <*gen> $33 = get root [S0root][root]
    <i32> $34 = linearized(ind {}, stride {})
    <*gen> $35 = [S0root][root]::lookup($33, $34) activate = false
    <*gen> $36 = get child [S0root->S9dense] $35
    <i32> $37 = const 0
    <i32> $38 = bit_shr $15 $37
    <i32> $39 = const 0
    <i32> $40 = bit_shr $2 $39
    <i32> $41 = linearized(ind {$38, $40}, stride {8192, 3})
    <*gen> $42 = [S9dense][dense]::lookup($36, $41) activate = false
    <*f32> $43 = get child [S9dense->S10place<f32>] $42
    <f32> $44 = global load $43
    <*f32> $45 = global ptr [S4place<f32>], index [$32] activate=true
    <*gen> $46 = get root [S0root][root]
    <i32> $47 = linearized(ind {}, stride {})
    <*gen> $48 = [S0root][root]::lookup($46, $47) activate = false
    <*gen> $49 = get child [S0root->S3dense] $48
    <i32> $50 = const 0
    <i32> $51 = bit_shr $32 $50
    <i32> $52 = linearized(ind {$51}, stride {8388608})
    <*gen> $53 = [S3dense][dense]::lookup($49, $52) activate = false
    <*f32> $54 = get child [S3dense->S4place<f32>] $53
    <f32> $55 = atomic add($54, $44)
  }
}
}
[I 04/20/23 19:15:56.307 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Access flagged III:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*gen> $3 = get root [S0root][root]
  <i32> $4 = linearized(ind {}, stride {})
  <*gen> $5 = [S0root][root]::lookup($3, $4) activate = false
  <*gen> $6 = get child [S0root->S5dense] $5
  <i32> $7 = const 0
  <i32> $8 = bit_shr $1 $7
  <i32> $9 = const 0
  <i32> $10 = bit_shr $2 $9
  <i32> $11 = linearized(ind {$8, $10}, stride {8192, 3})
  <*gen> $12 = [S5dense][dense]::lookup($6, $11) activate = false
  <*i64> $13 = get child [S5dense->S6place<i64>] $12
  <i64> $14 = global load $13
  <i32> $15 = cast_value<i32> $14
  <*f32> $16 = global ptr [S8place<f32>], index [$15, $2] activate=false
  <f32> $17 = const 0.0
  <*gen> $18 = get root [S0root][root]
  <i32> $19 = linearized(ind {}, stride {})
  <*gen> $20 = [S0root][root]::lookup($18, $19) activate = false
  <*gen> $21 = get child [S0root->S7dense] $20
  <i32> $22 = const 0
  <i32> $23 = bit_shr $15 $22
  <i32> $24 = const 0
  <i32> $25 = bit_shr $2 $24
  <i32> $26 = linearized(ind {$23, $25}, stride {8192, 3})
  <*gen> $27 = [S7dense][dense]::lookup($21, $26) activate = false
  <*f32> $28 = get child [S7dense->S8place<f32>] $27
  $29 : global store [$28 <- $17]
  <i32> $30 = const 1
  $31 : reversed for in range($2, $30) block_dim=adaptive {
    <i32> $32 = loop $31 index 0
    <*gen> $33 = get root [S0root][root]
    <i32> $34 = linearized(ind {}, stride {})
    <*gen> $35 = [S0root][root]::lookup($33, $34) activate = false
    <*gen> $36 = get child [S0root->S9dense] $35
    <i32> $37 = const 0
    <i32> $38 = bit_shr $15 $37
    <i32> $39 = const 0
    <i32> $40 = bit_shr $2 $39
    <i32> $41 = linearized(ind {$38, $40}, stride {8192, 3})
    <*gen> $42 = [S9dense][dense]::lookup($36, $41) activate = false
    <*f32> $43 = get child [S9dense->S10place<f32>] $42
    <f32> $44 = global load $43
    <*f32> $45 = global ptr [S4place<f32>], index [$32] activate=false
    <*gen> $46 = get root [S0root][root]
    <i32> $47 = linearized(ind {}, stride {})
    <*gen> $48 = [S0root][root]::lookup($46, $47) activate = false
    <*gen> $49 = get child [S0root->S3dense] $48
    <i32> $50 = const 0
    <i32> $51 = bit_shr $32 $50
    <i32> $52 = linearized(ind {$51}, stride {8388608})
    <*gen> $53 = [S3dense][dense]::lookup($49, $52) activate = false
    <*f32> $54 = get child [S3dense->S4place<f32>] $53
    <f32> $55 = atomic add($54, $44)
  }
}
}
[I 04/20/23 19:15:56.307 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Operations demoted:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*gen> $3 = get root [S0root][root]
  <i32> $4 = linearized(ind {}, stride {})
  <*gen> $5 = [S0root][root]::lookup($3, $4) activate = false
  <*gen> $6 = get child [S0root->S5dense] $5
  <i32> $7 = const 0
  <u32> $8 = reinterpret_cast_bits<u32> $1
  <u32> $9 = cast_value<u32> $7
  <u32> $10 = bit_sar $8 $9
  <i32> $11 = reinterpret_cast_bits<i32> $10
  <i32> $12 = const 0
  <u32> $13 = reinterpret_cast_bits<u32> $2
  <u32> $14 = cast_value<u32> $12
  <u32> $15 = bit_sar $13 $14
  <i32> $16 = reinterpret_cast_bits<i32> $15
  <i32> $17 = linearized(ind {$11, $16}, stride {8192, 3})
  <*gen> $18 = [S5dense][dense]::lookup($6, $17) activate = false
  <*i64> $19 = get child [S5dense->S6place<i64>] $18
  <i64> $20 = global load $19
  <i32> $21 = cast_value<i32> $20
  <*f32> $22 = global ptr [S8place<f32>], index [$21, $2] activate=false
  <f32> $23 = const 0.0
  <*gen> $24 = get root [S0root][root]
  <i32> $25 = linearized(ind {}, stride {})
  <*gen> $26 = [S0root][root]::lookup($24, $25) activate = false
  <*gen> $27 = get child [S0root->S7dense] $26
  <i32> $28 = const 0
  <u32> $29 = reinterpret_cast_bits<u32> $21
  <u32> $30 = cast_value<u32> $28
  <u32> $31 = bit_sar $29 $30
  <i32> $32 = reinterpret_cast_bits<i32> $31
  <i32> $33 = const 0
  <u32> $34 = reinterpret_cast_bits<u32> $2
  <u32> $35 = cast_value<u32> $33
  <u32> $36 = bit_sar $34 $35
  <i32> $37 = reinterpret_cast_bits<i32> $36
  <i32> $38 = linearized(ind {$32, $37}, stride {8192, 3})
  <*gen> $39 = [S7dense][dense]::lookup($27, $38) activate = false
  <*f32> $40 = get child [S7dense->S8place<f32>] $39
  $41 : global store [$40 <- $23]
  <i32> $42 = const 1
  $43 : reversed for in range($2, $42) block_dim=adaptive {
    <i32> $44 = loop $43 index 0
    <*gen> $45 = get root [S0root][root]
    <i32> $46 = linearized(ind {}, stride {})
    <*gen> $47 = [S0root][root]::lookup($45, $46) activate = false
    <*gen> $48 = get child [S0root->S9dense] $47
    <i32> $49 = const 0
    <u32> $50 = reinterpret_cast_bits<u32> $21
    <u32> $51 = cast_value<u32> $49
    <u32> $52 = bit_sar $50 $51
    <i32> $53 = reinterpret_cast_bits<i32> $52
    <i32> $54 = const 0
    <u32> $55 = reinterpret_cast_bits<u32> $2
    <u32> $56 = cast_value<u32> $54
    <u32> $57 = bit_sar $55 $56
    <i32> $58 = reinterpret_cast_bits<i32> $57
    <i32> $59 = linearized(ind {$53, $58}, stride {8192, 3})
    <*gen> $60 = [S9dense][dense]::lookup($48, $59) activate = false
    <*f32> $61 = get child [S9dense->S10place<f32>] $60
    <f32> $62 = global load $61
    <*f32> $63 = global ptr [S4place<f32>], index [$44] activate=false
    <*gen> $64 = get root [S0root][root]
    <i32> $65 = linearized(ind {}, stride {})
    <*gen> $66 = [S0root][root]::lookup($64, $65) activate = false
    <*gen> $67 = get child [S0root->S3dense] $66
    <i32> $68 = const 0
    <u32> $69 = reinterpret_cast_bits<u32> $44
    <u32> $70 = cast_value<u32> $68
    <u32> $71 = bit_sar $69 $70
    <i32> $72 = reinterpret_cast_bits<i32> $71
    <i32> $73 = linearized(ind {$72}, stride {8388608})
    <*gen> $74 = [S3dense][dense]::lookup($67, $73) activate = false
    <*f32> $75 = get child [S3dense->S4place<f32>] $74
    <f32> $76 = atomic add($75, $62)
  }
}
}
[I 04/20/23 19:15:56.307 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Simplified IV:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*gen> $3 = get root [S0root][root]
  <*gen> $4 = [S0root][root]::lookup($3, $2) activate = false
  <*gen> $5 = get child [S0root->S5dense] $4
  <i32> $6 = const 1
  <i32> $7 = const 3
  <i32> $8 = mul $1 $7
  <*gen> $9 = [S5dense][dense]::lookup($5, $8) activate = false
  <*i64> $10 = get child [S5dense->S6place<i64>] $9
  <i64> $11 = global load $10
  <i32> $12 = cast_value<i32> $11
  <f32> $13 = const 0.0
  <*gen> $14 = get child [S0root->S7dense] $4
  <i32> $15 = mul $12 $7
  <*gen> $16 = [S7dense][dense]::lookup($14, $15) activate = false
  <*f32> $17 = get child [S7dense->S8place<f32>] $16
  $18 : global store [$17 <- $13]
  $19 : reversed for in range($2, $6) block_dim=adaptive {
    <i32> $20 = loop $19 index 0
    <*gen> $21 = get child [S0root->S9dense] $4
    <*gen> $22 = [S9dense][dense]::lookup($21, $15) activate = false
    <*f32> $23 = get child [S9dense->S10place<f32>] $22
    <f32> $24 = global load $23
    <*gen> $25 = get child [S0root->S3dense] $4
    <*gen> $26 = [S3dense][dense]::lookup($25, $20) activate = false
    <*f32> $27 = get child [S3dense->S4place<f32>] $26
    <f32> $28 = atomic add($27, $24)
  }
}
}
[I 04/20/23 19:15:56.308 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Autodiff stack size determined:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*gen> $3 = get root [S0root][root]
  <*gen> $4 = [S0root][root]::lookup($3, $2) activate = false
  <*gen> $5 = get child [S0root->S5dense] $4
  <i32> $6 = const 1
  <i32> $7 = const 3
  <i32> $8 = mul $1 $7
  <*gen> $9 = [S5dense][dense]::lookup($5, $8) activate = false
  <*i64> $10 = get child [S5dense->S6place<i64>] $9
  <i64> $11 = global load $10
  <i32> $12 = cast_value<i32> $11
  <f32> $13 = const 0.0
  <*gen> $14 = get child [S0root->S7dense] $4
  <i32> $15 = mul $12 $7
  <*gen> $16 = [S7dense][dense]::lookup($14, $15) activate = false
  <*f32> $17 = get child [S7dense->S8place<f32>] $16
  $18 : global store [$17 <- $13]
  $19 : reversed for in range($2, $6) block_dim=adaptive {
    <i32> $20 = loop $19 index 0
    <*gen> $21 = get child [S0root->S9dense] $4
    <*gen> $22 = [S9dense][dense]::lookup($21, $15) activate = false
    <*f32> $23 = get child [S9dense->S10place<f32>] $22
    <f32> $24 = global load $23
    <*gen> $25 = get child [S0root->S3dense] $4
    <*gen> $26 = [S3dense][dense]::lookup($25, $20) activate = false
    <*f32> $27 = get child [S3dense->S4place<f32>] $26
    <f32> $28 = atomic add($27, $24)
  }
}
}
[I 04/20/23 19:15:56.308 1820177] [compile_to_offloads.cpp:operator()@23] [composite_train_fw_c77_0_reverse_grad] Bit struct stores optimized:
kernel {
$0 = offloaded range_for(0, 8192) grid_dim=2624 block_dim=256
body {
  <i32> $1 = loop $0 index 0
  <i32> $2 = const 0
  <*gen> $3 = get root [S0root][root]
  <*gen> $4 = [S0root][root]::lookup($3, $2) activate = false
  <*gen> $5 = get child [S0root->S5dense] $4
  <i32> $6 = const 1
  <i32> $7 = const 3
  <i32> $8 = mul $1 $7
  <*gen> $9 = [S5dense][dense]::lookup($5, $8) activate = false
  <*i64> $10 = get child [S5dense->S6place<i64>] $9
  <i64> $11 = global load $10
  <i32> $12 = cast_value<i32> $11
  <f32> $13 = const 0.0
  <*gen> $14 = get child [S0root->S7dense] $4
  <i32> $15 = mul $12 $7
  <*gen> $16 = [S7dense][dense]::lookup($14, $15) activate = false
  <*f32> $17 = get child [S7dense->S8place<f32>] $16
  $18 : global store [$17 <- $13]
  $19 : reversed for in range($2, $6) block_dim=adaptive {
    <i32> $20 = loop $19 index 0
    <*gen> $21 = get child [S0root->S9dense] $4
    <*gen> $22 = [S9dense][dense]::lookup($21, $15) activate = false
    <*f32> $23 = get child [S9dense->S10place<f32>] $22
    <f32> $24 = global load $23
    <*gen> $25 = get child [S0root->S3dense] $4
    <*gen> $26 = [S3dense][dense]::lookup($25, $20) activate = false
    <*f32> $27 = get child [S3dense->S4place<f32>] $26
    <f32> $28 = atomic add($27, $24)
  }
}
}
no stack 8192.0
50674.547
with stack 8192.0
=========================================================================
Kernel Profiler(count, default) @ CUDA on NVIDIA GeForce RTX 3090
=========================================================================
[      %     total   count |      min       avg       max   ] Kernel name
-------------------------------------------------------------------------
[ 30.63%   0.000 s      1x |    0.090     0.090     0.090 ms] torch2ti_c74_0_kernel_0_serial
[ 30.48%   0.000 s      1x |    0.089     0.089     0.089 ms] tensor_to_ext_arr_c6_1_kernel_0_range_for
[ 14.71%   0.000 s      1x |    0.043     0.043     0.043 ms] fill_tensor_c0_1_kernel_0_range_for
[  7.61%   0.000 s      1x |    0.022     0.022     0.022 ms] composite_train_fw_stack_c79_0_reverse_grad_kernel_0_range_for
[  2.69%   0.000 s      1x |    0.008     0.008     0.008 ms] torch2ti_c74_1_kernel_1_range_for
[  2.66%   0.000 s      1x |    0.008     0.008     0.008 ms] tensor_to_ext_arr_c6_0_kernel_0_range_for
[  2.52%   0.000 s      1x |    0.007     0.007     0.007 ms] torch2ti_c74_0_kernel_1_range_for
[  2.40%   0.000 s      1x |    0.007     0.007     0.007 ms] torch2ti_c74_1_kernel_0_serial
[  1.75%   0.000 s      1x |    0.005     0.005     0.005 ms] fill_tensor_c0_2_kernel_0_range_for
[  1.75%   0.000 s      1x |    0.005     0.005     0.005 ms] composite_train_fw_stack_c78_0_kernel_0_range_for
[  1.40%   0.000 s      1x |    0.004     0.004     0.004 ms] fill_tensor_c0_0_kernel_0_range_for
[  1.40%   0.000 s      1x |    0.004     0.004     0.004 ms] fill_tensor_c0_3_kernel_0_range_for
-------------------------------------------------------------------------
[100.00%] Total execution time:   0.000 s   number of results: 12
=========================================================================
50674.547
no stack 8192.0
=========================================================================
Kernel Profiler(count, default) @ CUDA on NVIDIA GeForce RTX 3090
=========================================================================
[      %     total   count |      min       avg       max   ] Kernel name
-------------------------------------------------------------------------
[ 31.27%   0.000 s      1x |    0.093     0.093     0.093 ms] torch2ti_c74_0_kernel_0_serial
[ 29.98%   0.000 s      1x |    0.089     0.089     0.089 ms] tensor_to_ext_arr_c6_1_kernel_0_range_for
[ 14.10%   0.000 s      1x |    0.042     0.042     0.042 ms] fill_tensor_c0_1_kernel_0_range_for
[  7.37%   0.000 s      1x |    0.022     0.022     0.022 ms] composite_train_fw_c77_0_reverse_grad_kernel_0_range_for
[  3.10%   0.000 s      1x |    0.009     0.009     0.009 ms] torch2ti_c74_0_kernel_1_range_for
[  2.70%   0.000 s      1x |    0.008     0.008     0.008 ms] torch2ti_c74_1_kernel_1_range_for
[  2.69%   0.000 s      1x |    0.008     0.008     0.008 ms] torch2ti_c74_1_kernel_0_serial
[  2.06%   0.000 s      1x |    0.006     0.006     0.006 ms] composite_train_fw_c76_0_kernel_0_range_for
[  1.91%   0.000 s      1x |    0.006     0.006     0.006 ms] tensor_to_ext_arr_c6_0_kernel_0_range_for
[  1.72%   0.000 s      1x |    0.005     0.005     0.005 ms] fill_tensor_c0_0_kernel_0_range_for
[  1.72%   0.000 s      1x |    0.005     0.005     0.005 ms] fill_tensor_c0_2_kernel_0_range_for
[  1.38%   0.000 s      1x |    0.004     0.004     0.004 ms] fill_tensor_c0_3_kernel_0_range_for
-------------------------------------------------------------------------
[100.00%] Total execution time:   0.000 s   number of results: 12
=========================================================================
